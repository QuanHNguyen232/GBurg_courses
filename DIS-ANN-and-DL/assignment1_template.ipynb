{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin**: *Friday*, March 11, 2022, 23:59<br>\n",
    "The most important helper functions are copied from their original exercise sets but there are some you will have to copy by yourself. etc). Run the code so that the code produces outputs and plots. Once you're satisfied go ahead and upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def step(z, threshold=0.5):\n",
    "    if z > threshold:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Feed forward neural network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # Q1: Print these out for a network of size `[2, 3, 1]`, and explain their structure and contents\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        \n",
    "        # Q2: What is `a`? How many iterations will this loop run? For a `sizes=[2, 3, 1]`\n",
    "        # network, what is the shape of `a` at each iteration?\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    # --------------------------------------------------------------------------- #\n",
    "    # Don't worry too much about the code below. We will look into that next week #\n",
    "    # --------------------------------------------------------------------------- #\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, silent=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  ``epochs`` represents the number of training iterations\n",
    "        over the training data.  ``eta`` is the learning rate.\n",
    "        If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        \n",
    "        n = len(training_data)\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            if not silent:\n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        self.weights = [\n",
    "            w - eta / len(mini_batch) * nw\n",
    "            for w, nw in zip(self.weights, nabla_w)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            b - eta / len(mini_batch) * nb\n",
    "            for b, nb in zip(self.biases, nabla_b)\n",
    "        ]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book. Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on. It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return nabla_b, nabla_w\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        if self.sizes[-1] == 1:\n",
    "            test_results = [\n",
    "                (step(self.feedforward(x)), y)\n",
    "                for x, y in test_data\n",
    "            ]\n",
    "        else:\n",
    "            test_results = [\n",
    "                (np.argmax(self.feedforward(x)), y)\n",
    "                for x, y in test_data\n",
    "            ]\n",
    "        return sum(int(y_pred == y) for (y_pred, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return output_activations - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X_linear(N=200):\n",
    "    X = np.vstack([\n",
    "        np.random.normal([-2, -2], 1, size=(int(N/2), 2)),\n",
    "        np.random.normal([2, 2], 1, size=(int(N/2), 2))\n",
    "    ])\n",
    "\n",
    "    y = np.array([0] * int(N/2) + [1] * int(N/2)).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def generate_X_nonlinear(N=200, R=5):\n",
    "    X_inner = np.random.normal([0, 0], 1, size=(int(N/2), 2))\n",
    "\n",
    "    X_outer = np.array([\n",
    "        [R*np.cos(theta), R*np.sin(theta)]\n",
    "        for theta in np.linspace(0, 2 * np.pi, int(N/2))\n",
    "    ]) + np.random.randn(int(N/2), 2)\n",
    "\n",
    "    X = np.vstack([X_inner, X_outer])\n",
    "    y = np.array([0] * int(N/2) + [1] * int(N/2)).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "X, y = generate_X_linear()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Linear\", fontsize=12)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=list(y.reshape(-1)))\n",
    "\n",
    "X, y = generate_X_nonlinear()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Non-linear\", fontsize=12)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=list(y.reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be formatted a little different when we give it to Nielsen's code. Basically, it asks for (x, y) tuple pairs, so in each tuple you have a datapoint and it's label. Furthermore, both x and y must be vertical vectors **(if you don't undertstand why, you didn't complete Ex. 1.2.1 properly)**.\n",
    "\n",
    "The function below takes `X` and `y` and returns the data in this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y):\n",
    "    \"\"\"Format the dataset X and y so it fits with Nielsen's code.\"\"\"\n",
    "    return [\n",
    "        (X[i].reshape(-1, 1), y[i].reshape(-1, 1))  # our tuple (x, y)\n",
    "        for i in range(len(y))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.2.2**: Create a neural network called `net` with `sizes=[2, 1]`. This is actually just the basic Perceptron with two inputs and a single output (or a logistic regression with two variables). Generate 1e5 training datapoints and 1e4 testing datapoints using the `generate_X_linear` function. Once you've done that run the code below to train the network and to get predictions for the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.SGD(\n",
    "    training_data,\n",
    "    epochs=10,\n",
    "    mini_batch_size=10,\n",
    "    eta=1,\n",
    "    test_data=test_data\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = []\n",
    "for x in X_test:\n",
    "    y_pred.append(net.feedforward(x.reshape(-1, 1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Have a look at the predictions, e.g., print some of those values.  What do they look like? Are they just 0s and 1s? **Explain.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Once you've answered the question above, go ahead and run the code below to plot, side by side, the predictions versus the true labels.  **Comment on the differences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Predicted\")\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=list(y_pred))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Actual\")\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=list(y_test.reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.2.3**: Now we want to do the same thing as in Ex. 1.2.2, but with the data generated with the `generate_X_nonlinear` function. Comment on the result!\n",
    ">\n",
    ">*Hint: Think back to the lecture slides to figure out the size of the network you need in order to fit this kind of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/abjer/tsds/master/material_exercises/week_3/2_3_1_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.2.1**: The network above has a defined input and weights. If the true label for the datapoint `[4, 2]` is 1, what is the cost?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.2.2**: Knowing about backpropagation, we actually have everything we need to compute the gradients of the weights by hand. So go ahead and do that. Report your answer either as a diagram that includes the gradients (you can draw on my figure somehow and insert the resulting image), or just by writing what the gradient of each weight is.\n",
    ">\n",
    "> *Hint: When computing gradients with backprop, it can be a bit easier to think of the network as a computational graph. My computational graph looks like [this](https://github.com/abjer/tsds/blob/master/material_exercises/week_3/2_3_1_net_compgraph.png?raw=true).*\n",
    "> *Hint: Some of the gradients will become VERY small (remember the vanishing gradient problem?). If they get smaller than 1e-4 you can just set them to zero.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    "> **Ex. 3.1.2** Solve exercises 1.2.2 and 1.2.3 from week 1, but solve them using Keras.\n",
    "> Comment on differences in speed (and outcome if you observe any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.2.1** To understand overfitting, we will intentionally overfit some data! Let's fit a neural network to the following data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code given in week1 exercise 1.2.2, slightly modified to have some crossover points\n",
    "np.random.seed(2)\n",
    "def generate_X_linear(N=200):\n",
    "    X = np.vstack([\n",
    "        np.random.normal([-0.5, -0.5], 1, size=(int(N/2), 2)),\n",
    "        np.random.normal([1, 1], 1, size=(int(N/2), 2))\n",
    "    ])\n",
    "\n",
    "    y = np.array([0] * int(N/2) + [1] * int(N/2)).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "ntr=100\n",
    "nts=1e4\n",
    "x_train, y_train = generate_X_linear(N=ntr)\n",
    "x_test, y_test = generate_X_linear(N=nts)\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=[10,4])\n",
    "ax[0].scatter(x_train[:,0],x_train[:,1],color=['C'+str(i) for i in y_train.ravel()])\n",
    "ax[1].scatter(x_test[:,0],x_test[:,1],color=['C'+str(i) for i in y_test.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The theoretical optimal decision boundary here is a straight line, and since the clusters are overlapping there should be some error when fitting the model to the data. Try to reduce the error to 0 anyway (accuracy = 1) by adding layers of ``overfitter.add(Dense(200, activation = 'relu'))`` to the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfitter = Sequential()\n",
    "overfitter.add(Dense(200, input_dim=2, activation = 'relu'))\n",
    "overfitter.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "overfitter.compile(optimizer='adam',\n",
    "              loss=keras.losses.binary_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit the model to the training data\n",
    "history = overfitter.fit(x_train, y_train, epochs=300, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When you get acc=1, plot the test data and colour it using the predictions of your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = overfitter.predict_classes(x_test).ravel()\n",
    "fig,ax = plt.subplots(1,1, figsize=[5,5])\n",
    "ax.scatter(x_test[:,0],x_test[:,1],c=['C'+str(i) for i in pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now add some L2 regularization ( ``... activation = 'relu', kernel_regularizer=l2(0.05)) `` ) into that same overfitting network in order to get a linear decision boundary. Plot the test data colour it using your predictions as before. Comment on the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.2**: You input an image of dimensions $28 \\times 28 \\times 3$, use a padding of 2, a stride of 1,\n",
    "and then slide your $5 \\times 5 \\times 3$ filter across the image. What is the dimensionality of the resulting activation map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.3**: Let's say you now want to use a stride of 2, instead of 1. What problem does this immediately cause?\n",
    "How can we solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.5**: Implement Nielsen's [convolutional neural networks](http://neuralnetworksanddeeplearning.com/chap6.html#exercise_683491)\n",
    "(for this assignment only include the last network from the textbook, the one with 2 convolutional layers and dropout), and score an accuracy higher than 98%. The implementations don't have to be\n",
    "fully identical, but his solutions are pretty great, so getting close is a cheap way to score a high accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dis_py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "823ced02a4c06fdc704ca2ddc861142e9ba58a574b7efc036227e6c275405131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
