{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83865600},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"1ba70f","input":"","pos":6,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9b5401","input":"","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"fe0e0d","input":"","pos":8,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"67527a","input":"# Imports\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import fetch_california_housing, make_blobs\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\n","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"0a6bef","input":"def getBlobsWithNoise(plotting=False):\n    X, y = make_blobs(n_samples=2000, centers=4, random_state=0, cluster_std=0.4)\n    if plotting:\n        plt.figure(figsize=(8, 8))\n        plt.scatter(X[:, 0], X[:, 1], s=1)\n        plt.title('Blobs Data Without Noise')\n        plt.show()\n    np.random.seed(0)\n    X = np.vstack((X, np.random.uniform(low=-25, high=25, size=(200, 2))))\n    np.random.shuffle(X)\n    if plotting:\n        plt.figure(figsize=(8, 8))\n        plt.scatter(X[:, 0], X[:, 1], s=1)\n        plt.title('Blobs Data With Noise')\n        plt.show()\n    return X\n\nX = getBlobsWithNoise(plotting=True)\n\n","output":{"0":{"data":{"image/png":"cddb84a2ffb99f4f77ddd9c3a697d17543bbc7cf","text/plain":"<Figure size 576x576 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":481,"width":474},"needs_background":"light"},"output_type":"execute_result"},"1":{"data":{"image/png":"bbd1aef73aac22c417e8470694766d1672461b22","text/plain":"<Figure size 576x576 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":481,"width":488},"needs_background":"light"},"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"ee0a98","input":"df = pd.read_csv('http://cs.gettysburg.edu/~tneller/ds256/data/missing/sberbank.csv')\n\n","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"10afb3","input":"# More Blobs!\n\nBelow is a blobs dataset of 20,000 points with 200 random, dispersed points as outliers (some of which may fall into blob regions). Apply the techniques above to approximate the original blobs without outliers and compare their results.","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"39efdd","input":"# Data Cleaning: Outliers\n\nLearning Objectives:\n* Students will learn about sources of and classifications for outliers.\n* Students will practice common methods of identifying and dealing with outliers.\n* Students will apply different scaling techniques that are robust to outliers.\n\nReadings before class:\n* Sergio Santoya's [A Brief Overview of Outlier Detection Techniques](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561)\n* Wikipedia's [Anomoly Detection article section on \"Popular techniques\"](https://en.wikipedia.org/wiki/Anomaly_detection#Popular_techniques)\n* Sci-Kit Learn's article [Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)\n\nReference:\n* More anomaly detection demonstrations with visualization from Susan Li's [Anomaly Detection for Dummies](https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1)\n* An empirical study and online datasets for anomaly detection: Campos et el. [On the Evaluation of Unsupervised Outlier Detection: Measures, Datasets, and an Empirical Study](https://www.dbs.ifi.lmu.de/research/outlier-evaluation/) 2016.\n\nBefore class:\n* Read the readings listed above and fill out the \"TO-DO\" sections with your answers.\n\nIn class:\n* We will work through one outlier example together and solve the remaining example in small groups.\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"610b40","input":"## Homework\n","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"681ef2","input":"# Data Cleaning: Outliers\n\nAnother common task of data cleaning is the identification (i.e. \"detection\") of outliers in data.  This is also called _anomaly detection_.  Regardless of the name, outliers are values that are improbably different from other data so as to invite question of whether the data observations are valid or not.  As with missing values, a good understanding of the problem domain is important for determining both the validity of data and the proper handling of it.\n\n**TO-DO:** Copy the list of \"Most common causes of outliers on a data set\" from the Santoya article here and describe a specific plausible example of each.  The first is done for you.\n\nMost common causes of outliers on a data set:\n* Data entry errors (human errors) - example: a missing or extra decimal point in floating-point data\n\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"8bc11d","input":"## Categorical Outliers\n\nWith categorical data, suppose that you observe that 99% of observations have one of 10 categorical values, and the remaining 1% have 90 distinct values.  These 1% could be valid data entries and merely infrequent, and/or they could be typos of the other 10 values, errors in entry, or forms of missing values.  This underscores the importance of studying the data.  Possible ways to handle this scenario:\n\n* Delete observations with infrequent values.\n* Treat infrequent values as missing, or as a special \"Other category\".\n* Use the hashing trick to encode the categorical data with a significantly larger number of hashing values than 10 such that collisions are few.\n* Keep the data as-is.  Embrace the outliers and let them speak for themselves.\n* Convert all categorical values to have the same case (e.g. lowercase) and compute \"distances\" between each outlier value and each common value.  For example, the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) is the minimum number of character substitutions, insertions, or deletions needed to transform one string into another.  If an infrequent string value has a distance to a frequent string value below a chosen threshold, we may opt to assume it is a typo and correct it to the  frequent string value.\n\nCleaning categorical variable outliers often requires inspection of infrequent variables and normalization to a common representation, but one must be careful to know the knowledge domain to do this well.  An illustrative example comes to us from the mostly excellent notebook [\"EDA To Prediction (DieTanic)\"](https://www.kaggle.com/ash316/eda-to-prediction-dietanic/notebook):\n\n| Okay so there are some misspelled Initials like Mlle or Mme that stand for Miss. I will replace them with Miss and same thing for other values.\n\n| ```data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],```\n| ```                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'], inplace=True)```\n\nThe author assumes that \"Mlle.\" and \"Mme.\" are misspelled initial formal addresses to be correct to \"Miss\".  In fact, these are abbreviations for \"Mademoiselle\" and \"Madame\" which translate to \"Miss\" and \"Mrs.\", respectively.  For the dataset, this erroneous assumption throws away age information, because \"Miss\" and \"Mrs.\" carry different age distributions, and age mattered to survival rate.  Did French nationality as indicated by the French titles carry additional information that should have been retained?  Would a Lady or Countess be more likely to have a first class ticket on the Titanic, boosting the likelihood for predicted survival?\n\nThese are challenging questions that will be unique to each application, and this is why data cleaning can take so much time when done well.  Data Scientists seek to look deeply into the nature of the data and the problem domain in order to handle the data with respect.\n\nThat said, one can commit the opposite error.  One can obsess over each detail and give careful treatment to a handful of relatively unimportant observations.  A good practice can be to _try the simplest approach first_.  What if you spent hours giving careful consideration to missing and outlier values, engineering features, etc., only to find that a simple Naive Bayes Classifier on all complete, non-outlier data yielded an excellent model?\n\nThe [KISS Principle](https://en.wikipedia.org/wiki/KISS_principle) is a fundamental lesson of engineering.  KISS stands for \"Keep It Simple, Stupid\".  Related to minimalize concepts such as [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor), it is a call to keep one's design simple, elegant, and easy to understand. **Try the simple thing first and see if it works.**  If it works well, you have a model that is relatively easy to explain, apply, etc.\n\nThen, if better performance is needed or desired, one can drill down into the details that yield performance improvements.\n\nIn summary, categorical outliers can be handled in many ways.  A sensible approach would be to drop observations with outliers and see whether a good, simple model can be built without them, followed by closer inspection of infrequent values and special attention to the meaning of the entries in context.","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"904fca","input":"## DBSCAN\n\nFor DBSCAN, we don't need (or even want) data to resemble Gaussian distributions for clustering.  For this application, we'll use RobustScaler which will scale most data within a consistent range and leave extreme outliers as extreme.\n\n* Reload the 2D housing data as before.\n* Scatter plot the untransformed data.\n* Transform the data using the RobustScaler.\n* Scatter plot the transformed data.\n* Apply DBSCAN, tuning the epsilon parameter until there is a single cluster a similar number of \"noise\" points as removed outliers in the previous z-score example.\n* Remove the noise points from the transformed data.\n* Scatter plot the transformed data without the noise points.\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"a28001","input":"**Exercise 1:** Complete the in-class exercises if you haven't already.\n\n**Exercise 2:** Using your choice of outlier detection method, detect and remove outliers in the pair relationship of ```life_sq``` to ```price_doc``` in the Sberbank dataset.\n\n* Scatter plot the raw data.\n* Remove rows with missing values.\n* Scale just the 'life_sq' and 'price_doc' data with RobustScaler.\n* Scatter plot the scaled data.\n* Remove outliers and print the number of outliers.\n* Scatter plot the transformed data without predicted outliers.  Most of the plot area should depict three vertically-stretched areas of highest density with surrounding diffuse inliers.  If your plot has only one small high-density area, you're not removing enough outliers.\n","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"a6715e","input":"# In Class\n\nIn class, you will apply the three focus techniques of Sergio Santoya's [A Brief Overview of Outlier Detection Techniques](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561) on the data of Sci-Kit Learn's article [Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html).  Before each section, review the reading on the technique.\n\n## z-score\nFirst, we consider the z-score method.  Since the z-score method assumes Gaussian (i.e. normal) distributions, it is helpful to apply a Scaler that scales and transforms the data to a more Gaussian distribution. Both PowerTransformer and QuantileTransformer (Gaussian output) fit this description.\n\n* Load the 2D housing data (using median income, number of households) as shown in the sklearn article.\n* Scatter plot the untransformed data.\n* Transform the data using the PowerTransformer.\n* Scatter plot the transformed data.\n* For each of the common 2.5, 3, 3.5 z-score thresholds, print how many values in each column would be considered outliers.  You'll notice from your printed output and from the graphs in Sci-Kit Learn's article [Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) that one dimension (column) of the data in particular has significant outliers.\n* Use the z-score threshold of 3.5 for that column only to remove outlier rows from the dataset. Print the data shape before and after.\n* Scatter plot of the points remaining to verify the removal of outliers.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"b23f20","input":"(end of homework)","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"d79142","input":"## Isolation Forests\n\n* Load the 2D housing data (using median income, number of households) as shown in the sklearn article.\n* Scatter plot the untransformed data.\n* Transform the data using the RobustScaler.\n* Scatter plot the transformed data.\n* Use IsolationForest to compute an anomaly score for each data point and print the number of outliers.  Use the default contamination parameter. Print the number of outliers.\n* Remove outliers predicted by IsolationForest.\n* Scatter plot the transformed data without predicted outliers.\n* Observe the significant difference from the DBSCAN result.  Now go back and tune the contamination parameter so as to get a distribution similar to (but not the same as) DBSCAN's.  Note what differs in the two distributions.","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"eadfb4","input":"## Numerical Outliers\n\nMost techniques concerning outliers assume numerical data, because categorical data can be converted to numerical data through one-hot encoding and the hashing trick.\n\n**TO-DO:** List the three techniques the Santoya article goes in depth on and create a link to the documentation for the Python library documentation associated with each:\n\n_(Create your list of three techniques and associated Python documentation links here)_\n\nIn addition, Wikipedia's [Anomoly Detection article section on \"Popular techniques\"](https://en.wikipedia.org/wiki/Anomaly_detection#Popular_techniques) provides an even more extensive list of techniques for \"anomaly detection\", which is often used as a synonym for \"outlier identification\".  \n\n### Scaling with Numeric Outliers\n\nFor any technique that uses a distance measure (e.g. k-NN, DBSCAN, etc.), it is important to scale the data using a technique _that is robust to outliers_ unless one has already removed the outliers.\n\n**TO-DO:** From a careful read of Sci-Kit Learn's article [Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html), create a bulleted list below of the sklearn Scaler objects that are robust to extreme outliers.  For each, create a link to their documentation and summarize how it is distinct from the others.  (To see which are robust to extreme outliers, look at the numeric range of the vertical dimension's zoomed range after scaling.  Values much smaller than the horizontal range indicate a vulnerability to extreme outliers for this dataset.)\n\n_(Create your descriptive lists of Scalers that are robust to extreme numerical outliers here.)_\n","pos":4,"type":"cell"}
{"id":0,"time":1605383292115,"type":"user"}
{"last_load":1605383208154,"type":"file"}