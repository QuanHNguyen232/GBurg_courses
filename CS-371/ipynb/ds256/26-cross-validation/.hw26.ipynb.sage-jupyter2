{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":66801664},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"20e502","input":"","pos":29,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"36ba1e","input":"","pos":33,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"38cc49","input":"","pos":25,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6669bb","input":"","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b63bb0","input":"","pos":31,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e74cc4","input":"","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"e69529","input":"# Imports\n\nimport math\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport seaborn as sns\nfrom sklearn.datasets import load_iris, make_classification\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, validation_curve, GridSearchCV\n  # Note that all imports from model_selection but train_test_split have moved there since VanderPlas' text was published.\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Set random seed for reproducibility\nrandom.seed(0)\nnp.random.seed(0)","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"36ca44","input":"def make_data(N, err=.5, rseed=0):\n    # randomly sample the data\n    rng = np.random.RandomState(rseed)\n    X = rng.rand(N, 1) ** 2\n    print(type(X))\n    y = math.pi - np.sin(15 * X) + 5 * X + err * rng.randn(N, 1)\n    return X, y\n\nX, y = make_data(40)\n\nX_test = np.linspace(-0.1, 1.1, 500)[:, None]\n\nplt.scatter(X.ravel(), y, color='black')\naxis = plt.axis()\nfor degree in [2, 4, 8, 16]:\n    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\nplt.xlim(-0.1, 1.0)\nplt.ylim(-2, 12)\nplt.legend(loc='best');","output":{"0":{"name":"stdout","output_type":"stream","text":"<class 'numpy.ndarray'>\n"},"1":{"data":{"image/png":"2b4e7c8ac3c20a3b4988328d36dac88842f84253","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":10,"metadata":{"image/png":{"height":415,"width":713},"needs_background":"light"},"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"6f8bff","input":"df = pd.read_csv('http://cs.gettysburg.edu/~tneller/ds256/data/hw26/hw26.csv')\n","pos":36,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"8e250c","input":"# Load iris data and split into 70% train, 15% val, 15% test.\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=1)  # 70% for \"train\", 30% for \"val\" and \"test\" combined\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, shuffle=True, random_state=1)  # 50% * 30% = 15% each for \"val\" and \"test\"\nprint('For train, val, and test, our partition sizes are {}, {}, and {}, respectively.'.format(len(y_train), len(y_val), len(y_test)))","output":{"0":{"name":"stdout","output_type":"stream","text":"For train, val, and test, our partition sizes are 105, 22, and 23, respectively.\n"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"764e44","input":"def get_accuracy(k):  # get validation accuracy\n    # Build the k-NN classifier\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    # Compute predictions\n    return accuracy_score(y_val, model.predict(X_val))  # <-- NOTE: We're using \"val\" sets here now!\n\nks = list(range(1, 21))\naccuracies = [get_accuracy(k) for k in ks]\nbest_idx = np.argmax(accuracies)\nbest_k, best_accuracy = ks[best_idx], accuracies[best_idx]\nprint('For the iris data, k-NN has the best validation accuracy of {} for k = {}.'.format(best_accuracy, best_k))","output":{"0":{"name":"stdout","output_type":"stream","text":"For the iris data, k-NN has the best validation accuracy of 1.0 for k = 7.\n"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"76a6ea","input":"# Build the k-NN classifier\nmodel = KNeighborsClassifier(n_neighbors=best_k)  # build our best model with the hyperparameter found with the validation set\nmodel.fit(X_train, y_train)\n# Compute predictions\nprint('Test accuracy:', accuracy_score(y_test, model.predict(X_test)))  # <-- NOTE: We're using \"test\" sets here now just once.","output":{"0":{"name":"stdout","output_type":"stream","text":"Test accuracy: 0.9565217391304348\n"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"a1bfef","input":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, random_state=1)  # 85% for \"train\", 15% for \"test\"\n\nbest_k = 1\nbest_accuracy = 0\nfor k in ks:\n    model = KNeighborsClassifier(n_neighbors=k)\n    avg_accuracy = np.mean(cross_val_score(model, X_train, y_train, cv=5))\n    if avg_accuracy > best_accuracy:\n        best_accuracy = avg_accuracy\n        best_k = k\nprint('For the iris data, k-NN has the best 5-fold cross-validation accuracy of {} for k = {}.'.format(best_accuracy, best_k))","output":{"0":{"name":"stdout","output_type":"stream","text":"For the iris data, k-NN has the best 5-fold cross-validation accuracy of 0.976 for k = 8.\n"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"76efe4","input":"best_k = 1\nbest_accuracy = 0\nfor k in ks:\n    model = KNeighborsClassifier(n_neighbors=k)\n    avg_accuracy = np.mean(cross_val_score(model, X_train, y_train, cv=LeaveOneOut()))  # Note that LeaveOneOut now takes no arguments.\n    if avg_accuracy > best_accuracy:\n        best_accuracy = avg_accuracy\n        best_k = k\nprint('For the iris data, k-NN has the best leave-one-out cross-validation accuracy of {} for k = {}.'.format(best_accuracy, best_k))","output":{"0":{"name":"stdout","output_type":"stream","text":"For the iris data, k-NN has the best leave-one-out cross-validation accuracy of 0.9763779527559056 for k = 10.\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"ea604b","input":"# Build the k-NN classifier\nmodel = KNeighborsClassifier(n_neighbors=best_k)  # build our best model with the hyperparameter found with leave-one-out validation.\nmodel.fit(X_train, y_train)\n# Compute predictions\nprint('Test accuracy:', accuracy_score(y_test, model.predict(X_test)))","output":{"0":{"name":"stdout","output_type":"stream","text":"Test accuracy: 0.9565217391304348\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"4af0e1","input":"# Generate classification dataset\nX, y = make_classification(flip_y=.2, random_state=26)\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"RandomForestClassifier(n_estimators=10, max_depth=3, random_state=0)\nTest accuracy: 0.8666666666666667\n"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"9cb537","input":"def PolynomialRegression(degree=2, **kwargs):\n    return make_pipeline(PolynomialFeatures(degree),\n                         LinearRegression(**kwargs))","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"02ae02","input":"Next we'll see what happens with the validation curves if we have 5 times as much data, just like in the text:","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"0f6372","input":"For small datasets like Iris, one can take this concept to the extreme, performing $n$-fold cross-validation for a dataset of size $n$.  This means that we build $n$ models from $n$ - 1 training examples and validate with a _single example_.  This computationally intensive approach, only suitable for small datasets, is called \"leave-one-out\" and looks like this:","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"11c152","input":"(end of homework)","pos":37,"type":"cell"}
{"cell_type":"markdown","id":"13cbe4","input":"With more data, we don't seem to suffer from overfitting as easily.  Still, we prefer the simplest degree polynomial that has a good score.  Find the best polynomial degree with this larger data set and plot it against the data scatterplot.","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"22f3b0","input":"## $k$-Fold Cross-Validation\n\nThere are other techniques for validation to consider.  Suppose, a Jake VanderPlas illustrates in this figure, that we split the data into five equal portions and have each fifth serve as the validation set in each of 5 model-building and validation iterations:\n\n<img src=\"https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-5-fold-CV.png\" alt=\"5-fold cross-validation\" title=\"5-fold cross-validation\" width=\"600\" />\n\nThis is what is known as 5-fold cross-validation.  We illustrate this approach by repartitioning our data into only \"train\" and \"test\", allowing our sklearn Python tools to perform the partitioning and testing for us.  Note the different Python code structure we use to find our best $k$.","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"292477","input":"# Training, Validation, and Testing\n\nThus far in the course, for simplicity (and mirroring common practice), we have split data into two training and testing sets.  However, like many, we haven't always used the testing set as it truly should be used, as a final unbiased assessment of the expected performance of our model.  We have instead made use of the testing set to tune hyperparameters (e.g. finding the best $k$ for $k$-nearest-neighbor).  This is the proper role of a _validation set_.  We've been occasionally using our testing set as a validation set.\n\nWhy shouldn't we?  The simple answer is that the use of the testing set to tune hyperparameters of our models can result in a subtle bias where we may, through hyperparameter tuning, learn something of our testing set and no longer get an unbiased assessment of expected performance.\n\n(Tangent on the **difference between _parameters_ and _hyperparameters_**:  To the Computer Scientist, a _parameter_ is a value passed to an object or function.  To the Data Scientist, a _parameter_ is an automatically learned value of the model, such as linear regression intercept and coefficients or neural network weights, whereas a _hyperparameter_ is a manually set value that is passed to the model to customize learning of the model _parameters_.  As a result of the naming overlap, we will often use _parameter_ and _hyperparameter_ interchangeably. After all, a Python model hyperparameter in Data Science terminology is also a Python object constructor or function parameter in Computer Science programming language terminology.)\n\nLet us reconsider Jason Brownlee's list of definitions for training, validating, and testing dataset as we prepare to shift our thinking on them:\n\n* **Training Dataset**: The sample of data used to fit the model.\n* **Validation Dataset**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n* **Test Dataset**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n\nWhat this practically means is that we should\n* split our data into 3 partitions (\"train\"/\"val\"/\"test\"),\n* iteratively build our models with the \"train\" data for different hyperparameters, seeking to optimize error scores observed when applying the \"train\" model for each hyperparameter choice to the \"val\" data,\n* rebuild our best \"train\" model using its best hyperparameters, and\n* perform a final evaluation of this model with the \"test\" data.\n\nNote that the \"test\" data is ideally used _only once_ for final evaluation.","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"2991f1","input":"Visualize the validation curve with 7-fold cross-validation.","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"2c73d2","input":"We'll turn our attention now to grid search cross validation that will allow us to automatically optimize hyperparameters.  Perform the same grid search process you see in the VanderPlas text.","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"453b66","input":"## Homework\n","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"7fbbcb","input":"Compare the new validation curve to the old for this larger dataset.","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"81b633","input":"## Revisiting the Iris Example\n\nSo how to we get three sets when train_test_split only divides into two sets of X's and y's?  We apply the split twice.  Suppose we use the typical 70% / 15% / 15% split for \"train\" / \"val\" / \"test\", respectively:\n","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"894da5","input":"Pick the best degree according to the validation curve and plot it over the scatterplot of the data.","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"8d9624","input":"Next, we'll create a dataset that's not polynomial, but that we'll seek to fit with successive degree polynomials of degree 2, 4, 8, and 16.","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"a97dad","input":"Now, we'll use the validation set to help us tune $k$ for $k$-Nearest Neighbor classification.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"aea85b","input":"Note that our test accuracy is lower.  This isn't surprising.  In fact, this is the _point_.  If we tune hyperparameters to our validation set, we expect to have optimized our accuracy for validation.  Holding out a test set for final evaluation means we get an _unbiased_ assessment of our model's quality.  The validation set isn't a true test of how our model will perform on unseen data because we see our performance with the validation set again and again and we bias our model hyperparameters consciously or unconsciously to get a best fit to the validation set.","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"bfc39d","input":"## In Class\n\nIn class, we will replicate Jake VanderPlas' approach to polynomial regression to demonstrate underfitting, overfitting, and hyperparameter grid search, but we will _skip_ the learning curve portion.\n\nChanges since the text was published:  Not only have many of the classes used here moved to the sklearn.model_selection package, but there is a syntactic difference as well.  Whereever you see pipeline parameters \"'polynomialfeatures__degree', degree,\", change the code to \"param_name='polynomialfeatures__degree', param_range=degree,\".  Also, omit the \"hold=True\" parameter in the final grid search fit.\n\nFirst, we create the pipeline for polynomial regression.  This creates a process where features are processed automatically into polynomial terms according to the given degree.  (It's much easier this way, isn't it!  But it is limited to polynomials, and it was important to understand earlier how to do arbitrary nonlinear regression with linear regression.)  After that processing in the pipeline, these preprocessed polynomial features and all other keyword arguments (kwargs) are passed along to the LinearRegression model for modeling.","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"cec320","input":"## TO-DO: Pre-class Exercise\n\nSplit off 15% of the dataset below as a test dataset.  Use the remainder with some form of cross-validation in order to tune hyperparameters for some of the classifiers you've come to understand in this course.  How high an accuracy can you achieve with cross-validation?\n\nOnce you've settled on a best model with best hyperparameters, compute accuracy _only once_ with the test dataset.","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"e238c0","input":"**Exercise 1:** Complete the in-class exercises if you haven't already.\n\n**Exercise 2:**  Compute a model with $R^2$ score greater than 0.8 with the dirty DataFrame loaded below using the data cleaning methods of previous classes and cross-validation with a suitable regression method.  Only test your $R^2$ with **30% testing data** once at the end of your code.\n","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"eba08d","input":"This last leave-one-out approach indicates highest accuracy among our three attempts at hyperparameter tuning, but also notice that it is the approach that gets to use the _largest fraction of the non-test data_ for model building.  As it turns out, our choice of k is not so critical for this very small dataset:","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"ec65de","input":"# Cross-Validation\n\nLearning Objectives:\n* Students will partition data into training, validation, and testing sets, and understand the rationale for such partitioning.\n* Students will learn of and practice cross-validation for hyperparameter tuning.\n* Students will increase experiential understanding of bias-variance tradeoffs and under/overfitting through the visualization of training/validation curves.\n\nReadings before class:\n* Jason Brownlee's [What is the Difference Between Test and Validation Datasets?](https://machinelearningmastery.com/difference-test-validation-datasets/)\n* Jake VanderPlas's [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/index.html) section on [Hyperparameters and Model Validation](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html)\n\nBefore class:\n* Read the readings listed above, study the notebook below, and apply cross-validation to the \"TO-DO\" exercise below.\n\nIn class:\n* We will complete the section marked \"In Class\" together.\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"ee7b66","input":"Finally, we test our final model against the \"test\" data.","pos":7,"type":"cell"}
{"id":0,"time":1605877349878,"type":"user"}
{"last_load":1605689556882,"type":"file"}