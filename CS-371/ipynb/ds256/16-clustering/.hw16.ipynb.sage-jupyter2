{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":275292160},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1602663015184,"exec_count":1,"id":"e17fe2","input":"# Imports\n\nimport math\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\nimport pandas as pd\nimport math\nimport random\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n","kernel":"python3","no_halt":true,"pos":2,"start":1602663010906,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663015629,"exec_count":2,"id":"e11a81","input":"my_seed = 0\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\n\nX, y = make_blobs(n_samples=1000, centers=4,\n                  random_state=my_seed, cluster_std=0.4)\n\n# As with k-nearest neighbor classification/regression where distance metrics matter,\n# we need to normalize the data.  One sklearn tool from this is the StandardScaler:\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# StandardScaler makes the data have mean 0 and standard deviation 1 in each dimension.\n# We could call just the transform functionin order to retain and reuse the same scaler transformation in the future without fitting it to the new given data.\n\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n","kernel":"python3","no_halt":true,"output":{"0":{"data":{"image/png":"4e2ba51e53fc61d82a58aabf9ac557bb2f08f1f8","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":357,"width":380},"needs_background":"light"}}},"pos":4,"start":1602663015198,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663016100,"exec_count":3,"id":"adabf2","input":"kmeans = KMeans(n_clusters=4, random_state=0).fit(X)\nprint('First 20 X Labels:', kmeans.labels_[:20])\nprint('Cluster Centers:\\n', kmeans.cluster_centers_)\ny_clusters = kmeans.predict(X)\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y_clusters, s=50, cmap='rainbow');\n","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"First 20 X Labels: [2 1 2 0 3 1 1 3 1 1 2 3 0 2 3 0 0 2 1 3]\nCluster Centers:\n [[-1.00747345 -0.41813065]\n [ 0.58147614  0.1229734 ]\n [-0.85966395  1.50280844]\n [ 1.28566127 -1.20765119]]\n"},"1":{"data":{"image/png":"ad5f3a4297b7fdae0f36eabed0fe042e05fd4071","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":357,"width":380},"needs_background":"light"}}},"pos":6,"start":1602663015640,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663016557,"exec_count":4,"id":"1618ce","input":"kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\nprint('First 20 X Labels:', kmeans.labels_[:20])\nprint('Cluster Centers:\\n', kmeans.cluster_centers_)\ny_clusters = kmeans.predict(X)\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y_clusters, s=50, cmap='rainbow');","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"First 20 X Labels: [0 1 0 2 1 1 1 1 1 1 0 1 2 0 1 2 2 0 1 1]\nCluster Centers:\n [[-0.85966395  1.50280844]\n [ 0.94136848 -0.54787407]\n [-0.99213693 -0.40927785]]\n"},"1":{"data":{"image/png":"2ec4591a3c007096a92d2528a0f053c1f18e6235","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":357,"width":380},"needs_background":"light"}}},"pos":8,"start":1602663016110,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663017089,"exec_count":5,"id":"c736fc","input":"def get_WCSS(X, kmeans):\n    centroids = kmeans.cluster_centers_[kmeans.labels_]\n    diffs = X - centroids\n    return sum(sum(diffs * diffs))\n\nk_values = [1, 2, 3, 4, 5, 6, 7]\nWCSS_values = [get_WCSS(X, KMeans(n_clusters=k, random_state=0).fit(X)) for k in k_values]\nprint('k values:', k_values)\nprint('WCSS values:', WCSS_values)","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"k values: [1, 2, 3, 4, 5, 6, 7]\nWCSS values: [1999.9999999999993, 831.0630206874047, 369.6129536092786, 87.03135159690099, 75.66652075634391, 65.34345833652259, 55.551167807151906]\n"}},"pos":10,"start":1602663016570,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663017616,"exec_count":6,"id":"02c2c5","input":"X_uniform = np.random.uniform(size=(1000, 2))\nX_uniform = StandardScaler().fit_transform(X_uniform)\nWCSS_values_uniform = [get_WCSS(X_uniform, KMeans(n_clusters=k, random_state=0).fit(X_uniform)) for k in k_values]\nprint('k values:', k_values)\nprint('Uniform WCSS values:', WCSS_values_uniform)","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"k values: [1, 2, 3, 4, 5, 6, 7]\nUniform WCSS values: [2000.0000000000011, 1233.2920691906415, 761.9117625641154, 480.31461559348395, 402.70409460086773, 343.09942197756357, 289.76477646289015]\n"}},"pos":12,"start":1602663017098,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663018655,"exec_count":7,"id":"e7931e","input":"plt.figure(figsize=(6, 18))\n\nplt.subplot(3, 1, 1)\nplt.plot(k_values, WCSS_values, color='orange');\nplt.plot(k_values, WCSS_values_uniform, color='blue');\nplt.title('Behavior of WCSS and log(WCSS) as k increases for blob (orange) and uniform (blue) data')\nplt.ylabel('WCSS')\n\nplt.subplot(3, 1, 2)\nplt.plot(k_values, np.log(WCSS_values), color='orange');\nplt.plot(k_values, np.log(WCSS_values_uniform), color='blue');\nplt.ylabel('log(WCSS)')\n\n\nplt.subplot(3, 1, 3)\nplt.plot(k_values, np.log(WCSS_values_uniform) - np.log(WCSS_values), color='green');\nplt.ylabel('log differences')\nplt.xlabel('k')\n\nplt.show()\n","kernel":"python3","no_halt":true,"output":{"0":{"data":{"image/png":"a07c3fcda1d637442a64797149a9a32d36a2c391","text/plain":"<Figure size 432x1296 with 3 Axes>"},"metadata":{"image/png":{"height":1038,"width":556},"needs_background":"light"}}},"pos":14,"start":1602663017653,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663019346,"exec_count":8,"id":"934f98","input":"# Define a visualization according to https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py:\n\ndef visualize_DBSCAN(X, dbscan):\n    plt.figure(figsize=(6, 6))\n    core_samples_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_samples_mask[dbscan.core_sample_indices_] = True\n    labels = dbscan.labels_\n\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n\n    # Black removed and is used for noise instead.\n    unique_labels = set(labels)\n    colors = [plt.cm.Spectral(each)\n              for each in np.linspace(0, 1, len(unique_labels))]\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            # Black used for noise.\n            col = [0, 0, 0, 1]\n\n        class_member_mask = (labels == k)\n\n        xy = X[class_member_mask & core_samples_mask]\n        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n                 markeredgecolor='k', markersize=14)\n\n        xy = X[class_member_mask & ~core_samples_mask]\n        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n                 markeredgecolor='k', markersize=6)\n\n    plt.title('Estimated number of clusters: %d' % n_clusters_)\n    plt.show()\n\n    print('Estimated number of noise points: %d' % n_noise_)\n\n\n# Compute DBSCAN and visualize:\ndbscan = DBSCAN(eps=0.3, min_samples=10).fit(X)\nvisualize_DBSCAN(X, dbscan);","kernel":"python3","no_halt":true,"output":{"0":{"data":{"image/png":"8e7601514dd18772705b21f3f9b10cc25a94231c","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}},"1":{"name":"stdout","text":"Estimated number of noise points: 0\n"}},"pos":17,"start":1602663018676,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663019911,"exec_count":9,"id":"e28cd1","input":"epsilon = 1\nmin_points = 10\nprint('Epsilon: {}, Minimum points: {}'.format(epsilon, min_points))\ndbscan = DBSCAN(eps=epsilon, min_samples=min_points).fit(X)\nvisualize_DBSCAN(X, dbscan)\n\nepsilon = .01\nmin_points = 10\nprint('Epsilon: {}, Minimum points: {}'.format(epsilon, min_points))\ndbscan = DBSCAN(eps=epsilon, min_samples=min_points).fit(X)\nvisualize_DBSCAN(X, dbscan)","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Epsilon: 1, Minimum points: 10\n"},"1":{"data":{"image/png":"095ec987d6d35d58fb438277e98dde786306c0e7","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}},"2":{"name":"stdout","text":"Estimated number of noise points: 0\nEpsilon: 0.01, Minimum points: 10\n"},"3":{"data":{"image/png":"0f981cc05dcb805253ef2688e0cde14eb05889a8","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}},"4":{"name":"stdout","text":"Estimated number of noise points: 1000\n"}},"pos":19,"start":1602663019360,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663020629,"exec_count":10,"id":"acdfb7","input":"epsilon = .3\nmin_points = 200\nprint('Epsilon: {}, Minimum points: {}'.format(epsilon, min_points))\ndbscan = DBSCAN(eps=epsilon, min_samples=min_points).fit(X)\nvisualize_DBSCAN(X, dbscan)\n\nepsilon = .3\nmin_points = 1\nprint('Epsilon: {}, Minimum points: {}'.format(epsilon, min_points))\ndbscan = DBSCAN(eps=epsilon, min_samples=min_points).fit(X)\nvisualize_DBSCAN(X, dbscan)","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Epsilon: 0.3, Minimum points: 200\n"},"1":{"data":{"image/png":"0f981cc05dcb805253ef2688e0cde14eb05889a8","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}},"2":{"name":"stdout","text":"Estimated number of noise points: 1000\nEpsilon: 0.3, Minimum points: 1\n"},"3":{"data":{"image/png":"b89c0c95734a671848ac31f081a03e5590cee069","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}},"4":{"name":"stdout","text":"Estimated number of noise points: 0\n"}},"pos":21,"start":1602663019923,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663021829,"exec_count":11,"id":"e6178a","input":"k = 4\n\nmodel = AgglomerativeClustering(n_clusters=k).fit(X)\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], c=model.labels_, s=50, cmap='rainbow')\nplt.title('Ward linkage (default), k = ' + str(k))\nplt.show()\n\nmodel = AgglomerativeClustering(linkage='average', n_clusters=k).fit(X)\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], c=model.labels_, s=50, cmap='rainbow')\nplt.title('Average linkage, k = ' + str(k))\nplt.show()\n\nmodel = AgglomerativeClustering(linkage='complete', n_clusters=k).fit(X)\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], c=model.labels_, s=50, cmap='rainbow')\nplt.title('Complete (maximum) linkage, k = ' + str(k))\nplt.show()\n\nmodel = AgglomerativeClustering(linkage='single', n_clusters=k).fit(X)\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], c=model.labels_, s=50, cmap='rainbow')\nplt.title('Single linkage, k = ' + str(k))\nplt.show()","kernel":"python3","no_halt":true,"output":{"0":{"data":{"image/png":"397cfe87d2ff31f927739e25426ffff0713afec3","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}},"1":{"data":{"image/png":"7d1fbaba951b46cf49690f1e04c7d8cf5e6fe84e","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}},"2":{"data":{"image/png":"53da74906e1a0458baf8e1276fbc7bb330963e9f","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}},"3":{"data":{"image/png":"7b61178fcc522ebdf9cc2dd27a9a769baf975f6b","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}}},"pos":24,"start":1602663020649,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663021847,"exec_count":12,"id":"baa1b9","input":"# Load ic16-1.csv into a DataFrame df and print a description and head to get a sense of the 2D data.\n\n\n\n# Use X = df.values to get a numpy array of the loaded data.  Use StandardScaler to normalize the data.\n\n\n\n# Scatter plot the 2D data\n\n\n\n# Plot cluster-colored scatter plots and print cluster centers showing the results of k-means clustering for k = 3, 4, 5.\n# Each plot title should indicate the use of k-means clustering and specify the k for the plot,\n# e.g. \"k-Means Clustering with k = 3\"\n\n\n\n# Now apply DBSCAN with default setting and plot the results as a cluster-colored scatter plot along with the estimated number of noise points.\nprint('\\n\\nDBSCAN Default')\n\n\n\n# Apply DBSCAN with three different values of epsilon (and default min_samples), with one epsilon too high, one too low, and one just right.\n# Visualize and print number of noise points as before.\nprint('\\n\\nDBSCAN epsilon = ' + str(1.0)) # Sample print statement before plot\n\n\n\n# Now apply DBSCAN with two different values of min_samples (and \"just right\" eps from the previous attempt),\n# such that the first has a < 10 noise points and the second has none.\n# Visualize and print number of noise points as before.\n\n\n\n# Apply agglomerative hierarchical clustering with k = 4 for each of the four linkage options.\n# Use default parameter values except for n_clusters and linkage.\n# Plot the results of each as done previously.\n# Print which works best and why.\nprint(\"\\n\\nAgglomerative Hierachical Clustering:\")\n\n","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"\n\nDBSCAN Default\n\n\nDBSCAN epsilon = 1.0\n\n\nAgglomerative Hierachical Clustering:\n"}},"pos":26,"start":1602663021840,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663027762,"exec_count":13,"id":"22c262","input":"import cv2\n\n# specify image filename and dataset size\nfilename = 'smiley.jpg'\nnum_points = 1000\n\n# Read and show original image\nimg = cv2.imread(filename)\nRGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(6, 6))\nplt.imshow(RGB_img)\n\n# Convert image into cluster dataset\n\n# Adapted from https://pythonexamples.org/python-opencv-convert-image-to-black-and-white/\n# read image\nimg_grey = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n\n# define a threshold (128 is the middle of black and white in grey scale)\nthresh = 128\n\n# threshold the image\nimg_binary = cv2.threshold(img_grey, thresh, 255, cv2.THRESH_BINARY)[1]\n\n# grab the image dimensions\nh = img_binary.shape[0]\nw = img_binary.shape[1]\n\n# loop over the image, pixel by pixel, and take a random sample of desired size\nrandom.seed(0)\nb_pts = [[x, -y] for y in range(0, h) for x in range(0, w) if img_binary[y, x] < thresh]\nb_pts = random.sample(b_pts, num_points)\ndf = pd.DataFrame(data=b_pts, columns=['x1', 'x2'])\ndf.index.name = 'id'\n\n# Now you can perform the same experimentation, but change your image to a different one with dark/light pixels indicating cluster/no-cluster regions.\n\n# Use X = df.values to get a numpy array of the loaded data.  Use StandardScaler to normalize the data.\n\nX = df.values\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Scatter plot the 2D data\n\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], s=50, cmap='rainbow')\nplt.show()\n\n# TO-DO: Change the input image from smiley.jpg to one of your own that defines different shaped clusters.\n# Then practice and experiment with it.\n\nepsilon = 0.3\nprint('\\n\\nDBSCAN epsilon = ' + str(epsilon))\ndbscan = DBSCAN(eps=epsilon).fit(X)\nvisualize_DBSCAN(X, dbscan);","kernel":"python3","no_halt":true,"output":{"0":{"data":{"image/png":"013401dee9461bcf9e0a5e3cd4b80368e0bb1749","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":360,"width":364},"needs_background":"light"}},"1":{"data":{"image/png":"1971cc68733e69c1d0bb9a1d2d41d98241ed6dff","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":357,"width":380},"needs_background":"light"}},"2":{"name":"stdout","text":"\n\nDBSCAN epsilon = 0.3\n"},"3":{"data":{"image/png":"e818025d6602b7bec573c784e793538e550040e4","text/plain":"<Figure size 432x432 with 1 Axes>"},"metadata":{"image/png":{"height":372,"width":380},"needs_background":"light"}},"4":{"name":"stdout","text":"Estimated number of noise points: 0\n"}},"pos":28,"start":1602663021966,"state":"done","type":"cell"}
{"cell_type":"code","end":1602663027778,"exec_count":14,"id":"f4460f","input":"# Load http://cs.gettysburg.edu/~tneller/ds256/data/hw16/hw16-1.csv into a DataFrame df and print a description and head to get a sense of the 2D data.\n\n\n\n# Use X = df.values to get a numpy array of the loaded data.  Use StandardScaler to normalize the data.\n\n\n\n# Scatter plot the 2D data\n\n\n\n# Plot cluster-colored scatter plots and print cluster centers showing the results of k-means clustering for k = 5, 10, and 15.\n# Each plot title should indicate the use of k-means clustering and specify the k for the plot,\n# e.g. \"k-Means Clustering with k = 5\"\n\n\n\n# Now apply DBSCAN with default setting and plot the results as a cluster-colored scatter plot along with the estimated number of noise points.\nprint('\\n\\nDBSCAN Default')\n\n\n\n# Apply DBSCAN with three different values of epsilon (and default min_samples), with one epsilon too high, one too low, and one just right.\n# Your \"just right\" value should estimate 10 clusters.\n# Visualize and print number of noise points as before.\nprint('\\n\\nDBSCAN epsilon = ' + str(1.0)) # Sample print statement before plot\n\n\n\n# Now apply DBSCAN with two different values of min_samples (and \"just right\" eps from the previous attempt),\n# such that the first has a < 10 noise points and the second has none.\n# Visualize and print number of noise points as before.\n\n\n\n# Apply agglomerative hierarchical clustering with k = 10 for each of the four linkage options.\n# Use default parameter values except for n_clusters and linkage.\n# Plot the results of each as done previously.\n# Print which works best and why.\nprint(\"\\n\\nAgglomerative Hierachical Clustering:\")\n\n","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"\n\nDBSCAN Default\n\n\nDBSCAN epsilon = 1.0\n\n\nAgglomerative Hierachical Clustering:\n"}},"pos":31,"start":1602663027772,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"2098de","input":"### Hierarchical Clustering\n\nHierarchical clustering algorithms are generally one of two types:\n* _Divisive_ clustering starts with all points in a single cluster and iteratively divides one cluster into two until the desired number of clusters is reached.\n* _Agglomerative_ clustering starts with all points being a singleton cluster and iteratively merges two clusters into one until the desired number of clusters is reached.\n\nHere we focus on agglomerative clustering, and ```sklearn``` offers four different _linkage_ criteria to choose from in order to decide which two clusters to merge in a given iteration.  The linkage options are called \"ward\" (the default), \"complete\", \"average\", and \"single\".  Think of each of these as a distance metric between clusters.  On each iteration, agglomerative clustering merges two clusters with minimum distance according to the chosen metric.  Here is a description of each from the documentation:\n\n* ```ward``` minimizes the variance of the clusters being merged.\n* ```average``` uses the average of the distances of each observation of the two sets.\n* ```complete``` or maximum linkage uses the maximum distances between all observations of the two sets.\n* ```single``` uses the minimum of the distances between all observations of the two sets.\n\nPut another way, ```ward``` minimizes $k$-means clustering WCSS divided by the number of points in the combined clusters.\nGiven two clusters, $A$ and $B$, the other methods look at all point pairs $(a, b)$ where $a \\in A$ and $b \\in B$, and consider average, maximum, and minimum distances over pairs.  For example, ```single linkage``` will merge two clusters that have a pair of points closest to one another.\n\nEach of these strategies has strengths and weaknesses.  Hierarchical clustering is often applied in bioinformatics to discovering structure in genetic similarities.\n\nLike $k$-means clustering, we need to supply the desired number of clusters $k$.  We will now apply this to the same blobs data:","pos":23,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4905a8","input":"\n# Supervised Versus Unsupervised Learning\n\n* **Supervised Learning** – Given training input and output (x, y) pairs, learn a function $y = \\hat{f}(x)$ to approximate an unknown function $y = f(x)$.\n* **Unsupervised Learning** - Given only training input, learn structural information about the data.\n\n\n## Clustering\n\nOne common unsupervised learning problem is that of clustering data.  _Clustering_ is grouping a set of objects such that objects in the same group (i.e. cluster) are more similar to each other in some sense than to objects of different groups.\n\nWe will experiment with the simplest of clustering problems: finding clusters of 2-dimensional points.\n\n* Given: \n  * a set of $n$ observations ${\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n}$, where each observation is a $d$-dimensional real vector\n  * a number of clusters $k$\n* Compute: a cluster assignment mapping $𝐶(\\mathbf{x}_i)\\in{1, \\ldots, k}$ that minimizes the within cluster sum of squares (WCSS):\n  * $\\Sigma_{i=1}^n || \\mathbf{x}_𝑖 − \\mu_{𝐶(\\mathbf{x}_𝑖)} ||^2$ where centroid $\\mu_{𝐶(\\mathbf{x}_𝑖)}$ is the mean of the points in cluster $𝐶(\\mathbf{x}_𝑖)$\n\n### $k$-Means Clustering\n\nThe general algorithm for $k$-means clustering is as follows:\n\n* Randomly choose $k$ cluster centroids $𝝁_1,𝝁_2, … 𝝁_𝑘$ and arbitrarily initialize cluster assignment mapping $𝐶$.\n* While remapping $𝐶$ from each $\\mathbf{x}_𝑖$ to its closest centroid $𝝁_𝑗$ causes a change in $𝐶$:\n  * Recompute $𝝁_1,𝝁_2, … 𝝁_𝑘$ according to the new $𝐶$\n\nSummarizing, in order to minimize the WCSS, we alternately:\n* Recompute $𝐶$ to minimize the WCSS holding $𝝁_𝑗$ fixed.\n* Recompute $𝝁_𝑗$ to minimize the WCSS holding $𝐶$ fixed.\n\nIn minimizing the WCSS, we seek a clustering that minimizes Euclidean distance variance within clusters.\n","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4e91a2","input":"Watch what happens with values of $\\epsilon$ set too high and too low:","pos":18,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"50fe03","input":"We first create a blobs dataset of 1000 points in 4 distinct clusters:","pos":3,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"571578","input":"## Homework\n","pos":29,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"697377","input":"When the minimum number of points is too high, then nothing can be a core point and thus everything is noise.  When the minimum number of points is too low, we are only saved by the choice of epsilon and the separation of the data here.  We have the correct number of clusters, but note that there are _no border points_.  Every point is a core point.\n\nIt is the correct selection of the $\\epsilon$ radius and the number of points within that radius that defines the density and extend of the reachability relationship that brings out the best performance of this algorithm.\n\nHaving one more parameter than $k$-means clustering makes it more complex to tune, but it is able to handle many more cluster shapes than $k$-means clustering.","pos":22,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"6c96b3","input":"### DBSCAN\n\nThe **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise (DBSCAN) algorithm makes the assumption that clusters are defined by a density threshold of data, and that points not within such areas of density are labeled as \"noise\".  Unlike $k$-means clustering, DBSCAN clustering is not supplied with the number of clusters $k$.  Instead, it is supplied a radius $\\epsilon$ and a minimum number of points/samples.  Consider this example from Wikipedia:\n\n![Wikipedia DBSCAN example figure](https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/800px-DBSCAN-Illustration.svg.png)\n\nIn this example we have 2-D points and $\\epsilon$ is the radius of the circle around each point. The minimum number of points within each circle for the point at the center to be considered a _core_ point is 4.  These core points (e.g. point A) and their circles are colored red.  Points without any other points within their circle (e.g. point N), are called _noise_ points and do not belong to any cluster.  Points with less than the minimum number of points to be considered core, yet having at least one other point within $\\epsilon$ (e.g. points B and C) are called _border_ points.\n\nThe structure of clusters is built from relationships between core and border points.  We say that a point $P_j$ is _directly reachable_ from point $P_i$ if $P_i$ is a core point and $P_j$ is within $\\epsilon$ of $P_i$.  Put another way, points can only be directly reachable from core points, and any point $P_j$ within $\\epsilon$ of core point $P_i$ is directly reachable from $P_i$.  (A border point can be directly reachable from a core point, but by definition, nothing is directly reachable from a border point.)\n\nPoints can also be indirectly reachable from a core point.  If we can construct a chain of points, $P_1, P_2, \\ldots, P_n$ such $P_i$ is directly reachable from $P_{i-1}$, then we say that $P_n$ is (indirectly) reachable from $P_1$.  \n\nA cluster $C$ is a set of points where (1) $C$ contains as least one core point, and (2) every point in $C$ is reachable from any core point.\n\nWhile this takes the burden off of the user for determining $k$, it means that one needs to tune _two_ variables: **$\\epsilon$** and **the minimum points** within $\\epsilon$ for a point to be considered core.  Too large or small an $\\epsilon$, for example, and we can end up with one large cluster or all noise, respectively.  Let's see this in action with ```sklearn.cluster.DBSCAN```:","pos":16,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"6de44a","input":"For the blob and uniform data as orange and blue lines, respectively, let us plot:\n* WCSS values of each,\n* log(WCSS) values of each, and\n* the difference of those logarithms (in green).","pos":13,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"729d76","input":"Next, we apply $k$-means clustering with $k = 4$ using the ```sklearn.cluster.KMeans```.  We will end up with different labels (represented by different colors), yet the labels will have a consistent one-to-one mapping with the original labels that were not supplied to the algorithm.  This is the ideal, to discern clustering order in data.","pos":5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"756a02","input":"Watch what happens when we choose a different $k = 3$:","pos":7,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a35a4d","input":"In practice, we would perform iterative $k$-means clustering and retain the minimum WCSS clustering, and we would also perform a number of uniform data clusterings and take the average as a baseline WCSS for comparison, the simple example above shows both\n 1. the discernable \"elbow\" at $k=4$, and\n 2. the fact that the difference in log(WCSS) between the data and uniform data peaks at $k=4$.\n\nIn short, to discern the best $k$ for $k$-means clustering, one should try a range of $k$ values and use one of these methods to select the minimum $k$ that best describes structure in the data.","pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b2a7c6","input":"**Exercise 1:** Complete the in-class exercises if you haven't already.\n\n**Exercise 2:** Perform all the same clustering exercises you did in-class with the data of [hw16-1.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw16/hw16-1.csv).\nRead the comment instructions carefully to note variations in parameter settings, etc.\n","pos":30,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"bd929b","input":"# Clustering\n\nLearning Objectives:\n* Students will learn the difference between supervised and unsupervised learning.\n* Students will be exposed to three algorithmic approaches to clustering data: k-means, DBSCAN, and agglomerative hierarchical clustering.\n* Students will practice the application of these three clustering algorithms with sklearn's corresponding modules.\n\nVideo/Readings before class:\n* Supervised Learning versus Unsupervised Learning and $k$-Means Clustering:\n  * Watch this video on [$k$-Means Clustering](http://cs.gettysburg.edu/~tneller/videos/ds256/kmeans.mp4) ([slides](http://modelai.gettysburg.edu/2016/kmeans/assets/k-Means_Clustering.pdf)).\n  * Play with the [OnMyPhd.com demonstration of $k$-Means Clustering](http://www.onmyphd.com/?p=k-means.clustering&ckattempt=1)\n  * Jake VanderPlas. [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/):\n      * [Chapter 5 section \"In Depth: k-Means Clustering\"](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n  * Optional readings/reference: Wikipedia articles on [$k$-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering) and [Determining the number of clusters in a dataset](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)\n* DBSCAN:\n  * [scikit learn documentation on DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n  * [scikit learn DBSCAN demonstration](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html)\n  * [AnalyticsVidhya's \"How DBSCAN Works\"](https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/)\n* Hierarchical Clustering:\n  * [scikit learn documentation on Agglomerative Hierarchical Clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)\n\nBefore class:\n* View/read the above resources so as to understand how each of the three different clustering methods works.  Below, we will demonstrate the application of the clustering methods as implemented in the sklearn library.\n\nIn class:\n* We will work together on the exercises in section \"In Class\".\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.\n","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"c41d1d","input":"Now let us do the same for generated uniform data.","pos":11,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"c56902","input":"(end of homework)","pos":32,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"cb2306","input":"Below, we will define a function for computing the Within-Cluster Sum of Squares (WSCC) for different $k$ on this data.","pos":9,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e38334","input":"If you have extra time in class, use the demonstration code below to create your own clustering data.  Use a paint program to create cluster regions or download an image with dark clusters on a light background.  The following code loads an image file (e.g. .png, .jpg), thresholds and samples random pixels that are more dark than light, and creates data suitable for experimentation.  This can provide good means of experimenting with clustering techniques.\n\n**TO-DO (if time): Create your own clustering challenge problem by processing an image as shown below.  Experiment with different clustering methods to see which works best for your clusters.**","pos":27,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"ec79ae","input":"If the circles (or _hyperspheres_ in higher dimensional data) have a radius $\\epsilon$ that is too high, then everything is in the same cluster.  If $\\epsilon$ is too low, then everything is noise and there are no clusters.\n\nNow lets see what happens when we set the minimum points too high and too low:","pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f9ec03","input":"# In Class\n\nPerform the following steps commented below.\n","pos":25,"state":"done","type":"cell"}
{"id":0,"time":1602833562735,"type":"user"}
{"last_load":1602662933703,"type":"file"}