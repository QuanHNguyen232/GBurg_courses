{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":80363520},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"035640","input":"","pos":42,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"313bfc","input":"","pos":34,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9cdfbd","input":"","pos":38,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ee5c87","input":"","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f66751","input":"","pos":36,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f9255f","input":"","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"c0bc1d","input":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Ridge\nimport math\n\n# NOTE: It is not necessary for you to understand this code at this point.  It is supplied for the curious.\n\n# We begin by importing our entire pseudorandom number generator (PRNG) module and giving it a \"seed\" for reproducability:\nfrom random import *\nseed(0)\nnp.random.seed(0)\n\n# We generate 500 random uniform numbers of student hours in the range 0.5 to 5.5:\ndf1 = pd.DataFrame(np.random.uniform(low=0.5, high=5.5, size=(500, 1)), columns=['hours_studied'])\n\n# From these, we use an underlying logistic model (that we'll later try to estimate) to generate pass/fail results.\nreal_intercept = -4\nreal_hours_coeff = 1.5\ndef pass_prob(hours_studied):\n    return (1 / (1 + math.exp(-(real_hours_coeff * hours_studied + real_intercept))))\ndf1['result'] = df1['hours_studied'].apply(lambda x: 'pass' if random() < pass_prob(x) else 'fail')\n\nprint(df1.describe())\nprint(df1.sample(10))","output":{"0":{"name":"stdout","output_type":"stream","text":"       hours_studied\ncount     500.000000\nmean        2.982988\nstd         1.458782\nmin         0.523477\n25%         1.739129\n50%         2.934210\n75%         4.153729\nmax         5.494235\n     hours_studied result\n168       4.149953   pass\n243       1.160341   fail\n108       1.615408   fail\n474       5.201048   pass\n173       0.592609   fail\n67        0.985506   fail\n52        5.441869   pass\n495       1.858264   fail\n35        3.588177   pass\n69        0.980492   fail\n"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"3f0945","input":"make_ohe = onehotencoder.transform(df_new.make.values.reshape(-1,1)).toarray()\nprint(\"Our one-hot-encoded make with new data:\")\nprint(make_ohe)\n# Create a new dataframe with these columns and our one-hot encoded data\ndfOneHot = pd.DataFrame(make_ohe, columns = one_hot_column_names)\n# 3) Concatentate our original dataframe df with the dataframe containing our new columns\ndf_new = pd.concat([df_new, dfOneHot], axis=1)\ndf_new.head()","output":{"0":{"name":"stdout","output_type":"stream","text":"Our one-hot-encoded make with new data:\n[[1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]]\n"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>car_years</th>\n      <th>make</th>\n      <th>make_0</th>\n      <th>make_1</th>\n      <th>make_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>Citroen</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9</td>\n      <td>Toyota</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Jeep</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   car_years     make  make_0  make_1  make_2\n0         10  Citroen     1.0     0.0     0.0\n1          9   Toyota     0.0     0.0     1.0\n2          3     Jeep     0.0     1.0     0.0"},"exec_count":10,"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"8748b5","input":"X = df_new[['car_years', 'make_0', 'make_1', 'make_2']]  # select input(s)\ny = linear_regressor.predict(X)\nprint(y)\ndf_new['maintenance_cost'] = y\ndf_new","output":{"0":{"name":"stdout","output_type":"stream","text":"[[114.23261127]\n [ 56.48973199]\n [161.81536144]]\n"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>car_years</th>\n      <th>make</th>\n      <th>make_0</th>\n      <th>make_1</th>\n      <th>make_2</th>\n      <th>maintenance_cost</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>Citroen</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>114.232611</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9</td>\n      <td>Toyota</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>56.489732</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Jeep</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>161.815361</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   car_years     make  make_0  make_1  make_2  maintenance_cost\n0         10  Citroen     1.0     0.0     0.0        114.232611\n1          9   Toyota     0.0     0.0     1.0         56.489732\n2          3     Jeep     0.0     1.0     0.0        161.815361"},"exec_count":11,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"9ce177","input":"# Generating power law distribution of numbers according to https://en.wikipedia.org/wiki/Pareto_distribution#Random_sample_generation\n# with Pareto Principle alpha value.\nseed(0)\nnum_values = 10000\nprint(\"Number of numeric values generated:\", num_values)\nx_m = 1.0\nalpha = 1.16\nX = [x_m / (random() ** (1/alpha)) for i in range(10000)]\nx_max = max(X)\nX = [round(x / x_max, 4) for x in X]\nprint(\"Histogram of Distribution:\")\n_ = plt.hist(X, bins=1000)\nplt.show()\n\n# Convert numbers to strings of letters\nalpha = 'abcdefghij'\ndef convert_to_str(x):\n    x_str = str(x)\n    x_str = x_str.replace('.','')\n    x_chars = list(x_str)\n    l_chars = [alpha[int(ch)] for ch in x_chars]\n    return \"\".join(l_chars)\nS = [convert_to_str(x) for x in X]\nS_set = set(S)\n\n# Associate values:\nval_dict = {s:random() - 0.5 for s in S_set}\ndf = pd.DataFrame(S, columns=['c1'])\ny_vals = [val_dict[s] + np.random.normal(scale=0.001) for s in S]\ndf['y'] = y_vals\nprint(df.c1.describe())\ndf.head(10)","output":{"0":{"name":"stdout","output_type":"stream","text":"Number of numeric values generated: 10000\nHistogram of Distribution:\n"},"1":{"data":{"image/png":"1b43768ce2bd47b7d1f3abe2b9c536449c676301","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":12,"metadata":{"image/png":{"height":248,"width":381},"needs_background":"light"},"output_type":"execute_result"},"2":{"name":"stdout","output_type":"stream","text":"count     10000\nunique      129\ntop       aaaab\nfreq       3668\nName: c1, dtype: object\n"},"3":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>c1</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aaaab</td>\n      <td>0.082508</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aaaab</td>\n      <td>0.082369</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aaaac</td>\n      <td>-0.432031</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aaaad</td>\n      <td>-0.493635</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aaaac</td>\n      <td>-0.432336</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>aaaac</td>\n      <td>-0.432212</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>aaaab</td>\n      <td>0.084397</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>aaaad</td>\n      <td>-0.492776</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>aaaac</td>\n      <td>-0.434290</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>aaaac</td>\n      <td>-0.433184</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"      c1         y\n0  aaaab  0.082508\n1  aaaab  0.082369\n2  aaaac -0.432031\n3  aaaad -0.493635\n4  aaaac -0.432336\n5  aaaac -0.432212\n6  aaaab  0.084397\n7  aaaad -0.492776\n8  aaaac -0.434290\n9  aaaac -0.433184"},"exec_count":12,"output_type":"execute_result"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"28b675","input":"num_variables = 40  # far less than the number of unique values\nvar_names = [\"h\" + str(int(i)) for i in range(num_variables)]\nprint(\"hashing trick variables:\", var_names)\ndf_hash = pd.DataFrame(np.zeros((num_values, num_variables)), columns=var_names)\nhash_vals = [abs(hash(s)) % num_variables for s in df['c1']] # many Python built-in types have a hash predefined\nprint(\"First 10 hash values:\", hash_vals[0:10])\nfor i, val in enumerate(hash_vals):\n    var_name = 'h' + str(val)\n    df_hash[var_name][i] = 1 # put a 1 in row i under the hash variable h# for the i_th hash value #\nprint(df_hash.head(10))\n\n# With this encoding, we can now perform whatever regression is appropriate.  \n# Here and in exercises, apply _ridge_ regression.  Linear regression applied\n# to these problems can result in large coefficients.  Ridge regression is linear\n# regression but with a \"regularization\" penalty for large coefficients.\nX = df_hash  # select input(s)\ny = df[['y']]  # select output\nridge_regressor = Ridge()  # create ridge regression object\nridge_regressor.fit(X, y)  # perform ridge regression of output onto inputs\nprint('y-intercept:', ridge_regressor.intercept_)  # print the y-intercept\nprint('X coefficient(s):', ridge_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', ridge_regressor.score(X, y))  # print R^2 score\n\n# Predictions for 'aaaab', 'aaaac', 'aaaad', 'aaaae', 'aaaaf', and 'aaaag'\nS_new = ['aaaab', 'aaaac', 'aaaad', 'aaaae', 'aaaaf', 'aaaag']\ndf_hash_new = pd.DataFrame(np.zeros((len(S_new), num_variables)), columns=var_names)\nhash_vals = [abs(hash(s)) % num_variables for s in S_new] # many Python built-in types have a hash predefined\nfor i, val in enumerate(hash_vals):\n    var_name = 'h' + str(val)\n    df_hash_new[var_name][i] = 1 # put a 1 in row i under the hash variable h# for the i_th hash value #\nX = df_hash_new\nprint(ridge_regressor.predict(X))","output":{"0":{"name":"stdout","output_type":"stream","text":"hashing trick variables: ['h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'h8', 'h9', 'h10', 'h11', 'h12', 'h13', 'h14', 'h15', 'h16', 'h17', 'h18', 'h19', 'h20', 'h21', 'h22', 'h23', 'h24', 'h25', 'h26', 'h27', 'h28', 'h29', 'h30', 'h31', 'h32', 'h33', 'h34', 'h35', 'h36', 'h37', 'h38', 'h39']\nFirst 10 hash values: [34, 34, 36, 22, 36, 36, 34, 22, 36, 36]\n"},"1":{"name":"stdout","output_type":"stream","text":"    h0   h1   h2   h3   h4   h5   h6   h7   h8   h9  ...  h30  h31  h32  h33  \\\n0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n\n   h34  h35  h36  h37  h38  h39  \n0  1.0  0.0  0.0  0.0  0.0  0.0  \n1  1.0  0.0  0.0  0.0  0.0  0.0  \n2  0.0  0.0  1.0  0.0  0.0  0.0  \n3  0.0  0.0  0.0  0.0  0.0  0.0  \n4  0.0  0.0  1.0  0.0  0.0  0.0  \n5  0.0  0.0  1.0  0.0  0.0  0.0  \n6  1.0  0.0  0.0  0.0  0.0  0.0  \n7  0.0  0.0  0.0  0.0  0.0  0.0  \n8  0.0  0.0  1.0  0.0  0.0  0.0  \n9  0.0  0.0  1.0  0.0  0.0  0.0  \n\n[10 rows x 40 columns]\ny-intercept: [0.04148306]\nX coefficient(s): [[-0.1992895   0.1708218   0.01868574 -0.22561482  0.10144809 -0.35580465\n  -0.15067126 -0.36223429  0.22958716  0.19926064  0.01527276 -0.1356599\n  -0.24164339  0.29412457  0.06704127  0.01560419  0.18987062  0.15033273\n   0.20098851  0.19397095  0.21858696 -0.07731236 -0.32797796 -0.19077006\n  -0.08349057 -0.07532784  0.0331355   0.04240875  0.          0.02838124\n   0.20953686  0.0160892   0.09239943  0.3516411   0.0403429   0.10471641\n  -0.45745412 -0.06405812  0.         -0.03693854]]\nR^2 score: 0.7440666850980412\n[[ 0.08182596]\n [-0.41597106]\n [-0.2864949 ]\n [-0.2864949 ]\n [ 0.05675582]\n [ 0.39312416]]\n"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"8ff51d","input":"y = df1['result'].apply(lambda s: 1 if s == 'pass' else 0)\nX = df1[['hours_studied']]\nplt.scatter(X, y);","output":{"0":{"data":{"image/png":"0c6039ba26f8127b03037f00bda5b738a2906bd0","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":2,"metadata":{"image/png":{"height":248,"width":372},"needs_background":"light"},"output_type":"execute_result"}},"pos":4,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"9e8d66","input":"logistic_regressor = LogisticRegression()\nlogistic_regressor.fit(X, y)\nprint('y-intercept:', logistic_regressor.intercept_)  # print the y-intercept\nprint('x coefficient(s):', logistic_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', logistic_regressor.score(X, y))  # print R^2 score\n\n# Plot the scatterplot of the data (blue), the original logistic generating function (blue),\n# the fit logistic function (orange), and the model category prediction (green).\nx_vals = np.linspace(start=X.min(), stop=X.max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\ny_fit = logistic_regressor.predict_proba(x_vals)  # n-by-2 nd array that has false/true probability estimates as second 0/1 index\ny_predict = logistic_regressor.predict(x_vals)\ny_source = [pass_prob(x) for x in x_vals]\nprint(y_fit.shape)\nplt.scatter(X, y, color='blue')\nplt.plot(x_vals, y_source, color='blue')\nplt.plot(x_vals, y_fit[:, 1], color='orange')\nplt.plot(x_vals, y_predict, color='green');","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [-3.30365824]\nx coefficient(s): [[1.22762643]]\nR^2 score: 0.8\n(1000, 2)\n"},"1":{"data":{"image/png":"56eb7cd6ffd0c0f72cafbe59cffcb518e4183322","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":248,"width":372},"needs_background":"light"},"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"1d90e7","input":"def predict_pass_fail(hours_studied):\n    return 'pass' if logistic_regressor.predict([[hours_studied]])[0] else 'fail'  # note that a 1 / 0 value can be used in place of True / False\nprint(predict_pass_fail(0.6))\nprint(predict_pass_fail(3.0))\nprint(predict_pass_fail(6.0))","output":{"0":{"name":"stdout","output_type":"stream","text":"fail\npass\npass\n"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"dc9fbb","input":"makes = ['Citroen', 'Jeep', 'Toyota']\nnum_makes = len(makes)\nbase_values = [114, 173, 58]\nstdev = 10.0\nyear_factor = 1.5\nmax_year = 20\n\nnum_rows = 500\ndf = pd.DataFrame(np.random.randint(low=0, high=max_year + 1, size=(num_rows, 1)), columns=['car_years'])\nrand_indices = np.random.randint(low=0, high=num_makes, size=(num_rows, 1))\nmake_vals = [makes[rand_indices[i][0]] for i in range(num_rows)]\nnoise = np.random.normal(0, stdev, num_rows)\ncost_vals = [max(0, base_values[rand_indices[i][0]] + year_factor * (df['car_years'][i] - max_year / 2) + noise[i]) for i in range(num_rows)]\ndf['make'] = make_vals\ndf['maintenance_cost'] = cost_vals\ndf.head(10)\n","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>car_years</th>\n      <th>make</th>\n      <th>maintenance_cost</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Jeep</td>\n      <td>154.995212</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>Jeep</td>\n      <td>184.449713</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>Jeep</td>\n      <td>158.576381</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13</td>\n      <td>Citroen</td>\n      <td>116.721869</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Jeep</td>\n      <td>168.009981</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>15</td>\n      <td>Toyota</td>\n      <td>76.047579</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>Toyota</td>\n      <td>63.100477</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>18</td>\n      <td>Toyota</td>\n      <td>65.835009</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4</td>\n      <td>Toyota</td>\n      <td>46.231770</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>14</td>\n      <td>Jeep</td>\n      <td>190.239053</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   car_years     make  maintenance_cost\n0          0     Jeep        154.995212\n1         20     Jeep        184.449713\n2         13     Jeep        158.576381\n3         13  Citroen        116.721869\n4          5     Jeep        168.009981\n5         15   Toyota         76.047579\n6          7   Toyota         63.100477\n7         18   Toyota         65.835009\n8          4   Toyota         46.231770\n9         14     Jeep        190.239053"},"exec_count":5,"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"f63417","input":"X = pd.get_dummies(df.make, prefix='make')\nX.head(10)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>make_Citroen</th>\n      <th>make_Jeep</th>\n      <th>make_Toyota</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   make_Citroen  make_Jeep  make_Toyota\n0             0          1            0\n1             0          1            0\n2             0          1            0\n3             1          0            0\n4             0          1            0\n5             0          0            1\n6             0          0            1\n7             0          0            1\n8             0          0            1\n9             0          1            0"},"exec_count":6,"output_type":"execute_result"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"7af4d1","input":"from sklearn import preprocessing\nonehotencoder = preprocessing.OneHotEncoder()\n# Reshape the make one-dimensional array to 2D as fit_transform expects a 2D array and fit the object \nmake_ohe = onehotencoder.fit_transform(df.make.values.reshape(-1,1)).toarray()\nprint(\"Our one-hot-encoded make data:\")\nprint(make_ohe)\n# If we wanted to add this back into the original dataframe,\n# 1) Create column names for the encoding.\none_hot_column_names = [\"make_\"+str(int(i)) for i in range(make_ohe.shape[1])]\nprint('New columns:', one_hot_column_names)\n# 2) Create a new dataframe with these columns and our one-hot encoded data\ndfOneHot = pd.DataFrame(make_ohe, columns = one_hot_column_names)\n# 3) Concatentate our original dataframe df with the dataframe containing our new columns\ndf = pd.concat([df, dfOneHot], axis=1)\ndf.head()","output":{"0":{"name":"stdout","output_type":"stream","text":"Our one-hot-encoded make data:\n[[0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n ...\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]]\nNew columns: ['make_0', 'make_1', 'make_2']\n"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>car_years</th>\n      <th>make</th>\n      <th>maintenance_cost</th>\n      <th>make_0</th>\n      <th>make_1</th>\n      <th>make_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Jeep</td>\n      <td>154.995212</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>Jeep</td>\n      <td>184.449713</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>Jeep</td>\n      <td>158.576381</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13</td>\n      <td>Citroen</td>\n      <td>116.721869</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Jeep</td>\n      <td>168.009981</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   car_years     make  maintenance_cost  make_0  make_1  make_2\n0          0     Jeep        154.995212     0.0     1.0     0.0\n1         20     Jeep        184.449713     0.0     1.0     0.0\n2         13     Jeep        158.576381     0.0     1.0     0.0\n3         13  Citroen        116.721869     1.0     0.0     0.0\n4          5     Jeep        168.009981     0.0     1.0     0.0"},"exec_count":7,"output_type":"execute_result"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"009100","input":"X = df[['car_years', 'make_0', 'make_1', 'make_2']]  # select input(s)\ny = df[['maintenance_cost']]  # select output\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('X coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [99.37145854]\nX coefficient(s): [[  1.56469678  -0.78581503  57.74981257 -56.96399754]]\nR^2 score: 0.9614389037101071\n"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"776e27","input":"np.random.seed(6)\nnum_rows = 3\ndf_new = pd.DataFrame(np.random.randint(low=0, high=max_year + 1, size=(num_rows, 1)), columns=['car_years'])\nrand_indices = np.random.randint(low=0, high=num_makes, size=(num_rows, 1))\nmake_vals = [makes[rand_indices[i][0]] for i in range(num_rows)]\nnoise = np.random.normal(0, stdev, num_rows)\ndf_new['make'] = make_vals\ndf_new","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>car_years</th>\n      <th>make</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>Citroen</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9</td>\n      <td>Toyota</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Jeep</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   car_years     make\n0         10  Citroen\n1          9   Toyota\n2          3     Jeep"},"exec_count":9,"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"markdown","id":"32465b","input":"## One-Hot Encoding of Categorical Data\n\nAll of our input data thus far has been numeric.  All of our regression techniques accept only numeric data.  **What does one do if one has categorical input data and wants to apply regression techniques that require numeric inputs?**  Suppose we have numeric input ```car_years``` and categorical input auto ```make``` (```Citroen```, ```Jeep```, and ```Toyota```) as input and numeric ```maintenance_cost``` output. \n\nHere's what _not_ to do: Assign make categories to unique integers and perform a linear/nonlinear regression.  Why is this a bad approach?  There's no reason to believe that these auto makes have an ordering such that the integers would offer a help our our regression.\n\nHere's a better approach: Create new 1.0 / 0.0 (meaning True / False, respectively) numeric input features, one for each category, e.g. ```is_Citroen```, ```is_Jeep```, and ```is_Toyota```, and use these created binary category-indicating features in place of the original categorical input ```make```.\n\nThis is what is known as \"One Hot Encoding\".  Each category value is represented by having exactly one of the created binary features having a \"hot\" (1) value with all others having value 0.  Let's see this at work.  First, we'll generate our demonstration data.","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"36f25c","input":"## Power-Law Distributions and the Hashing Trick\n\nThere are many types of data that exhibit a [Power Law Distribution](https://en.wikipedia.org/wiki/Power_law) with most values being a popular few, yet having many different unique values.  In the code below, I'll generate some string data according to a Power Law Distributions and we'll see its characteristics:","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"39f76a","input":"**Exercise 3:**  Do the same as in-class exercise 3, but with  [hw8-3.csv](http://cs.gettysburg.edu/~tneller/ds256/data/inclass/hw8-3.csv).","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"53959c","input":"**Exercise 2:**  Perform the same work as in-class exercise 2, except using [hw8-2.csv](http://cs.gettysburg.edu/~tneller/ds256/data/inclass/hw8-2.csv) and predicting ```y``` for ```x1``` = -0.5, ```c1``` = 'z', and ```c2``` = 'q'.","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"553383","input":"There are two final important things to note.  One is a caveat.  Notice that given any two of these one-hot encoded values starting with ```make_```, we can predict the third.  Therefore, if we're looking for patterns in our one-hot encoded data, we can fall into the \"dummy variable\" trap, causing problems with some techniques because there isn't independence among these inputs.  The solution is simple: drop any one of our ```make_``` variables from our model building.\n\nThe second important thing to note is that one-hot encoding becomes too computationally cumbersome when there are too many distinct categorical values.  For this difficulty, we do something similar, using something called \"the hashing trick\".","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"5d2d32","input":"### Multiple logistic regression\n\nThe same techniques and syntax apply to multiple input variables, just as we saw with multiple linear regression.\n\n### Multiple output categories\n\nOne way to apply logistic regression to the prediction of one of more than two output categories is called one-versus-all.  The approach is relatively simple:\n* For each output ```y``` category ```c```:\n  * Create a target ```y_c``` that is 1 when ```y``` is ```c``` and 0 otherwise (\"versus-all\").\n  * Perform logistic regression to create a model ```m_c``` to predict the probability of belonging to category ```c```, i.e. ```y_c``` = 1.\n* For an input ```x```, predict category ```c``` for whichever model ```m_c``` yields the highest probability of ```x``` belonging to ```c```.\n\nIn summary, we build a separate predictor for each category versus not belonging to that category.  We classify an input to the category whose predictor has highest confidence of the input belonging to that category.","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"63028c","input":"## Logistic Regression\n\nLogistic Regression seeks to fit a logistic function curve to data in the range $[0, 1]$.   We need to first convert our data target values to 1 (pass) and 0 (fail).  Below, we perform the conversion and visualize the data.","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"6add00","input":"From this model, we can see that our base cost prediction in the y-intercept.  We can also see that car_years contributes to cost increase, and that Jeeps and Toyotas respectively increase and decrease the expected cost.\n\nLet suppose we have some new car data.  How can we use our learned model?  Thankfully, our ```OneHotEncoding``` object still exists for transforming the data.  Watch how we do not \"fit\" its transformation, but rather just use its transformation.\n\nFirst, we'll generate a 3 data values as we did above but without values.","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"6d4820","input":"Note how there is a greater frequency of passing as the number of hours studied increases.  Just looking at the data, we could infer that the probability of passing is very low or very high with few or many hours of study, respectively.  However, in the middle range of values, there is not a sharp transitions from all failures to all passes; there is no magical number of hours of study to predict the outcome.  We expect a gradual transition of the underlying probability.  This is the most common use of logistic regression, to fit a logistic curve to the data so that we have a model for the underlying probability.  We can then round the model's prediction to 1 or 0 to make predictions for any input.\n\nPerforming a logistic regression is much like performing a linear regression, and not just syntactically.  The logistic function for multiple inputs is of the form $\\frac{1}{1 + b^{-(\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_n x_n)}}$.  Note the parenthesized expression in the denominator.  Where have you seen it before?\n\nThe parenthesized form is what we fit for linear regression, so that logistic regression is just linear regression form transformed through a logistic function.  Thus, it is no surprise that we find [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) in the linear regression module of ```sklearn```:","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"70adba","input":"# Categorical Data\n\nCategorical data is, as the name implies, data that expresses categories to be assigned to [categorical variables](https://en.wikipedia.org/wiki/Categorical_variable).  Examples include a car's make (e.g. Toyota) and model (e.g. Corolla), sex (e.g. Female), qualitative characterizations (e.g. sweet, sour, bitter), and any other set of labels that aren't necessarily ordered/numeric in nature.  Below, we generate data based on the Wikipedia Logistic Regression [Examples (section 2)](https://en.wikipedia.org/wiki/Logistic_regression#Examples) with numeric input variable ```hours_studied``` and categorical output ```outcome``` with values ```pass```/```fail```.  Our objective in this example is to predict the outcome of an exam given the number of hours a student studied for it.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"735036","input":"## Homework\n\n**Exercise 1:** Perform logistic regression on the data of [hw8-1.csv](http://cs.gettysburg.edu/~tneller/ds256/data/inclass/hw8-1.csv) following all steps of in-class exercise 1, with predictions printed in the original target categorical form for input values -0.5, 0.0, and 0.5.  ","pos":37,"type":"cell"}
{"cell_type":"markdown","id":"82838e","input":"**Exercise 3:** The data of [ic8-3.csv](http://cs.gettysburg.edu/~tneller/ds256/data/inclass/ic8-3.csv) has one categorical input ```c1``` and one numeric output ```y```.  First find the number of unique categorical values.  Then find a number of hashing variables less than half of the number of unique values that yields an $R^2$ for ridge regression greater than or equal to .9.  Print the prediction of the most frequent value. (Hint: ```print(df.c1.describe())``` to see statistics on categorical variable ```c1``` in dataframe ```df```.)","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"844089","input":"# In Class\n\nTogether in class, you will seek to build models for each of the given data sets using the techniques above.","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"91b29b","input":"Next we'll go through the same steps to apply one-hot encoding but _reapplying_ what we've already created:","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"9d524a","input":"We've generated our demonstration data with a linear relationship, so we are now in a position to linearly regress ```maintenance_cost``` onto our numeric features: ```car_years```, ```make_0```, ```make_1```, and ```make_2```.  (Of course, we could hand nonlinear relationships in data as well.)","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"a8bfa5","input":"However, to use this with unseen future data is awkward, as we have no stored way of performing the encoding again for individual instances.  That's why some prefer the ```sklearn``` [```OneHotEncoder```](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"bb6054","input":"**Exercise 2:** The data of [ic8-2.csv](http://cs.gettysburg.edu/~tneller/ds256/data/inclass/ic8-2.csv) has one numeric input ```x1``` and two categorical inputs ```c1``` and ```c2```.  Assume that there is a linear relationship between input data and numerical output ```y```.  Build a linear regression model for ```y``` using the steps described above in the One-Hot Encoding section.  Finally, predict the ```y``` value that would correspond to ```x1``` = 0.1, ```c1``` = 'b', and ```c2``` = 'y'.  You should have an $R^2$ score of about 0.985 and your predicted ```y``` value should be about 0.401.","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"dc16d2","input":"# Logistic Regression and Categorical Data\n\nLearning Objectives:\n* Students will learn about the nature of categorical data.\n* Students will learn about the logistic function, logistic regression, and the application of logistic regression models to categorical output.\n* Students will learn about and apply one-hot encoding of categorical input features.\n* Students will learn about power-law distributions of data and apply the hashing trick for encoding power-law-distributed data.\n\nBefore class:\n* Wikipedia article on [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression), [Introduction](https://en.wikipedia.org/wiki/Logistic_regression), [Applications (section 1)](https://en.wikipedia.org/wiki/Logistic_regression#Applications), and [Examples (section 2)](https://en.wikipedia.org/wiki/Logistic_regression#Examples).\n* Introductory definition sections for [One-Hot](https://en.wikipedia.org/wiki/One-hot) and [Dummy Variable](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) and [Power Law](https://en.wikipedia.org/wiki/Power_law)\n* Read below up to (but not including) the section marked In Class.\n\nOptional reference/tutorial:\n* [Real Python \"Logistic Regression in Python\" article](https://realpython.com/logistic-regression-python/)\n* [Analytics Vidhya \"One-Hot Encoding vs. Label Encoding using Scikit-Learn\"](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/)\n\nIn class:\n* We will work together on the exercises in section \"In Class\".\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"e09ae8","input":"Over the original data, we see the original curve providing the source of the probabilities for the data (blue), the fit logistic curve of probability estimates (orange), and the true (1) / false (0) prediction (green) that would be made for our model by rounding the estimated probabilities (orange).  Note that we expect lower $R^2$ scores here because of the high variance of the data for intermediate values between the extremes.  We cannot expect high accuracy for such output.  However, we note that even with 500 data points, we were able to reconstruct the generating probability function fairly well.\n\nWe can close the loop in terms of our representation by creating a function to map back to ```pass``` and ```fail``` values:","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"e10a27","input":"Thus, we can see that the hashing trick can perform a decent regression on a power law distribution using a number of binary variables for encoding categorical data with a much larger number of unique values.  Looking at the top size most frequent values, we can see here that the 1st and 4th most common values collide to the detriment of the regression accuracy.  However, even with this severe collision scenario, we can see that the technique is largely successful.  On a commercial scale, we would have many more hashing variables and and many magnitudes larger for the number of unique values.\n\nFor an imperfect analogy, imagine that each hashed category is a different post office box.  We can associate a recipient with each box.  Letters meant for one recipient will get delivered to the same P.O. box as another recipient, but if almost all of the mail is going to few recipients, then the fraction of miss-delivered mail will be relatively small.  In the same way, the coefficients will be good estimates for the most frequent values provided that our number of hashing variables is large enough.\n\nThis hashing trick is often used in industry to deal with massive amounts of categorical data that follows a power law distribution.","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"f63179","input":"(end of homework)","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"f90ce2","input":"**Exercise 1:** Perform logistic regression on the data of [ic8-1.csv](http://cs.gettysburg.edu/~tneller/ds256/data/inclass/ic8-1.csv).  Perform the same steps as with the ```hours_studied```/```result``` example above.  In this case, you will not have the generating model (the blue curve plotted above), and you'll predict output ```positive```/```negative``` for ```ppm``` values 5, 20, and 35.  You should have an $R^2$ score of 0.818 and your answers should be 'negative', 'positive', and 'positive', respectively.","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"fa6218","input":"In ```pandas```, it is particularly easy to create the one-hot-enconding variables: ","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"fa66d0","input":"Finally, we can apply our linear regression model to predict maintenance cost values.","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"fb4f7c","input":"What we've created are 10,000 numeric values scaled to $[0, 1]$ and rounded to 4 decimal places. The distribution shows that, after scaling, almost all values are very small and the smallest comprises over a third of the total values.  And yet there are 129 unique values among the 10000.  These are converted into categorical data strings and each string is associated with some base value, and a bit of noise is introduced to the dataset.\n\nThis is a small sample for demonstration purposes.  The [Kaggle Criteo Display Advertising Challenge](https://www.kaggle.com/c/criteo-display-ad-challenge) had one categorical variable with over 10 _million_ unique values, but relatively few values made up a large share of values for that variable.\n\nHere we'll describe and then demonstrate what is known as the hashing trick.  Here is our strategy:\n* For each category value, put it through a [hash function](https://en.wikipedia.org/wiki/Hash_function) that maps data of arbitrary size to a fixed set of values.  These values are generally integers that appear random, but are arrived at through a deterministic process like a pseudorandom number generator.\n* Take the resulting integer's absolute value, and then compute modulus (%) some number of binary 0/1 variables that one wants as a limit.  These function similar to one-hot variables, yet there can be _collisions_ (two different values mapping to the same index) and there will be collisions guaranteed if the number of binary variables is less than the number of unique values.\n* One then does an encoding as if one has computed one-hot variable indices.\n\nHere's the trick that makes this work well.  If there are relatively few common values, and you have chosen enough values such that these common values do not collide, you get most of the performance of one-hot encoding without exploding your memory usage.  Let's see this in action.","pos":27,"type":"cell"}
{"id":0,"time":1600802918139,"type":"user"}
{"last_load":1600244905477,"type":"file"}