{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":250511360},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1601448677587,"exec_count":1,"id":"0bb93b","input":"# Imports\n\nimport math\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\n","kernel":"python3","no_halt":true,"pos":2,"start":1601448672680,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448678258,"exec_count":2,"id":"4d5c91","input":"my_seed = 0\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\n\n# We generate 1000 clustered 2D data points sampled from 3 normally-distributed clusters \n#   with each cluster representing a different class.\nX, y = make_blobs(n_samples=1000, centers=3,\n                  random_state=my_seed, cluster_std=1.0)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n","kernel":"python3","no_halt":true,"output":{"0":{"data":{"image/png":"32b0b0d6ae4989cc3d548cda5325c9c153b19790","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":411,"width":705},"needs_background":"light"}}},"pos":3,"start":1601448677597,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448678736,"exec_count":3,"id":"05d737","input":"# As before, we will split our datasets into training and testing datasets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=my_seed, shuffle=True)\n\n# Unlike the text, we train our decision tree with just the training dataset.\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\n# We can visualize the model by using VanderPlas' visualization code\n# modified to _assume the model is already fit_:\ndef visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n    ax = ax or plt.gca()\n\n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    ax.axis('off')\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    # Create a color plot with the results\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap=cmap, #clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n\nprint(\"\\nDecision Tree Model with Training Data:\")\nvisualize_classifier(model, X_train, y_train, ax = plt.gca())\nplt.show()\n\n# Our accuracy numbers will look great if we compute accuracy using the training set, which is # why we do our testing against held-out data to watch for overfitting.\n# Accuracy score is for binary True/False or 1/0, so we'll need to test it for each class:\ny_predict = model.predict(X_train)\nfor c in range(3):\n    print('Class', c, 'training accuracy: ', accuracy_score(y_train == c, y_predict == c))\n\n# Next we visualize how the classifier works (or doesn't work) with the testing data:\nprint(\"Decision Tree Model with Testing Data:\")\nvisualize_classifier(model, X_test, y_test, ax = plt.gca())\nplt.show()\n\n# We can test this against our held-out testing dataset to see how the classifier generalizes for accuracy.  \ny_predict = model.predict(X_test)\nfor c in range(3):\n    print('Class', c, 'testing accuracy: ', accuracy_score(y_test == c, y_predict == c))\n","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"\nDecision Tree Model with Training Data:\n"},"1":{"data":{"image/png":"f1fa045e99a295d59ab3bfac56e8b58fdf26acd6","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"}},"2":{"name":"stdout","text":"Class 0 training accuracy:  1.0\nClass 1 training accuracy:  1.0\nClass 2 training accuracy:  1.0\nDecision Tree Model with Testing Data:\n"},"3":{"data":{"image/png":"32cfeba97bca28e203911dac8b79e7da92f5259a","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"}},"4":{"name":"stdout","text":"Class 0 testing accuracy:  0.898\n"},"5":{"name":"stdout","text":"Class 1 testing accuracy:  0.956\nClass 2 testing accuracy:  0.926\n"}},"pos":4,"start":1601448678270,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448679210,"exec_count":4,"id":"fe6f4f","input":"model = DecisionTreeClassifier(max_depth=3)\nmodel.fit(X_train, y_train)\n\nprint(\"Decision Tree Model (depth limit = 3) with Training Data:\")\nvisualize_classifier(model, X_train, y_train, ax = plt.gca())\nplt.show()\ny_predict = model.predict(X_train)\nfor c in range(3):\n    print('Class', c, 'training accuracy: ', accuracy_score(y_train == c, y_predict == c))\n\nprint(\"\\nDecision Tree Model (depth limit = 3) with Testing Data:\")\nvisualize_classifier(model, X_test, y_test, ax = plt.gca())\nplt.show()\ny_predict = model.predict(X_test)\nfor c in range(3):\n    print('Class', c, 'testing accuracy: ', accuracy_score(y_test == c, y_predict == c))","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Decision Tree Model (depth limit = 3) with Training Data:\n"},"1":{"data":{"image/png":"5997151d4e39e47195e06a2cd2ebb18228b7ad50","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"}},"2":{"name":"stdout","text":"Class 0 training accuracy:  0.938\nClass 1 training accuracy:  0.962\nClass 2 training accuracy:  0.952\n\nDecision Tree Model (depth limit = 3) with Testing Data:\n"},"3":{"data":{"image/png":"cb763636f4b92d1d8a0c3ccc54f8c9f52241ee21","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"}},"4":{"name":"stdout","text":"Class 0 testing accuracy:  0.92\nClass 1 testing accuracy:  0.972\nClass 2 testing accuracy:  0.944\n"}},"pos":6,"start":1601448678756,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448680499,"exec_count":5,"id":"809f19","input":"model = RandomForestClassifier(n_estimators=100, random_state=0)\nmodel.fit(X_train, y_train)\n\nprint(\"Random Forest Model with Training Data:\")\nvisualize_classifier(model, X_train, y_train, ax = plt.gca())\nplt.show()\ny_predict = model.predict(X_train)\nfor c in range(3):\n    print('Class', c, 'training accuracy: ', accuracy_score(y_train == c, y_predict == c))\n\nprint(\"\\nRandom Forest Model with Testing Data:\")\nvisualize_classifier(model, X_test, y_test, ax = plt.gca())\nplt.show()\ny_predict = model.predict(X_test)\nfor c in range(3):\n    print('Class', c, 'testing accuracy: ', accuracy_score(y_test == c, y_predict == c))","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Random Forest Model with Training Data:\n"},"1":{"data":{"image/png":"4795cbf287e8bded3559a1faaf8d84d6ac95b588","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"}},"2":{"name":"stdout","text":"Class 0 training accuracy:  0.998\nClass 1 training accuracy:  0.998\nClass 2 training accuracy:  1.0\n\nRandom Forest Model with Testing Data:\n"},"3":{"data":{"image/png":"97a630790c48d71a087722b9e468cc5026995fa8","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"}},"4":{"name":"stdout","text":"Class 0 testing accuracy:  0.918\nClass 1 testing accuracy:  0.976\nClass 2 testing accuracy:  0.934\n"}},"pos":8,"scrolled":true,"start":1601448679243,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448681730,"exec_count":6,"id":"926f34","input":"model = RandomForestClassifier(n_estimators=100, random_state=0, max_samples=0.8)\nmodel.fit(X_train, y_train)\n\nprint(\"Random Forest Model (80% samples) with Training Data:\")\nvisualize_classifier(model, X_train, y_train, ax = plt.gca())\nplt.show()\ny_predict = model.predict(X_train)\nfor c in range(3):\n    print('Class', c, 'training accuracy: ', accuracy_score(y_train == c, y_predict == c))\n\nprint(\"\\nRandom Forest Model (80% samples) with Testing Data:\")\nvisualize_classifier(model, X_test, y_test, ax = plt.gca())\nplt.show()\ny_predict = model.predict(X_test)\nfor c in range(3):\n    print('Class', c, 'testing accuracy: ', accuracy_score(y_test == c, y_predict == c))","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Random Forest Model (80% samples) with Training Data:\n"},"1":{"data":{"image/png":"ab54771e7072e6170666de3df95bb5e548b50557","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"}},"2":{"name":"stdout","text":"Class 0 training accuracy:  0.996\nClass 1 training accuracy:  0.996\nClass 2 training accuracy:  1.0\n\nRandom Forest Model (80% samples) with Testing Data:\n"},"3":{"data":{"image/png":"5441a4b756a1347bfbe9c64f39cb206bf494702f","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"}},"4":{"name":"stdout","text":"Class 0 testing accuracy:  0.918\nClass 1 testing accuracy:  0.976\nClass 2 testing accuracy:  0.934\n"}},"pos":10,"start":1601448680510,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448682190,"exec_count":7,"id":"304176","input":"plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues);\n","kernel":"python3","no_halt":true,"output":{"0":{"data":{"image/png":"163f8a1dd3ed668b48b04c7b52ae0efbbad8042b","text/plain":"<Figure size 864x504 with 2 Axes>"},"metadata":{"image/png":{"height":424,"width":500},"needs_background":"light"}}},"pos":12,"start":1601448681738,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448683102,"exec_count":8,"id":"7bf0fc","input":"model = xgb.XGBClassifier(\n    gamma=1,\n    learning_rate=0.01,\n    max_depth=3,\n    n_estimators=100,\n    subsample=0.8,\n    random_state=my_seed\n)\n\nmodel.fit(X_train, y_train)\n\nprint(\"XGBoost Model (80% samples) with Testing Data:\")\nvisualize_classifier(model, X_test, y_test, ax = plt.gca())\nplt.show()\ny_predict = model.predict(X_test)\nfor c in range(3):\n    print('Class', c, 'testing accuracy: ', accuracy_score(y_test == c, y_predict == c))\n\nplot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues);\n","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"XGBoost Model (80% samples) with Testing Data:\n"},"1":{"data":{"image/png":"52f6e166887dc141e01c0661ad13283067e3a28b","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"}},"2":{"name":"stdout","text":"Class 0 testing accuracy:  0.922\nClass 1 testing accuracy:  0.972\nClass 2 testing accuracy:  0.942\n"},"3":{"data":{"image/png":"1605f21fcbe164ce2bba61961cd04d50a8ef1d0b","text/plain":"<Figure size 864x504 with 2 Axes>"},"metadata":{"image/png":{"height":424,"width":500},"needs_background":"light"}}},"pos":14,"start":1601448682201,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448684991,"exec_count":9,"id":"313bbc","input":"df = pd.read_csv('http://cs.gettysburg.edu/~tneller/ds256/data/hw6/hw6-1.csv', index_col='id')\n\nX = df[['x']]  # select input(s)\ny = df[['y']]  # select output\n\n# As before, we will split our datasets into training and testing datasets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=my_seed, shuffle=True)\n\n# Decision Tree Regression\nprint('Decision Tree Regression')\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)  # perform linear regression of output onto inputs\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nyfit = model.predict(Xfit)  # predict y's from the linearly spaced x data\nplt.scatter(X['x'], y)  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quartic function\nplt.show()\n\nprint('R^2 Score:', model.score(X_test, y_test))\nprint('Mean squared error (MSE):', mean_squared_error(y_test, model.predict(X_test)))\n\n# Random Forest Regression\nprint('\\nRandom Forest Regression')\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train.values.ravel())\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nyfit = model.predict(Xfit)  # predict y's from the linearly spaced x data\nplt.scatter(X['x'], y)  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quartic function\nplt.show()\n\nprint('R^2 Score:', model.score(X_test, y_test))\nprint('Mean squared error (MSE):', mean_squared_error(y_test, model.predict(X_test)))\n\n# XGBoost Regression\nprint('\\nXGBoost Regression')\n\nmodel = xgb.XGBRegressor()\nmodel.fit(X_train, y_train.values.ravel())\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nyfit = model.predict(Xfit)  # predict y's from the linearly spaced x data\nplt.scatter(X['x'], y)  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quartic function\nplt.show()\n\nprint('Mean squared error (MSE):', mean_squared_error(y_test, model.predict(X_test)))\n\nprint('\\nXGBoost Regression with Parameter Tuning')\n\n# I have commented out the parameter tuning here to save you the intensive computation. \n# Approach adapted from: https://www.kaggle.com/pablocastilla/predict-house-prices-with-xgboost-regression\n\n# parameters_for_testing = {\n#    'gamma':[0,0.03,0.1,0.3],\n#    'min_child_weight':[1.5,6,10],\n#    'learning_rate':[0.1,0.07],\n#    'max_depth':[3,5],\n#    'n_estimators':[100],\n#    'reg_alpha':[1e-5, 1e-2,  0.75],\n#    'reg_lambda':[1e-5, 1e-2, 0.45],\n#    'subsample':[0.6,0.95]\n# }\n\n# model = xgb.XGBRegressor()\n\n# grid_search = GridSearchCV(model, param_grid=parameters_for_testing, n_jobs=6, verbose=10, scoring='neg_mean_squared_error')\n# grid_search.fit(X_train,y_train)\n# print('Best parameters:')\n# print(grid_search.best_params_)\n# print('Best score:')\n# print(grid_search.best_score_)\n\n\n# Best parameters:\n# {'gamma': 0, 'learning_rate': 0.07, 'max_depth': 3, 'min_child_weight': 1.5, 'n_estimators': 100, 'reg_alpha': 1e-05, 'reg_lambda': 1e-05, 'subsample': 0.6}\n# Best score:\n# -27185.70732805021\n\n\nmodel = xgb.XGBRegressor(gamma=0, learning_rate=0.07, max_depth=3, min_child_weight=1.5, n_estimators=100, reg_alpha=1e-5, subsample=0.6)\nmodel.fit(X_train, y_train.values.ravel())\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nyfit = model.predict(Xfit)  # predict y's from the linearly spaced x data\nplt.scatter(X['x'], y)  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quartic function\nplt.show()\n\nprint('Mean squared error (MSE):', mean_squared_error(y_test, model.predict(X_test)))\n\n\nprint('\\nPolynomial Regression using Linear Regression')\n\nmodel = LinearRegression()\ndfPoly = df.copy()\ndfPoly['xpow2'] = dfPoly['x'].pow(2)\ndfPoly['xpow3'] = dfPoly['x'].pow(3)\ndfPoly['xpow4'] = dfPoly['x'].pow(4)\nX = dfPoly[['x', 'xpow2', 'xpow3', 'xpow4']]  # select input(s)\ny = dfPoly[['y']]  # select output\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=my_seed, shuffle=True)\nmodel.fit(X_train, y_train)\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nXfit['xpow2'] = Xfit['x'].pow(2)\nXfit['xpow3'] = Xfit['x'].pow(3)\nXfit['xpow4'] = Xfit['x'].pow(4)\nyfit = model.predict(Xfit)  # predict y's from the linearly spaced x data\nplt.scatter(X['x'], y)  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quartic function\nplt.show()\n\nprint('Mean squared error (MSE):', mean_squared_error(y_test, model.predict(X_test)))\n","kernel":"python3","no_halt":true,"output":{"0":{"name":"stdout","text":"Decision Tree Regression\n"},"1":{"data":{"image/png":"d7da20179373d57e28cf908c1b00653ef6736946","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":411,"width":725},"needs_background":"light"}},"10":{"name":"stdout","text":"Mean squared error (MSE): 10775.39903255749\n"},"2":{"name":"stdout","text":"R^2 Score: 0.9845813699506454\nMean squared error (MSE): 26038.19438271302\n\nRandom Forest Regression\n"},"3":{"data":{"image/png":"a43d6568b9b9bb27084011fa45a39f71549483d6","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":411,"width":725},"needs_background":"light"}},"4":{"name":"stdout","text":"R^2 Score: 0.9876923785220757\nMean squared error (MSE): 20784.48211061796\n\nXGBoost Regression\n"},"5":{"data":{"image/png":"0a171434b84d7b7dd6cfe9a15bcfe61a25bfc8cf","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":411,"width":725},"needs_background":"light"}},"6":{"name":"stdout","text":"Mean squared error (MSE): 24059.27380954028\n\nXGBoost Regression with Parameter Tuning\n"},"7":{"data":{"image/png":"9fcca82165af8c558cafcb6eede3236af9396c95","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":411,"width":725},"needs_background":"light"}},"8":{"name":"stdout","text":"Mean squared error (MSE): 20480.226460127138\n\nPolynomial Regression using Linear Regression\n"},"9":{"data":{"image/png":"7cd3a321808cc52cbb32b9c2aafe47f67d16c5d5","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":411,"width":725},"needs_background":"light"}}},"pos":17,"start":1601448683119,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448685369,"exec_count":10,"id":"9a47ef","input":"my_seed = 7\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\nnum_classes = 5\n\n# We generate 2000 clustered 2D data points sampled from 5 normally-distributed clusters\n#   with each cluster representing a different class.\nX, y = make_blobs(n_samples=2000, centers=num_classes,\n                  random_state=my_seed, cluster_std=2.0)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n\n","kernel":"python3","no_halt":true,"output":{"0":{"data":{"image/png":"9f0a509ac8b9a7689372e4bd678580d383a6de72","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":411,"width":712},"needs_background":"light"}}},"pos":20,"start":1601448685001,"state":"done","type":"cell"}
{"cell_type":"code","end":1601448685663,"exec_count":11,"id":"999adb","input":"random.seed(42)\nnp.random.seed(42)\nx_names = ['x']\ndf = pd.DataFrame(np.random.uniform(low=-5, high=5, size=(1000, 1)), columns=x_names)\ndf['y'] = df['x'].map(lambda x: 42 + 20 * math.sin(x) + x * x + (-10 if round(x) % 2 == 0 else 0) + np.random.normal(scale=1))\nplt.scatter(df['x'], df['y'])  # scatter plot of original data\n\n","kernel":"python3","no_halt":true,"output":{"0":{"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7f7718693b50>"},"exec_count":11},"1":{"data":{"image/png":"3ba0069d000e078139aaab2cf6546618fc23f643","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":411,"width":703},"needs_background":"light"}}},"pos":22,"start":1601448685398,"state":"done","type":"cell"}
{"cell_type":"code","id":"16c2a3","input":"","pos":25,"type":"cell"}
{"cell_type":"code","id":"423b94","input":"","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"0aa7e9","input":"## Homework\n","pos":23,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"12f32e","input":"## Regression Trees\n\nDecision trees are usually classification trees that predict categorical values, i.e. discrete unordered values.\nDecision trees that predict ordered continuous values (typically real numbers) are called _regression trees_.  To illustrate, we will load our previous degree-4 polynomial data and show regression performance with regression analogues of the models above.  \n\nYou will see that plain decision tree regression badly overfits, yet has a high $R^2$ score.  For regressions, it is common to use _mean squared error_ (MSE), i.e. the average squared distance of a prediction from the true value, as the error measure.  Whereas a higher $R^2$ score is better and a perfect $R^2$ score is 1, a lower MSE is better and a perfect MSE is 0.","pos":16,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1300b6","input":"To see more detail of misclassifications, we can also show a _confusion matrix_ where each row is the known class and each column is the predicted class.  This shows us that classes 0 and 2 are most often being confused with each other.","pos":11,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"3270d9","input":"# Decision Trees, Random Forests, and Gradient Boosted Decision Trees\n\nLearning Objectives:\n* Students will learn what a decision tree is and how it applies to classification and regression.\n* Students will understand how a combination of decision trees as a random forest improves models.\n* Students will interactively learn about and apply gradient boosted decision trees.\n\nReadings before class:\n* Jake VanderPlas.  [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/):\n  * [Chapter 5 - Only read section \"In Depth: Decision Trees and Random Forests\"](https://jakevdp.github.io/PythonDataScienceHandbook/) _Below, we summarize this reading and further illustrate it.  Spend focused attention here._\n* Alex Rogozhnikov’s beautiful demonstration: [Gradient Boosting Explained](https://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)  _Not only is this a great interactive visual illustration of concepts, but it also has embedded explanations.  Don't skip over the underlined explanation links (\"how is the tree built?\", \"what is gradient boosting?\", \"interesting things\") - when clicked on, they reveal further text and explanations._\n\nIn class:\n* We will work together on the exercises in section \"In Class\".\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.\n","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"548bd6","input":"(end of homework)","pos":28,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"6412c0","input":"Perhaps the most popular family of decision-tree-related methods are _gradient-boosted machines_, especially _gradient-boosted decision trees_ and _gradient-boosted regression trees_.  The basic idea of these is that a base predicting is made, and successive trees are build to predict and correct error of the weighted combination of previous models.\n\n[XGBoost](https://xgboost.readthedocs.io/en/latest/) is the most popular of these.\n\n> \"Among the 29 challenge winning solutions published at Kaggle’s blog during 2015, 17 solutions [~59%] used XGBoost. Among these solutions, eight [~28%] solely used XGBoost to train the model, while most others combined XGBoost with neural nets in ensembles.\" - Tianqi Chen, Carlos Guestrin. [“XGBoost: A Scalable Tree Boosting System”](https://arxiv.org/pdf/1603.02754.pdf)\n","pos":13,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"730878","input":"# In Class\n\nFor all in-class exercises, perform a 50/50 training/testing data split.  Train models on the training data.  Test models on the testing data.\n\n## Classification\n\nFor the following classification data:\n* Compute a 50/50 training/testing data split.\n* Compute a classification model with the training data.\n* Visualize the classification model with a scatterplot of testing data.\n* Print the testing data accuracy score for each class.\n* Plot the confusion matrix.\n\nDo this for each of the following classification models:\n* Decision Tree\n* Random Forest\n* XGBoost Classifier\n* $k$-Nearest Neighbor with best $k$\n","pos":19,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"73690a","input":"## Regression\n\nFor the following regression data:\n* Compute a 50/50 training/testing data split.\n* Compute a regression model with the training data.\n* Visualize the regression model curve with a scatterplot of testing data.\n* Print the MSE for the testing data.\n\nDo this for each of the following regression models:\n* Regression Tree\n* Random Forest Regression\n* XGBoost Regressor (untuned)\n* $k$-Nearest Neighbor Regression with $k=3$ ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html))\n","pos":21,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"7ba6f4","input":"This is better, but the nature of the decision boundaries makes this model \"blocky\", strictly dividing along single dimension value boundaries.  There are a number of ways we can improve this.  One way is to create a large number of _randomized_ overfitting decision trees and have them \"vote\" for classifications.  (In regression applications, we would average predicted values.)  How are the trees randomized?  We can randomize the input variable choices for the decisions at each node.\n\nThis is the beginning of a theme you will see recur in many predictive approaches: An ensemble of imperfect predictors can together yield an improved predictor.  Think of this as a \"wisdom of the masses\" analogue.","pos":7,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"80af8b","input":"(The classification areas will look slightly different because the bounds of the training and testing dataset are slightly different, and there are some very thin sections that will show or not depending on the bounds at this resolution.)\n\nThese are some rather odd decision boundaries.  Overfitting is a characteristic of decision tree that branch until subsets of points are \"pure\", i.e. having the same classification or regression value.  One way to reduce overfitting is to limit the depth of a tree.  Here is the effect of limiting the tree to depth 3.\n","pos":5,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"8fc1ed","input":"**Exercise 3:** Model and regress the data of [hw12-2.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw12/hw12-2.csv) with all of the steps and models of your in-class regression work.","pos":26,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"98bfa3","input":"# Decision Trees\n\nPerhaps you have played the number guessing game as a child. \"I'm thinking of a number from 1 to 100.\"  \"50?\"  \"Lower.\"  \"25?\"  \"Higher.\" ...\n\nIt's unlikely that you've ever had a player guess like this: \"I'm thinking of a number from 1 to 100.\"  \"1?\"  \"Higher.\"  \"2?\"  \"Higher.\" \"3?\"  \"Higher.\" \"4?\"  \"Higher!\" ...\n\nWhy are the first and second dialogs, a good and bad way to play, respectively?  In the first case, we are gaining the most information, dividing outcomes as evenly as possible.  In the second case, we merely eliminate one more possibility each time.\n\nA decision tree is, as the name implies, a tree of decisions.  Let us look at Wikipedia's simple Kaggle Titanic dataset decision tree:\n\n![Wikipedia's simple Kaggle Titanic dataset decision tree](https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/Decision_Tree.jpg/440px-Decision_Tree.jpg)\n\nEach decision node seeks to divide data into similar instances until we have enough information to make a helpful prediction at the terminal nodes, the tips of branches.  Rules vary for when a node terminates instead of branching further.  One method might continue dividing until variance among training instances that travel the same branches falls below a threshold.  Other methods are depth-limited, always terminating branches at a given depth limit.  Regardless, the core idea is the same: Form question nodes that partition the data so that variance in predictions is reduced, and at terminal nodes, make a prediction (classification or regression).\n\nLet us begin by generating data as we saw in the VanderPlas text:\n","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"9ebe44","input":"Far and away, our previous polynomial regression works best here, but one must consider:\n\n1. If we know the underlying mathematical model for the data, it is easy to choose a simple, suitable statistical model.\n2. Real-world data isn't often so simple as a polynomial with noise, and in many cases, one prefers a tool that \"just works\" decently for a wide variety of problems.\n\nThe superiority of directly modeling a functional form with know plus noise isn't a summary judgment against these overfitting tree-based regressions.  Rather, it's a wonder that ensembles of tree work as well as they do for a variety of classification and regression tasks!\n","pos":18,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a34f29","input":"**Exercise 1:** Complete the in-class exercises if you haven't already.\n\n**Exercise 2:** Model and classify the data of [hw12-1.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw12/hw12-1.csv) with all of the steps and models of your in-class classification work.  _However_, you must first preprocess and normalize your data.\n\nTip: If you have problems with an error \"TypeError: '(slice(None, None, None), 0)' is an invalid key\", use the following when you split your data:\n\n```X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, train_size=0.5, random_state=my_seed, shuffle=True)```\n\nMore on this in chapters on ```numpy``` and ```pandas```.\n","pos":24,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b2e33d","input":"While our XGBoost performance slightly better than the best previous performance, we can see that the classification boundaries are the simplest and cleanest yet.","pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"bed543","input":"We can also build each randomized tree based on different subsets of the training data (e.g. a random 80% of the training set) as shown below:","pos":9,"state":"done","type":"cell"}
{"id":0,"time":1601448588924,"type":"user"}
{"last_load":1601448573292,"type":"file"}