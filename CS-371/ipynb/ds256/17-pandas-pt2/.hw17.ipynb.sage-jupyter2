{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":82485248},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"type":"settings"}
{"cell_type":"code","exec_count":1,"id":"abca47","input":"# Place your imports here.\n\nimport numpy as np\nimport pandas as pd\nimport random\nrandom.seed(0)\nnp.random.seed(0)  # seed for reproducibility\n","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"26118b","input":"print('\\ndf_ints', df_ints, sep='\\n')\n\n# Print the median value of each column of df_ints (defined above).\n\n\n\n\n# Print the sum of all values of each row of df_ints.\n\n\n\n\n# Below is a randomly generated DataFrame df_rand with labels a, b, and c.\nlabels=list('abc' * 10)\nrandom.shuffle(labels)\nrand_data = {'label':labels, 'value':np.random.normal(loc=10, scale=20, size=(len(labels)))}\ndf_rand = pd.DataFrame(data=rand_data, columns=list(rand_data.keys()))\nprint('\\ndf_rand.head()', df_rand.head(), sep='\\n')\n# Group by label and transform each label's values to have mean of 0 and standard deviation by\n# subtracting the group mean and then dividing by the group standard deviation.\n# (Assign the transformed values back to the 'value' column.)\n# This can be done in one line.\n\n\n\n# This is test code to see if you have successfully normalized the data by label.\nprint('\\nThe mean and std should be approximately 0 and 1 respectively:', df_rand.groupby('label').aggregate([np.mean, np.std]), sep='\\n')","output":{"0":{"name":"stdout","output_type":"stream","text":"\ndf_ints\n     a    b    c    d    e\n0  100 -104 -186  205  -14\n1   51 -413 -326  100  349\n2  177   37  345 -428  277\n3  416 -385  476  255  209\n4  347  -69  -52  350 -401\n\ndf_rand.head()\n  label      value\n0     b   3.041757\n1     c  13.126979\n2     b  34.605814\n3     c  34.047597\n4     b   2.253464\n\nThe mean and std should be approximately 0 and 1 respectively:\n          value           \n           mean        std\nlabel                     \na      6.025357   9.316300\nb     -3.502713  16.561677\nc      7.634104  23.926277\n"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"024adc","input":"# Assign s3 to be the Pandas concatenation of the two following Series.\n# Read the documentation of concat to see how you can ignore the s1 and s2 indices,\n# resulting in s3 having indices 0 through 5.\n# Print the result.\ns1 = pd.Series(['x1', 'x2', 'x3'])\ns2 = pd.Series(['x4', 'x5', 'x6'])\nprint('\\ns1', s1, sep='\\n')\nprint('\\ns2', s2, sep='\\n')\n\n\n\n\n# Assign df3 to be the Pandas concatenation of the two following DataFrames that adds the rows of df2 after those of df1.\n# One way to deal with them having the same indices is to create a multi-index.\n# Do so with top-level indices 'dataset1' and 'dataset2', such that printed output looks like this:\n# df3\n#             x1  x2  x3\n# dataset1 0  44  47  64\n#          1  67  67   9\n# dataset2 0  83  21  36\n#          1  87  70  88\ndf1 = pd.DataFrame(np.random.randint(100, size=(2, 3)), columns=['x' + str(i) for i in range(1, 4)])\ndf2 = pd.DataFrame(np.random.randint(100, size=(2, 3)), columns=['x' + str(i) for i in range(1, 4)])\nprint('\\ndf1', df1, sep='\\n')\nprint('\\ndf2', df2, sep='\\n')\n\n\n\n\n# We will now reassign columns so that df1 and df2 each have a column the other does not.\ndf2.columns = ['x' + str(i) for i in range(2, 5)]\nprint('\\ndf1', df1, sep='\\n')\nprint('\\ndf2', df2, sep='\\n')\n# Assign df3 to be the appending of df2 to the end of df1 using an outer join that yields a 4 row, 4 column result with NaN values.\n# Indices should be 0 through 3.  Print the result.\n\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"\ns1\n0    x1\n1    x2\n2    x3\ndtype: object\n\ns2\n0    x4\n1    x5\n2    x6\ndtype: object\n\ndf1\n   x1  x2  x3\n0  44  47  64\n1  67  67   9\n\ndf2\n   x1  x2  x3\n0  83  21  36\n1  87  70  88\n\ndf1\n   x1  x2  x3\n0  44  47  64\n1  67  67   9\n\ndf2\n   x2  x3  x4\n0  83  21  36\n1  87  70  88\n"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"d34c75","input":"# We now define df4 and df5 that share a common column course_id:\ncourse_data = [['2020_spring', 'CS111A', 20197],\n    ['2020_spring',' CS111B', 20364],\n    ['2020_fall', 'CS111A', 80179],\n    ['2020_fall', 'CS111B', 80180]]\ndf4 = pd.DataFrame(data=[[row[0], row[2]] for row in course_data], columns=['semester', 'course_id'])\nrandom.shuffle(course_data)\ndf5 = pd.DataFrame(data=[[row[1], row[2]] for row in course_data], columns=['section', 'course_id'])\nprint('\\ndf4', df4, sep='\\n')\nprint('\\ndf5', df5, sep='\\n')\n# Compute and print DataFrame df6 as a merge of df4 and df5 with a one-to-one join on course_id.\n\n\n\n\n# Create and print DataFrames df7 and df8 such that they print like this:\n#\n# df7\n#   x1\n# b  f\n# a  o\n# r  o\n#\n# df8\n#   x2\n# b  b\n# r  a\n# a  z\n#\n# Compute and print DataFrame df9 as the join of df7 and df8.\n# By default, the DataFrame join method will join on the indices (letters 'b', 'a', and 'r' in this case)\n\n\n\n\n# Compute and print df12, an outer join of df10 and df11 (using merge).\ndata1 = {'letter':['A', 'B', 'C'], 'phonetic':['Alfa', 'Bravo', 'Charlie']}\ndata2 = {'letter':['C', 'J', 'M'], 'name': ['Charles', 'Juliett', 'Mike']}\ndf10 = pd.DataFrame(data=data1, columns=list(data1.keys()))\ndf11 = pd.DataFrame(data=data2, columns=list(data2.keys()))\nprint('\\ndf10', df10, sep='\\n')\nprint('\\ndf11', df11, sep='\\n')\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"\ndf4\n      semester  course_id\n0  2020_spring      20197\n1  2020_spring      20364\n2    2020_fall      80179\n3    2020_fall      80180\n\ndf5\n   section  course_id\n0   CS111A      80179\n1   CS111A      20197\n2   CS111B      20364\n3   CS111B      80180\n\ndf10\n  letter phonetic\n0      A     Alfa\n1      B    Bravo\n2      C  Charlie\n\ndf11\n  letter     name\n0      C  Charles\n1      J  Juliett\n2      M     Mike\n"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"a3cdfd","input":"df_ints = pd.DataFrame(data=np.random.randint(1000, size=(5, 5)) - 500, columns=list('abcde'))\nprint('\\ndf_ints', df_ints, sep='\\n')\n\n# Print the maximum values of each column of df_ints.\n\n\n\n\n# Print the minimum values of each row of df_ints.\n\n\n\n\n# Use groupby and sum aggregation to compute and print the sum of values associated with each key of df_xyzzy,\n# with separate sums for data1 and data2 columns.\ndf_xyzzy = pd.DataFrame(data={'key': list('XYZZY'),\n                   'data1': np.random.randint(1000, size=(5)),\n                   'data2': np.random.randint(1000, size=(5))}, columns=['key', 'data1', 'data2'])\nprint('\\ndf_xyzzy', df_xyzzy, sep='\\n')\n\n\n\n\n# Use filtering to print only the rows of df_xyzzy where the sum of 'key' data1 or data2 associated values have a sum > 1000.\n\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"\ndf_ints\n     a    b    c    d    e\n0  100 -104 -186  205  -14\n1   51 -413 -326  100  349\n2  177   37  345 -428  277\n3  416 -385  476  255  209\n4  347  -69  -52  350 -401\n\ndf_xyzzy\n  key  data1  data2\n0   X    984    147\n1   Y    177    910\n2   Z    755    423\n3   Z    797    288\n4   Y    659    961\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"281d43","input":"# Assign df3 to be the Pandas concatenation of the two following DataFrames.\n# Read the documentation of concat to see how you can ignore the df1 and df2 indices,\n# resulting in df3 having indices 0 through 3.\n# Print the result.\ndf1 = pd.DataFrame(np.random.randint(100, size=(2, 3)), columns=['x' + str(i) for i in range(1, 4)])\ndf2 = pd.DataFrame(np.random.randint(100, size=(2, 3)), columns=['x' + str(i) for i in range(1, 4)])\nprint('\\ndf1', df1, sep='\\n')\nprint('\\ndf2', df2, sep='\\n')\n\n\n\n\n# We will now reassign columns so that df1 and df2 each have a column the other does not.\ndf2.columns = ['x' + str(i) for i in range(2, 5)]\nprint('\\ndf1', df1, sep='\\n')\nprint('\\ndf2', df2, sep='\\n')\n# Assign df3 to be the Pandas concatenation of df1 and df2 using an outer join that yields a 4 row, 4 column result with NaN values.\n# Indices should be 0 through 3.  Print the result.\n\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"\ndf1\n   x1  x2  x3\n0   9  57  32\n1  31  74  23\n\ndf2\n   x1  x2  x3\n0  35  75  55\n1  28  34   0\n\ndf1\n   x1  x2  x3\n0   9  57  32\n1  31  74  23\n\ndf2\n   x2  x3  x4\n0  35  75  55\n1  28  34   0\n"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"56fd41","input":"# We now define df4 and df5 that share a common column course_id:\ncourse_data = [['2020_spring', 'CS111A', 20197],\n    ['2020_spring','CS111B', 20364],\n    ['2020_fall', 'CS111A', 80179],\n    ['2020_fall', 'CS111B', 80180]]\nsection_letter_data = [['CS111A', 'A'], ['CS111B', 'B']]\n# Create and print DataFrame df4, which should contain course_data with columns 'semester', 'section', and 'course_id'.\n# Create and print DataFrame df5, which should contain section_letter_data with columns 'section' and 'section_letter'.\n# Compute and print DataFrame df6 as a merge of df4 and df5 with a many-to-one join on section.\n# It should have the same number of rows as df4, but with additional column 'section_letter'.\n\n\n\n\n# We now define df7 and df8 to have course_id as their indices.\ndf7 = df4[['course_id', 'semester']].copy()\ndf8 = df4[['course_id', 'section']].copy() # This will generate an error until you complete the previous exercise.\ndf7 = df7.set_index('course_id')\ndf8 = df8.set_index('course_id')\nprint('\\ndf7', df7, sep='\\n')\nprint('\\ndf8', df8, sep='\\n')\n# Compute and print DataFrame df9 as a one-to-one merge of df7 and df8 using left_index and right_index parameters.\n\n\n\n\n# Next, we redefine df8 with the same data, except 'course_id' is a column, not the index.\n# Compute and print df9 as a merge of df7 and df8, using the course_id of each for the merge.\n# Make sure the result still has 'course_id' as its index so the result appears as the prior exercise.\ndf8 = df4[['course_id', 'section']].copy()\nprint('\\ndf7', df7, sep='\\n')\nprint('\\ndf8', df8, sep='\\n')\n\n\n\n\n# Compute and print df12, the left join of df10 and df11 (using merge).\ndata1 = {'x1':list('ABC'), 'x2':list('XYZ')}\ndata2 = {'x1':list('BCD'), 'x3':list('PDQ')}\ndf10 = pd.DataFrame(data=data1, columns=list(data1.keys()))\ndf11 = pd.DataFrame(data=data2, columns=list(data2.keys()))\nprint('\\ndf10', df10, sep='\\n')\nprint('\\ndf11', df11, sep='\\n')\n\n\n","output":{"0":{"ename":"KeyError","evalue":"\"['section'] not in index\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-6adbc1c24837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# We now define df7 and df8 to have course_id as their indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'course_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'semester'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'course_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'section'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This will generate an error until you complete the previous exercise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mdf7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'course_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdf8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'course_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2906\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2908\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['section'] not in index\""]}},"pos":11,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"2fe1a0","input":"print('\\ndf_ints', df_ints, sep='\\n')\n\n# Print the mean value of each column of df_ints (defined above).\n\n\n\n\n# Print the standard deviation values of each row of df_ints.\n\n\n\n\n# Use groupby and aggregation to compute and print the minimum and maximum values associated with each key of df_xyzzy,\n# with data1 and data2 being first-level column names, and 'min' and 'max' being second-level in the hierarchical column indexing.\n# The output should look like this:\n#\n#     data1      data2     \n#       min  max   min  max\n# key                      \n# X     984  984   147  147\n# Y     177  659   910  961\n# Z     755  797   288  423\nprint('\\ndf_xyzzy', df_xyzzy, sep='\\n')\n\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"\ndf_ints\n     a    b    c    d    e\n0  100 -104 -186  205  -14\n1   51 -413 -326  100  349\n2  177   37  345 -428  277\n3  416 -385  476  255  209\n4  347  -69  -52  350 -401\n\ndf_xyzzy\n  key  data1  data2\n0   X    984    147\n1   Y    177    910\n2   Z    755    423\n3   Z    797    288\n4   Y    659    961\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"9caf59","input":"# Assign df3 to be the Pandas concatenation of the two following DataFrames, concatenating rows.\n# Use \"axis=1\", rather than \"axis='col'\".\n# Print the resulting df3 which should have 2 rows and 6 columns.\ndf1 = pd.DataFrame(np.random.randint(100, size=(2, 3)), columns=['x' + str(i) for i in range(1, 4)])\ndf2 = pd.DataFrame(np.random.randint(100, size=(2, 3)), columns=['x' + str(i) for i in range(4, 7)])\nprint('\\ndf1', df1, sep='\\n')\nprint('\\ndf2', df2, sep='\\n')\n\n\n\n\n# We will now reassign columns so that df1 and df2 each have a column the other does not.\ndf2.columns = ['x' + str(i) for i in range(2, 5)]\nprint('\\ndf1', df1, sep='\\n')\nprint('\\ndf2', df2, sep='\\n')\n# Assign df3 to be the Pandas concatenation of df1 and df2 using an inner join that yields a 4 row, 2 column result with no NaN values.\n# Indices should be 0 through 3.  Print the result.\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"\ndf1\n   x1  x2  x3\n0   0  36  53\n1   5  38  17\n\ndf2\n   x4  x5  x6\n0  79   4  42\n1  58  31   1\n\ndf1\n   x1  x2  x3\n0   0  36  53\n1   5  38  17\n\ndf2\n   x2  x3  x4\n0  79   4  42\n1  58  31   1\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"65ac25","input":"# Assign df6 to be the many-to-many merge of the two following DataFrames.\n# Print and observe the result.\ndf4 = pd.DataFrame(data=[['N', 'A'], ['I', 'D'], ['C', 'D'], ['E', 'R']], columns=['x1', 'x2'])\ndf5 = pd.DataFrame(data=[['R', 'P'], ['A', 'I'], ['D', 'N'], ['A', 'G'], ['R', 'S']], columns=['x2', 'x3'])\nprint('\\ndf4', df4, sep='\\n')\nprint('\\ndf5', df5, sep='\\n')\n\n\n\n\n# We now change the column names of df5 to be 'y1' and 'y2'.\ndf5.columns = ['y1', 'y2']\nprint('\\ndf5', df5, sep='\\n')\n# Perform the same computation of df6, but specify that 'x2' and 'y1' are the columns used in the merge.\n# Then drop 'y1' and print the result, which should be the same as the prior result except that 'x3' is now 'y2'.\n\n\n\n\n# Compute and print df9, an inner join of df7 and df8 (using merge).\ndata1 = {'letter':['A', 'B', 'C'], 'phonetic':['Alfa', 'Bravo', 'Charlie']}\ndata2 = {'letter':['C', 'J', 'M'], 'name': ['Charles', 'Juliett', 'Mike']}\ndf7 = pd.DataFrame(data=data1, columns=list(data1.keys()))\ndf8 = pd.DataFrame(data=data2, columns=list(data2.keys()))\nprint('\\ndf7', df7, sep='\\n')\nprint('\\ndf8', df8, sep='\\n')\n\n\n\n\n# Sometime objects in different databases have different identifiers and we want to retain both in a merge of data.\n# For example, Magic: The Gathering playing cards have a Scryfall id and a MTG Arena id.\nscryfall_data = {'card_name':['Pacifism', 'Llanowar Elves', 'Fblthp, the Lost'], 'id':[\"9e0671ff-ad06-43ae-87cd-06a1341e971b\", \"581b7327-3215-4a4f-b4ae-d9d4002ba882\", \"52558748-6893-4c72-a9e2-e87d31796b59\"]}\narena_data = {'card_name':['Pacifism', 'Llanowar Elves', 'Fblthp, the Lost'], 'id':[75457, 67440, 69501]}\ndf_scryfall = pd.DataFrame(data=scryfall_data, columns=list(scryfall_data.keys()))\ndf_arena = pd.DataFrame(data=arena_data, columns=list(arena_data.keys()))\nprint('\\ndf_scryfall', df_scryfall, sep='\\n')\nprint('\\ndf_arena', df_arena, sep='\\n')\n# Create and print DataFrame df_card_ids that is the merge of df_scryfall and df_arena on card_name, and adding suffixes '_scryfall' and '_arena' to the 'id' columns.\n# When printed, it should look like this:\n#\n#           card_name                           id_scryfall  id_arena\n# 0          Pacifism  9e0671ff-ad06-43ae-87cd-06a1341e971b     75457\n# 1    Llanowar Elves  581b7327-3215-4a4f-b4ae-d9d4002ba882     67440\n# 2  Fblthp, the Lost  52558748-6893-4c72-a9e2-e87d31796b59     69501\n\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"\ndf4\n  x1 x2\n0  N  A\n1  I  D\n2  C  D\n3  E  R\n\ndf5\n  x2 x3\n0  R  P\n1  A  I\n2  D  N\n3  A  G\n4  R  S\n\ndf5\n  y1 y2\n0  R  P\n1  A  I\n2  D  N\n3  A  G\n4  R  S\n\ndf7\n  letter phonetic\n0      A     Alfa\n1      B    Bravo\n2      C  Charlie\n\ndf8\n  letter     name\n0      C  Charles\n1      J  Juliett\n2      M     Mike\n\ndf_scryfall\n          card_name                                    id\n0          Pacifism  9e0671ff-ad06-43ae-87cd-06a1341e971b\n1    Llanowar Elves  581b7327-3215-4a4f-b4ae-d9d4002ba882\n2  Fblthp, the Lost  52558748-6893-4c72-a9e2-e87d31796b59\n\ndf_arena\n          card_name     id\n0          Pacifism  75457\n1    Llanowar Elves  67440\n2  Fblthp, the Lost  69501\n"}},"pos":17,"type":"cell"}
{"cell_type":"markdown","id":"259133","input":"# Homework\n\n(0) Complete the in-class exercises.  (This may be done with others beyond your assigned pairs.)\n\n(1) Combining Datasets: Concat and Append","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"34adb5","input":"(2) Combining Datasets: Merge and Join","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"51b18e","input":"# Pandas Exercises\n\n**To-Do:** Complete all of the exercises below before class.\n\n## Combining Datasets: Concat and Append","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"664bd2","input":"## Combining Datasets: Merge and Join","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"7d7ab5","input":"(3) Aggregation and Grouping","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"85abe7","input":"## Combining Datasets: Merge and Join","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"8f92c2","input":"## Aggregation and Grouping","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"daa1fd","input":"# Pandas - Part 2\n\nLearning Objectives:\n* Students will experientially gain competence in combining Pandas Series and DataFrame objects with concat, append, merge and join.\n* Students will experientially gain competence in performing aggregation and grouping operations Series and DataFrame objects.\n\nReadings before class:\n\n* Jake VanderPlas.  [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/):\n  * Chapter 3 - Data Manipulation with Pandas sections:\n    * [\"Combining Datasets: Concat and Append\"](https://jakevdp.github.io/PythonDataScienceHandbook/03.06-concat-and-append.html) _NOTE: ```pandas.concat``` no longer supports parameter ```join_axes``` as used in the text.  It is deprecated, meaning that it shouldn't be used henceforth.  Better software design allows for backwards compatability and suggests deprecation so that programmers don't need to be continually rewriting their code._\n    * [\"Combining Datasets: Merge and Join\"](https://jakevdp.github.io/PythonDataScienceHandbook/03.07-merge-and-join.html)\n    * [\"Aggregation and Grouping\"](https://jakevdp.github.io/PythonDataScienceHandbook/03.08-aggregation-and-grouping.html)\n\nBefore class:\n* Complete all of the readings exercises below as you do the reading.  You are encouraged to add code blocks and play with the forms to gain understanding and comfort with them.\n\nIn class:\n* We will work together on the exercises in section \"In Class\".\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"e1eed5","input":"(end of homework)","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"e2c94a","input":"## Aggregation and Grouping","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"e65f56","input":"# In Class\n\nFirst, check your pre-class work above with each other.\n\nThen, work together to complete the following exercises.\n\n## Combining Datasets: Concat and Append","pos":8,"type":"cell"}
{"id":0,"time":1602833520805,"type":"user"}
{"last_load":1602833283632,"type":"file"}