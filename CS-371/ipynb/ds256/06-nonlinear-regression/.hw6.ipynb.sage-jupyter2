{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":65396736},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"30833b","input":"","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"477473","input":"","pos":54,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a37925","input":"","pos":52,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b44fd8","input":"","pos":56,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"4d887d","input":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.tri as mtri\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\n\n# We begin by importing our entire pseudorandom number generator (PRNG) module and giving it a \"seed\" for reproducability:\nfrom random import *\n\n# The PRNG is \"pseudorandom\" because the seemingly random number generation is actually a sort of \"numeric blender\" function that is deterministic,\n# generates different sequences with high probability, and appears random in distribution and according to various statistical tests.\nseed(0)\nprint(random())\nprint(random())\n\n# The seed number creates the initial internal state of the PRNG, so starting with the same seed we get the same behavior:\nprint(\"Same seed, same PRNG behavior:\")\nseed(0)\nprint(random())\nprint(random())\n\n# We'll next create a number of random coefficients that we'll use in our examples, filtered to make sure they are not too small in magnitude.\ncoeffs = list(filter(lambda x: abs(x) >= 1, map(lambda x: 40 * (random() - .5), 100 * [1])))\n\n# Now we'll create the dataframes we'll use in most examples below.\nnp.random.seed(0)  # since we'll be using numpy's PRNG as well, we'll need to set its initial state with a seed too.\n\ndf1 = pd.DataFrame(np.random.uniform(low=-0.5, high=4.0, size=(20, 1)), columns=['x'])  # generate 20 random x values from -.5 to 4\nc1 = sample(coeffs, 2)  # random selection of y intercept and coefficient from coeffs\nm1 = lambda x: c1[0] + c1[1] * x\n# generate y's from a linear function of x with Gaussian noise.\ndf1['y'] = df1['x'].map(lambda x: m1(x) + np.random.normal())\n\ndf2 = pd.DataFrame(np.random.uniform(low=-5, high=5, size=(100, 1)), columns=['x'])\nc2 = sample(coeffs, 3)\nm2 = lambda x: c2[0] + c2[1] * x + c2[2] * x * x\n# generate y's from a nonlinear function of x with Gaussian noise.\ndf2['y'] = df2['x'].map(lambda x: m2(x) + np.random.normal(scale=10))\n\ndf3 = pd.DataFrame(np.random.uniform(low=-4, high=4, size=(100, 1)), columns=['x'])\nc3 = sample(coeffs, 1)\nm3 = lambda x: c3[0] ** x\ndf3['y'] = df3['x'].map(lambda x: abs(m3(x) + np.random.normal(scale=.01)))\n\ndf4 = pd.DataFrame(np.random.uniform(low=-5, high=5, size=(100, 1)), columns=['x'])\nc4 = sample(coeffs, 4)\nm4 = lambda x: c4[0] + c4[1] * x + c4[2] * x * x + c4[3] * x * x * x\n# generate y's from a nonlinear function of x with Gaussian noise.\ndf4['y'] = df4['x'].map(lambda x: m4(x) + np.random.normal(scale=100))\n\ndf5 = pd.DataFrame(np.random.uniform(low=-5, high=5, size=(500, 2)), columns=['x1', 'x2'])\nc5 = sample(coeffs, 4)\n# generate a nonlinear multiple regression featuring an interaction term\ndf5['y'] = df5.apply(lambda x: c5[0] + c5[1] * x['x1'] * x['x1'] + c5[2] * x['x1'] * x['x2'] + c5[3] * x['x2'] * x['x2'] + np.random.normal(scale=5), axis=1)\n\ndf6 = pd.DataFrame(np.random.uniform(low=-5, high=5, size=(500, 5)), columns=['x1', 'x2', 'x3', 'x4', 'x5'])\nc6 = sample(coeffs, 4)\ndf6['y'] = df6.apply(lambda x: c6[0] + c6[1] * x['x3'] + c6[2] * x['x3'] * x['x3'] + c6[3] * x['x4'] + np.random.normal(scale=1), axis=1)\n\ndf7 = pd.DataFrame(np.random.uniform(low=-5, high=5, size=(500, 3)), columns=['x1', 'x2', 'x3'])\nc7 = sample(coeffs, 4)\nm7 = lambda x: c7[0] + c7[1] * x + c7[2] * x * x + c7[3] * x * x * x\ndf7['y'] = df7['x1'].map(lambda x: m7(x) + np.random.normal(scale=10))\n","output":{"0":{"name":"stdout","output_type":"stream","text":"0.8444218515250481\n0.7579544029403025\nSame seed, same PRNG behavior:\n0.8444218515250481\n0.7579544029403025\n"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"66fe02","input":"plt.figure(figsize=(10,10))  # set the figure size\nax = plt.gca(projection='3d')  # Initialize...\nax.view_init(elev=60, azim=-45)  # set the elevation and the azimuth (angles up-down, left-right) of the view\nax.scatter3D(df5['x1'], df5['x2'], df5['y'], s=1, c='black');  # x1-x2-y 3D scatterplot with size 1 dots and grayscale coloring\ntriang = mtri.Triangulation(df5['x1'], df5['x2'])  # create a triangular mesh between the irregular (i.e. non-grid-mesh) x1, x2 points\nax.plot_trisurf(triang, df5['y'], cmap='jet')  # rainbow-colored (\"jet\") surface plot of triangular mesh\nplt.show()","output":{"0":{"data":{"image/png":"58363021b1ddc1a030eb4a0159717efd17d9d805","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":10,"metadata":{"image/png":{"height":559,"width":578},"needs_background":"light"},"output_type":"execute_result"}},"pos":20,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"b51b85","input":"df5['x1sqr'] = df5['x1'].pow(2)\ndf5['x2sqr'] = df5['x2'].pow(2)\ndf5['x1x2'] = df5['x1']*df5['x2']\nprint(df5.sample(5))\ncorr = df5.corr()\nprint(corr)","output":{"0":{"name":"stdout","output_type":"stream","text":"           x1        x2           y      x1sqr      x2sqr       x1x2\n176  0.189899 -4.339257   71.376229   0.036062  18.829154  -0.824022\n311 -4.162086  0.161237  320.049529  17.322964   0.025997  -0.671082\n214  1.646656 -0.769456   38.045357   2.711476   0.592062  -1.267029\n369 -1.405468  4.003675   63.969374   1.975342  16.029410  -5.627038\n67  -3.584935  3.659455  206.869872  12.851760  13.391609 -13.118908\n             x1        x2         y     x1sqr     x2sqr      x1x2\nx1     1.000000  0.059270  0.076864  0.069997  0.082723 -0.007241\nx2     0.059270  1.000000  0.008370 -0.048182  0.048493  0.101870\ny      0.076864  0.008370  1.000000  0.892842  0.222547  0.387578\nx1sqr  0.069997 -0.048182  0.892842  1.000000  0.017380 -0.011793\nx2sqr  0.082723  0.048493  0.222547  0.017380  1.000000 -0.003311\nx1x2  -0.007241  0.101870  0.387578 -0.011793 -0.003311  1.000000\n"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"acb813","input":"X = df5[['x1', 'x2', 'x1sqr', 'x2sqr', 'x1x2']]  # select input(s)\ny = df5[['y']]  # select output\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('X coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [-8.70994056]\nX coefficient(s): [[-5.41731680e-03  4.00088872e-02  1.92893114e+01  4.53299684e+00\n   7.34810381e+00]]\nR^2 score: 0.9990676061538825\n"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"c2f129","input":"X = df5[['x1sqr', 'x2sqr', 'x1x2']]  # select input(s)\ny = df5[['y']]  # select output\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('X coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [-8.70442637]\nX coefficient(s): [[19.28841374  4.53360907  7.34947688]]\nR^2 score: 0.9990670826268384\n"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"96c5d7","input":"print(df6.describe())\nprint(df6.sample(5))\ncorr = df6.corr()\nprint(corr)\nplt.matshow(corr)\nplt.show()\n\nsns.pairplot(df6)  # create a seaborn pair plot\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"               x1          x2          x3          x4          x5           y\ncount  500.000000  500.000000  500.000000  500.000000  500.000000  500.000000\nmean    -0.113264   -0.108520    0.030768   -0.097809   -0.143990   12.499032\nstd      2.806599    2.923620    2.747174    2.925213    2.865771   55.641361\nmin     -4.982622   -4.999276   -4.949481   -4.964893   -4.981805  -88.829290\n25%     -2.553194   -2.631763   -2.134008   -2.594103   -2.614885  -33.867854\n50%     -0.044843   -0.069502   -0.010031   -0.323283   -0.074085   13.127945\n75%      2.311194    2.433680    2.384615    2.551153    2.204551   60.279181\nmax      4.934052    4.999490    4.958300    4.999311    4.999640  141.951461\n           x1        x2        x3        x4        x5           y\n468 -1.831574  1.760678 -1.048063 -0.515951  3.561792   -9.553707\n323 -1.209319 -3.858487  0.618129  1.415937  4.870915   29.685263\n411 -0.615233 -2.230544  4.528195  3.022408  0.863030  104.275111\n386  0.454002 -4.183287 -0.531403 -1.782756  2.905333  -31.832248\n443 -1.869545  4.241620  4.596729 -4.011694  2.354139  -20.819413\n          x1        x2        x3        x4        x5         y\nx1  1.000000 -0.016763 -0.051027 -0.000053 -0.038099 -0.008030\nx2 -0.016763  1.000000 -0.042748 -0.082674  0.009891 -0.102259\nx3 -0.051027 -0.042748  1.000000  0.058973  0.033272  0.234162\nx4 -0.000053 -0.082674  0.058973  1.000000  0.008657  0.962233\nx5 -0.038099  0.009891  0.033272  0.008657  1.000000  0.028191\ny  -0.008030 -0.102259  0.234162  0.962233  0.028191  1.000000\n"},"1":{"data":{"image/png":"95cb7880bd9a9856dcacefc993fe8b4016aaa06c","text/plain":"<Figure size 288x288 with 1 Axes>"},"exec_count":14,"metadata":{"image/png":{"height":257,"width":251},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"4f2d44250ca94f7ed01dfcb46f2f265fafae0989","text/plain":"<Figure size 1080x1080 with 42 Axes>"},"exec_count":14,"metadata":{"image/png":{"height":1089,"width":1097},"needs_background":"light"},"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"2d1798","input":"df6['x3sqr'] = df6['x3'].pow(2)\ncorr = df6.corr()\nprint(corr)\nplt.matshow(corr)\nplt.show()\n\n#sns.pairplot(df6)  # create a seaborn pair plot","output":{"0":{"name":"stdout","output_type":"stream","text":"             x1        x2        x3        x4        x5         y     x3sqr\nx1     1.000000 -0.016763 -0.051027 -0.000053 -0.038099 -0.008030  0.010361\nx2    -0.016763  1.000000 -0.042748 -0.082674  0.009891 -0.102259 -0.084237\nx3    -0.051027 -0.042748  1.000000  0.058973  0.033272  0.234162  0.070038\nx4    -0.000053 -0.082674  0.058973  1.000000  0.008657  0.962233  0.035776\nx5    -0.038099  0.009891  0.033272  0.008657  1.000000  0.028191  0.076527\ny     -0.008030 -0.102259  0.234162  0.962233  0.028191  1.000000  0.251343\nx3sqr  0.010361 -0.084237  0.070038  0.035776  0.076527  0.251343  1.000000\n"},"1":{"data":{"image/png":"ab65469f55b3da3ff8318c9682143a18b3bf46fe","text/plain":"<Figure size 288x288 with 1 Axes>"},"exec_count":15,"metadata":{"image/png":{"height":257,"width":251},"needs_background":"light"},"output_type":"execute_result"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"24a2b7","input":"X = df6[['x4']]  # select input(s)\ny = df6[['y']]  # select output\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('X coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [14.28922864]\nX coefficient(s): [[18.30293335]]\nR^2 score: 0.9258929319042501\n"}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"29ef9d","input":"X = df6[['x4', 'x3sqr']]  # select input(s)\ny = df6[['y']]  # select output\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('X coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [1.39654581]\nX coefficient(s): [[18.15513056  1.70961572]]\nR^2 score: 0.9730069480013654\n"}},"pos":35,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"0bbca1","input":"X = df6[['x4', 'x3sqr', 'x3']]  # select input(s)\ny = df6[['y']]  # select output\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('X coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [1.93845045]\nX coefficient(s): [[17.9787672   1.62182017  3.32146742]]\nR^2 score: 0.9996820274052338\n"}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"26d607","input":"X = df6[['x4', 'x3sqr', 'x3', 'x5']]  # select input(s)\ny = df6[['y']]  # select output\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('X coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [1.92961547]\nX coefficient(s): [[17.9788686   1.62253724  3.32215671 -0.02376627]]\nR^2 score: 0.9996835157652755\n"}},"pos":39,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"273a37","input":"# We start by describing the data,\nprint(df1.describe())\n\n# printing a sample of it,\nprint(df1.sample(5))\n\n# looking at the correlation matrix,\ncorr = df1.corr()\nprint(corr)\n\n# visualizing the matrix,\nplt.matshow(corr)\nplt.show()\n\n# and plotting the points.\nplt.scatter(df1['x'], df1['y']);","output":{"0":{"name":"stdout","output_type":"stream","text":"               x          y\ncount  20.000000  20.000000\nmean    2.116997 -12.091230\nstd     1.273874   4.731198\nmin    -0.409017 -18.374208\n25%     1.453468 -15.948530\n50%     2.134318 -12.235436\n75%     3.108769 -10.131847\nmax     3.836482  -3.206352\n           x          y\n15 -0.107918  -3.258523\n16 -0.409017  -3.379501\n11  1.880027 -11.383298\n1   2.718352 -14.589116\n19  3.415055 -16.876914\n          x         y\nx  1.000000 -0.967919\ny -0.967919  1.000000\n"},"1":{"data":{"image/png":"2484274b73abf15345c6aa02f307f586850d5b18","text/plain":"<Figure size 288x288 with 1 Axes>"},"exec_count":2,"metadata":{"image/png":{"height":257,"width":251},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"d518cc2d4412945f9a275fb09f923ade69f6385d","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":2,"metadata":{"image/png":{"height":248,"width":377},"needs_background":"light"},"output_type":"execute_result"}},"pos":4,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"7779e0","input":"X = df1[['x']]  # select input(s)\ny = df1[['y']]  # select output\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('x coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score\n\n# Incidentally, the true intercept and coefficient before noise addition were\nprint('true intercept and coefficient before noise addition:', c1)\n\nx_vals = np.linspace(start=X.min(), stop=X.max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\n# (We only need the min and the max for plotting the linear fit, but the previous expression\n#  will be helpful for plotting nonlinear fits.)\nyfit = linear_regressor.predict(x_vals)\nplt.scatter(X, y)\nplt.plot(x_vals, yfit, color='orange');\n","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [-4.48088849]\nx coefficient(s): [[-3.59487644]]\nR^2 score: 0.9368679649823967\ntrue intercept and coefficient before noise addition: [-4.04705831102925, -3.8026345019834284]\n"},"1":{"data":{"image/png":"154a527291604725288fb80c4318a5da0a340892","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":248,"width":377},"needs_background":"light"},"output_type":"execute_result"}},"pos":6,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"e8bb35","input":"print(df2.describe())\nprint(df2.sample(5))\ncorr = df2.corr()\nprint(corr)\nplt.matshow(corr)\nplt.show()\nplt.scatter(df2['x'], df2['y']);","output":{"0":{"name":"stdout","output_type":"stream","text":"                x           y\ncount  100.000000  100.000000\nmean    -0.224314   15.661395\nstd      2.749698   55.663883\nmin     -4.953045  -44.941152\n25%     -2.478430  -20.792577\n50%     -0.325190    1.195412\n75%      1.682173   23.925362\nmax      4.883738  198.704346\n           x          y\n34 -2.038598 -38.560945\n27 -0.313488 -15.040852\n41 -2.346105 -33.419104\n4  -1.845716 -28.928520\n95 -0.685816 -33.202390\n          x         y\nx  1.000000  0.723678\ny  0.723678  1.000000\n"},"1":{"data":{"image/png":"2484274b73abf15345c6aa02f307f586850d5b18","text/plain":"<Figure size 288x288 with 1 Axes>"},"exec_count":4,"metadata":{"image/png":{"height":257,"width":251},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"ff8251761deed3cc00c486eb1f0cda7504873a66","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":4,"metadata":{"image/png":{"height":248,"width":377},"needs_background":"light"},"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"cf4f87","input":"X = df2[['x']]  # select input(s)\nX['x2'] = df2['x'].pow(2)  # add a new column 'x2' with values from column 'x' squared\ny = df2[['y']]  # select output\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('x coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score\n\n# Incidentally, the true intercept and coefficient before noise addition were\nprint('true intercept and coefficients before noise addition:', c2)\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nXfit['x2'] = Xfit['x'].pow(2)  # add square terms as new column 'x2'\nyfit = linear_regressor.predict(Xfit)  # predict y's from the linearly spaced x and x2 data\nplt.scatter(X['x'], y)  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quadratic function in orange","output":{"0":{"name":"stdout","output_type":"stream","text":"y-intercept: [-19.69130802]\nx coefficient(s): [[15.48119693  5.1522942 ]]\nR^2 score: 0.9728297386356879\ntrue intercept and coefficients before noise addition: [-19.954287227422867, 16.086638017583308, 5.205893616458912]\n"},"1":{"data":{"image/png":"895a9d8970b2edc534cb69f3a4caa416004b2fa4","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":248,"width":377},"needs_background":"light"},"output_type":"execute_result"}},"pos":10,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"b64d7d","input":"print(df3.describe())\nprint(df3.sample(5))\ncorr = df3.corr()\nprint(corr)\nplt.matshow(corr)\nplt.show()\nplt.scatter(df3['x'], df3['y']);","output":{"0":{"name":"stdout","output_type":"stream","text":"                x           y\ncount  100.000000  100.000000\nmean    -0.077459    2.517835\nstd      2.293049    3.274090\nmin     -3.908580    0.065685\n25%     -2.047756    0.257683\n50%     -0.326161    0.803129\n75%      1.875823    3.492703\nmax      3.783356   12.460859\n           x         y\n39  1.565004  2.855809\n64 -0.556780  0.689711\n59 -1.545519  0.363020\n37 -0.350875  0.795903\n25 -3.283176  0.108885\n          x         y\nx  1.000000  0.852239\ny  0.852239  1.000000\n"},"1":{"data":{"image/png":"2484274b73abf15345c6aa02f307f586850d5b18","text/plain":"<Figure size 288x288 with 1 Axes>"},"exec_count":6,"metadata":{"image/png":{"height":257,"width":251},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"611b21313753ca9696e4218a970af82c76c94f5b","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":6,"metadata":{"image/png":{"height":248,"width":369},"needs_background":"light"},"output_type":"execute_result"}},"pos":12,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"d695e1","input":"X = df3[['x']]  # select input(s)\ndf3['y_log'] = np.log(df3['y'])  # select output\nprint(df3.sample(5))\ny = df3[['y_log']]\nlinear_regressor = LinearRegression()  # create linear regression object\nlinear_regressor.fit(X, y)  # perform linear regression of output onto inputs\nprint('y-intercept:', linear_regressor.intercept_)  # print the y-intercept\nprint('x coefficient(s):', linear_regressor.coef_)  # print the x coefficient\nprint('R^2 score:', linear_regressor.score(X, y))  # print R^2 score\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nyfit = np.exp(linear_regressor.predict(Xfit))  # predict y's from the linearly spaced x data\nplt.scatter(X['x'], df3['y'])  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quadratic function","output":{"0":{"name":"stdout","output_type":"stream","text":"           x         y     y_log\n72 -2.502953  0.196863 -1.625248\n54 -0.321153  0.800711 -0.222255\n99 -2.724684  0.171072 -1.765669\n17 -2.142127  0.236836 -1.440387\n46  2.229563  4.418600  1.485823\ny-intercept: [-0.01099079]\nx coefficient(s): [[0.67344706]]\nR^2 score: 0.9992346524707725\n"},"1":{"data":{"image/png":"1014da3d142f9a88004a49c1745198bf89b6eada","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":7,"metadata":{"image/png":{"height":248,"width":369},"needs_background":"light"},"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"807cf1","input":"print(df5.describe())\nprint(df5.sample(5))\ncorr = df5.corr()\nprint(corr)\nplt.matshow(corr)\nplt.show()\nplt.scatter(df5['x1'], df5['y']);\nplt.show()\nplt.scatter(df5['x2'], df5['y']);\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"               x1          x2           y\ncount  500.000000  500.000000  500.000000\nmean     0.101295    0.107199  204.766554\nstd      2.966905    2.968324  163.064938\nmin     -4.993358   -4.996327  -12.949604\n25%     -2.428176   -2.364742   71.324889\n50%     -0.065073    0.066029  163.980418\n75%      2.731473    2.820355  327.454902\nmax      4.998086    4.983549  738.683307\n           x1        x2           y\n407 -1.420161 -0.648580   29.703968\n24  -0.916971 -1.225934   16.301406\n275  1.775084  4.378644  192.994450\n237 -2.963791  0.663116  154.616233\n98  -2.823372  4.738187  150.076473\n          x1       x2         y\nx1  1.000000  0.05927  0.076864\nx2  0.059270  1.00000  0.008370\ny   0.076864  0.00837  1.000000\n"},"1":{"data":{"image/png":"15dc7fe439a21d4ddbe02afd2d5e85638b7040f8","text/plain":"<Figure size 288x288 with 1 Axes>"},"exec_count":9,"metadata":{"image/png":{"height":257,"width":251},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"fb4f52bfc1f436a9a284fe96e6c45997ba81ae86","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":9,"metadata":{"image/png":{"height":248,"width":375},"needs_background":"light"},"output_type":"execute_result"},"3":{"data":{"image/png":"50859ddd820dc978204c63a60d59de38aa194466","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":9,"metadata":{"image/png":{"height":248,"width":375},"needs_background":"light"},"output_type":"execute_result"}},"pos":18,"scrolled":true,"type":"cell"}
{"cell_type":"code","id":"64052b","input":"","kernel":"python3","pos":47,"type":"cell"}
{"cell_type":"code","id":"6596e0","input":"","pos":49,"type":"cell"}
{"cell_type":"code","id":"6c5d0a","input":"","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"041a59","input":"For this next dataframe ```df3```, we're not looking at a polynomial relationship between $x$ and $y$.  What relationship do you see?","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"110428","input":"## Multiple Regression\n\n_Multiple regression_ refers to cases where we are regressing our output onto multiple input variables.  We have already been performing multiple linear regressions when we were introduced to Kaggle and in our Kaggle competition \"signal and noise\" homework.  In the previous pre-class exercise, we engineered additional polynomial term features $x^2$ and $x^3$ so as to be able to use these multiple inputs to model a cubic relationship between $y$ and the original sole input $x$.\n\nHowever we may have more complex multiple regressions.  Dataframe ```df5``` has two inputs ```x1``` and ```x2```, but the relationship between them and ```y``` is more complex:","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"196bbc","input":"Next, build a model for ```y``` using data from the URL [http://cs.gettysburg.edu/~tneller/ds256/data/inclass/data9.csv](http://cs.gettysburg.edu/~tneller/ds256/data/inclass/data9.csv).  The relationship here is more complex, so I recommend adding squared terms for each input and interaction terms for each pair of inputs.  Having added all of these engineered features to your dataframe, compute the correlation matrix and look at the correlations with ```y```.  Build your first model for ```y``` with the highest correlating input and check the $R^2$ score.  Then add the next highest  correlating input and compare the resulting $R^2$ with the previous one.  When you don't see significant improvement, return to the previous model and print out the y-intercept and coeefficients for your input terms.","pos":48,"type":"cell"}
{"cell_type":"markdown","id":"20baab","input":"The main lesson here is that nonlinear relationships can be predicted using linear regression if one\n* discerns the type of nonlinear relationships between the variables,\n* transforms inputs and/or outputs so as to mathematically arrive at a linear relationship between transformed variables,\n* performs a linear regression with the transformed data, and\n* uses pre- and/or post-processing of data before and/or after application of the linear model in order to predict non-linear relationships.\n\n**To-Do:** For practice before class, create a cell below and use linear regression to predict a nonlinear cubic relationship in dataframe ```df4``` similar to the quadratic example above but, in this case, you'll need to create and regress onto additional new columns ```x2``` and ```x3``` you will create that that contain $x^2$ and $x^3$, respectively.","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"2e7ae7","input":"# Nonlinear Regression Using Linear Regression\n\nLearning Objectives:\n* Students will learn about how nonlinear regression can be achieved using linear regression.\n* Students will visualize data in order to discern nonlinear relationships between variables.\n* Students will engineer features that allow linear regression to build models for predicting variables that result from nonlinear relationships.\n* Students will apply feature engineering to data in order to predict nonlinear relationships using such models.\n\nBefore class:\n* Read [Will Koehrsen's \"Visualizing Data with Pairs Plots in Python\"](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166)  _Medium.com, which includes the publication Towards Data Science, features a wealth of short articles on data science topics and is worth the monthly subscription.  Your first articles should be free._\n* Jake VanderPlas.  [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook):\n  * [5.4 - Feature Engineering subsection \"Derived Features\"](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb) _Read only the \"Derived Features\" subsection._\n* Read below up to (but not including) the section marked In Class.  **Perform listed tasks and supplemental reading as directed below.**\n\nOptional how-to reference for plotting:\n* Jake VanderPlas.  [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook):\n  * [4.1 - Simple Line Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.01-Simple-Line-Plots.ipynb)\n  * [4.2 - Simple Scatter Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.02-Simple-Scatter-Plots.ipynb)\n  * [4.12 - Three-Dimensional Plotting in Matplotlib subsection \"Three-dimensional Points and Lines\"](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.12-Three-Dimensional-Plotting.ipynb) _Read only the \"Three-dimensional Points and Lines subsection._\n* Fabrizio Guerrieri's [\"Use Python to plot Surface graphs of irregular Datasets\"](https://fabrizioguerrieri.com/blog/surface-graphs-with-irregular-dataset/)\n\nIn class:\n* We will work together on the exercises in section \"In Class\".\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"42b763","input":"Along the diagonals, we see histograms of each variable's distribution.  We also see that there's a clear linear relationship between ```x4``` and ```y```, but note the second-highest correlation variable, ```x3```.  Whereas there's what appears to be random noise in other grid cells (and low correlation), there's something ordered to the mass of points in the ```x3```-```y``` scatter plot.  This could indicate a squared relationship.  Let's test that.\n\nNote: Seaborn pair plots are computationally intensive and take up memory.  Feel free to comment out your pair plot lines (putting a \"#\" at the front of those lines) in order to improve the performance of your notebook.","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"4939d7","input":"It's a more complex relationship we're looking at here.  We can see what looks like a parabola in the first ```x1```-```y``` scatter plot.  The ```x2```-```y``` scatter plot relationship is less clear.  Sometimes, 3D visualization helps:","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"4cf6fe","input":"The $R^2$ score is almost exactly the same.  The difference is negligible, but it is slightly lower without the ```x1``` and ```x2``` terms.  Does that mean that they were helpful after all?\n\nThe answer, as we shall better understand in a future class, is _no_.   This is why, as we see on Kaggle, that we typically separate data into training and testing sets.  A model will use all training information given to fit and _perhaps overfit_ the data, crafting a model to minimize error of prediction for that data, but one could see worse generalization of the model to other unseen cases for having included irrelevant terms and used them to fit.\n\nThe $y$ values were indeed generated with $x1^2$, $x2^2$, and $x1 x2$, but without $x1$ and $x2$.  In the future when studying _validation_, we'll see how the \"less is more\" adage applies to modeling that generalizes better beyond the training dataset.","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"5f0487","input":"In the cell below, build a model to predict ```y``` given dataframe ```df7```.  Perform a pair plot before modeling.  Perform an $R^2$ test and plot your model fit using an appropriate visualization method from above.","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"78cb4c","input":"# Generating Example Data\n\nIn this section, I will demonstrate how one can generate the data we will use for illustrating techniques.  This section is _optional_ and is provided for those interested.  Eventually, we will cover Pandas in greater detail so that this will be comprehensible, but this necessary section creates the data we'll work with in most examples and exercises that follow.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"78f84f","input":"This shows another clear improvement.  Now let's look at our next highest correlator ```x5```, which has an order of magnitude lesser correlation than ```x3```.\n\n**```x4```, ```x3sqr```, ```x3```, and ```x5```:**","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"79a1d8","input":"**Exercise 3:**  In the next code cell,\n* Create a dataframe with the CSV data loaded directly from [http://cs.gettysburg.edu/~tneller/ds256/data/hw6/hw6-3.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw6/hw6-3.csv).\n* Print a description and a size 5 sample of it.\n* Print a correlation matrix and plot the color-coded representation of the correlation matrix.  (Relationships will not be clear yet.)\n* Graph irregular 3D surface plots for each pair of inputs with the output to look for higher order relationships  In other words, create 3D plots for the (x1, x2, y) surface, the (x1, x3, y) surface, and the (x2, x3, y) surface.  You will find that two are noisy evidenced by many spikes, and one is a well-known [paraboloid](https://en.wikipedia.org/wiki/Paraboloid) whose smooth surface is evident.\n* Add appropriate squared input terms to the dataframe to model that paraboloid.\n* Compute and print the new correlation matrix to verify the high correlation of added squared input terms.\n* Build a model of ```y``` using the relevant, highly-correlating subset of the input terms.\n* Print the terms used, the y-intercept, the input term coefficients, and the $R^2$ score.","pos":55,"type":"cell"}
{"cell_type":"markdown","id":"7f4e7e","input":"Here we see a somewhat bowl-like shape where the mesh coloration highlights that the values toward $(4, 4)$ and $(-4, -4)$ are higher valued for $y$ than those near $(4, -4)$ and $(-4, 4)$.  This can indicate that our model should include an _interaction term_ between ```x1``` and ```x2```, i.e. an ```x1 * x2``` term.\n\nSeeing the parabolic curving and noting the corner skew here, we will add such squared and interaction terms to our dataframe and see what the correlations with ```y``` tell us:","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"8427b0","input":"This looks to be an exponential relationship of the form $y = b^x$ where $b$ is positive.  If we take the $\\log_c$ of both sides $\\log_c(y) = \\log_c(b^x)$ for some positive $c$, the right hand side according to the [logarithm change of base rule](https://www.khanacademy.org/math/algebra2/x2ec2f6f830c9fb89:logs/x2ec2f6f830c9fb89:change-of-base/a/logarithm-change-of-base-rule-intro) is equivalent to $\\frac{\\log_b(b^x)}{\\log_b{c}} = \\frac{x}{\\log_c{b}}$ which is $x$ divided by a constant factor.  In other words, taking a logorithm of the output of an exponential function, we get values with a linear relationship.  This is visually illustrated through the use of [semi-log plots](https://en.wikipedia.org/wiki/Semi-log_plot).\n\nWhat does this mean to our use of linear regression for modeling an exponential relationship?  We can do one of two things:\n\n1. Transform our $y$ outputs to $\\log(y)$, regress $\\log(y)$ onto $x$, predict $\\log(y)$ values for which we then untransform predictions with the inverse transformation $e^{\\log(y)} = y$.\n2. Transform our $x$ inputs to $e^x$, regress $y$ onto $e^x$, and make predictions by first computing $e^x$ for each new input $x$.\n\nGeneral practice is to perform option (1) because linear regression is based on least-squares error when looking at the distances of points to the fit line.  If we're in the exponential space, this gives the large exponential values greater weight to the fit, whereas we would generally want all points to treats as having the same importance for fit.  In this case, we're engineering the output, not the feature(s), i.e. input(s).  Observe this approach:","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"854705","input":"This is _not_ a significant improvement, so we would tend to prefer the simpler model with ```x4```, ```x3sqr```, and ```x3```.  Indeed, these were the only variables used for generating the data.","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"8650c1","input":"(end of homework)","pos":57,"type":"cell"}
{"cell_type":"markdown","id":"8dd5ab","input":"While the parabola may stick out to you at first, that's just the relationship we engineered by adding the square term.  Instead, focus on the boundaries of the ```x3sqr```-```y``` plot.  There's a definite slant to them.  Looking at the correlations, we see that ```x3sqr``` has an even higher correlation to ```y``` than ```x3```.  One approach to modeling is to start with the highest correlating term and incrementally add the next highest and so on until we see no significant increase in error measures (like $R^2$).  Let's try that and see what occurs:\n\n**```x4``` only:**","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"8e571a","input":"Our $R^2$ score shows clear improvement.  Input ```x3sqr``` was a good addition.\n\n**```x4```, ```x3sqr```, and ```x3```:**","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"8fbb3e","input":"One last example of a technique that can be helpful in discerning relationships between variables is that of _pair plots_.  Like a correlation matrix, we have a grid of indicators for each pair of variables.  However each of the grid cells is, by default, a scatterplot to give us a visual overview of pair relationships.   For this, we'll use the ```seaborn``` visualization library to visually inspect dataframe ```df6```.","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"9a9c8f","input":"# Linear, Nonlinear, and Multiple Regression\n\n## Linear regression\n\nRecall that linear regression assumes a linear relationship between input(s) and output.  For a single input $x$ and single output $y$, it seeks to fit a line to $(x,y)$ points so as to predict $y$ values for _any_ $x$.  This line is our \"model\" of $y$ computed from $x$ and $y$ data.  In the AI subfield of Machine Learning, this is known as _supervised learning_.  A supervisor provides training examples of pairs of input(s) and output(s) and an algorithm \"learns\" a model from those examples.  Think of linear regression as one of our most simple, interpretable, and explainable forms of Machine Learning that comes to us from Statistics.\n\nLet's review our modeling process by looking at the dataframe ```df1```.  We start by describing the data, printing a sample of it, looking at the correlation matrix, visualizing the matrix, and plotting the points.  This gives us better understanding of what numeric data we have and the fact that there is a linear relationship between $x$ and $y$:","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"a8803d","input":"We note that this is a very good $R^2$ score, yet the ```x1``` and ```x2``` coefficients are small.  This in and of itself is not indicative of irrelevance.  It could be that the values are very large and contribute significantly even scaled down significantly.  That is not the case here.  Let's see what happens when we eliminate ```x1``` and ```x2``` from our multiple regression:","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"a98d38","input":"**Exercise 2:**  In the next code cell,\n* Create a dataframe with the CSV data loaded directly from [http://cs.gettysburg.edu/~tneller/ds256/data/hw6/hw6-2.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw6/hw6-2.csv).\n* Print a description and a size 5 sample of it.\n* Print a correlation matrix and plot the color-coded representation of the correlation matrix.\n* Print seaborn pair plots of the dataframe.\n* Build a model of ```y``` using the relevant, correlating subset of the input data.\n* Print the y-intercept, the input term coefficients, and the $R^2$ score.","pos":53,"type":"cell"}
{"cell_type":"markdown","id":"ab4e3e","input":"We next build our linear model using linear regression, \"regressing $y$ onto $x$\", print our $R^2$ score, and plot our prediction line against the data.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"b74fed","input":"## Homework\n\n**Exercise 1:** In the code cell below this:\n* Read CSV data into a dataframe from [http://cs.gettysburg.edu/~tneller/ds256/data/hw6/hw6-1.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw6/hw6-1.csv).\n* Print a description of the data and a sample of 5 rows.\n* Scatter plot the data.\n* Note the [polynomial](https://en.wikipedia.org/wiki/Polynomial) relationship between input ```x``` and output ```y```.  Add polynomial term inputs for the likely [polynomial](https://en.wikipedia.org/wiki/Polynomial).\n* Perform a nonlinear regression using linear regression with these (mostly nonlinear) polynomial input terms.\n* Print the y-intercept, coefficients, and $R^2$ score of your model.\n* Scatter plot the data again along with an orange fit curve to demonstrate your linear regression model fit.","pos":51,"type":"cell"}
{"cell_type":"markdown","id":"c2d088","input":"In you have time remaining, here are some options for further in-class practice:\n* Following the forms used at the beginning of this document, create data with similarly obscured models for each other to practice discerning the best model.\n* Practice some of the visualization techniques with the data from in-class exercises, e.g. the curve fit for the first in-class problem or a 3D plot of the surface for the last problem.","pos":50,"type":"cell"}
{"cell_type":"markdown","id":"c3224d","input":"For the next in-class exercise, you'll load your data from this [data8.csv](http://cs.gettysburg.edu/~tneller/ds256/data/inclass/data8.csv) file, which you can and should load directly using the URL [http://cs.gettysburg.edu/~tneller/ds256/data/inclass/data8.csv](http://cs.gettysburg.edu/~tneller/ds256/data/inclass/data8.csv) using the method we saw in the first class demonstration.  Not all inputs will be relevant to predicting the output ```y```.  Use the correlation matrix, its graphical representation, and seaborn pair plots to discern the relevant inputs.  Then build a model with those inputs, print which inputs you are using, the y-intercept, the input coefficients, and the $R^2$ score.","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"ca524d","input":"This relationship looks like a [parabola](https://en.wikipedia.org/wiki/Parabola) which means our relationship is likely [quadratic](https://en.wikipedia.org/wiki/Quadratic_function), i.e. of the form $y = ax^2 + bx + c$.  How can we use linear regression to perform quadratic regression?  We can add an $x^2$ value column to our dataframe and perform linear regression as usual.\n\nHow does this work?  Suppose we had input variables $A$ and $B$ with output $y$.  Then a linear regression would compute corresponding coefficients for $A$ and $B$, which we'll call $a$ and $b$ respectively, as well as a $y$-intercept that we'll call $c$.  Then the model we build will be of the form $y = aA + bB + c$.  If you substitute $x^2$ values for $A$ and corresponding $x$ values for $B$, we can see this is of the form $y = ax^2 + bx + c$, a quadratic equation.\n\nAdding or modifying features for modeling is called _feature engineering_.  Creating a new column with a squared term is a simple form of feature engineering.  Let's look at how this works in practice as we fit our quadratic relationship:","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"db7ea1","input":"## Nonlinear Regression\n\nSometimes we have a nonlinear relationship between input and output.  Observe that our simple exploratory data analysis for dataframe ```df2``` does not indicate a linear relationship:","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"ecd197","input":"**```x4``` and ```x3sqr```**:","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"f1e9e1","input":"# In Class\n\nTogether in class, you will seek to build models for each of the given data sets using the techniques above.","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"f4d4eb","input":"From the sample, we do a visual check that our feature engineering is as we expect.\n\nNow look at the ```y``` column in the correlation matrix.  Of the inputs, ```x1``` and ```x2``` have the weakest (smallest magnitude) correlations with ```y```.  Let's perform our multiple regression with and without them:","pos":23,"type":"cell"}
{"id":0,"time":1599224136076,"type":"user"}
{"last_load":1599019249391,"type":"file"}