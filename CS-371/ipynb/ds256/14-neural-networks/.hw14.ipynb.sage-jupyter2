{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":82239488},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"841480","input":"\n","pos":36,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"de1bee","input":"\n","pos":39,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"a91685","input":"# Imports\n\nimport math\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\nimport pandas as pd\nimport math\nimport random\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, mean_squared_error, plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\n","pos":2,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"670219","input":"model = MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=0)\nmodel.fit(X_train, y_train);\n\ny_predict = model.predict(X_test)\nprint('Accuracy: ', accuracy_score(y_test, y_predict))\n\nvisualize_classifier(model, X.values, y.values, ax = plt.gca())\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy:  0.896\n"},"1":{"data":{"image/png":"b2a94753d500b4ec6c38fa3b4c98ac66d6043ebb","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":10,"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"},"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"77a6a3","input":"df = pd.read_csv('http://cs.gettysburg.edu/~tneller/ds256/data/hw6/hw6-1.csv', index_col='id')\n\nX = df[['x']]  # select input(s)\ny = df['y']  # select output\n\n# As before, we will split our datasets into training and testing datasets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=my_seed, shuffle=True)\n\nmodel = MLPRegressor(max_iter=10000, random_state=0)\nmodel.fit(X_train, y_train);\n\nprint('Mean squared error (MSE):', mean_squared_error(y_test, model.predict(X_test)))\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nyfit = model.predict(Xfit)  # predict y's from the linearly spaced x data\nplt.scatter(X['x'], y, color='blue')  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quartic function\nplt.show()\n","output":{"0":{"name":"stderr","output_type":"stream","text":"/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n  warnings.warn(\n"},"1":{"name":"stdout","output_type":"stream","text":"Mean squared error (MSE): 389614.0723835577\n"},"2":{"data":{"image/png":"1f56869b86e1d17e4be5b6e15267322261a22e7d","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":11,"metadata":{"image/png":{"height":411,"width":725},"needs_background":"light"},"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"5e3864","input":"model = MLPRegressor(hidden_layer_sizes=(50, 50,), max_iter=10000, random_state=0)\nmodel.fit(X_train, y_train);\n\nprint('Mean squared error (MSE):', mean_squared_error(y_test, model.predict(X_test)))\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nyfit = model.predict(Xfit)  # predict y's from the linearly spaced x data\nplt.scatter(X['x'], y, color='blue')  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quartic function\nplt.show()\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Mean squared error (MSE): 17909.875476572794\n"},"1":{"data":{"image/png":"77d223ad4f10de39990784b0de5e36e8d3504e4a","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":12,"metadata":{"image/png":{"height":411,"width":725},"needs_background":"light"},"output_type":"execute_result"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"ddffab","input":"model = MLPRegressor(hidden_layer_sizes=(25, 25, 25, 25,), max_iter=10000, random_state=0)\nmodel.fit(X_train, y_train);\n\nprint('Mean squared error (MSE):', mean_squared_error(y_test, model.predict(X_test)))\n\nXfit = pd.DataFrame()\nXfit['x'] = np.linspace(start=X['x'].min(), stop=X['x'].max(), num=1000)  # 1000 linearly spaces points from the min to the max of x\nyfit = model.predict(Xfit)  # predict y's from the linearly spaced x data\nplt.scatter(X['x'], y, color='blue')  # scatter plot of original data\nplt.plot(Xfit['x'], yfit, color='orange');  # line plot showing fit of learned quartic function\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"Mean squared error (MSE): 14091.60149022795\n"},"1":{"data":{"image/png":"84ac949f1f7537a2bdb6bf68fc8e3fb3cd92bd29","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":13,"metadata":{"image/png":{"height":411,"width":725},"needs_background":"light"},"output_type":"execute_result"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"16cadf","input":"my_seed = 7\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\nnum_classes = 5\n\n# We generate 2000 clustered 2D data points sampled from 5 normally-distributed clusters\n#   with each cluster representing a different class.\nX, y = make_blobs(n_samples=2000, centers=num_classes,\n                  random_state=my_seed, cluster_std=2.0)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n\n","output":{"0":{"data":{"image/png":"9f0a509ac8b9a7689372e4bd678580d383a6de72","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":14,"metadata":{"image/png":{"height":411,"width":712},"needs_background":"light"},"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"1d49ac","input":"random.seed(42)\nnp.random.seed(42)\nx_names = ['x']\ndf = pd.DataFrame(np.random.uniform(low=-5, high=5, size=(1000, 1)), columns=x_names)\ndf['y'] = df['x'].map(lambda x: 42 + 20 * math.sin(x) + x * x + (-10 if round(x) % 2 == 0 else 0) + np.random.normal(scale=1))\nplt.scatter(df['x'], df['y'])  # scatter plot of original data\n\n\n","output":{"0":{"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7ff76bd84700>"},"exec_count":15,"output_type":"execute_result"},"1":{"data":{"image/png":"3ba0069d000e078139aaab2cf6546618fc23f643","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":15,"metadata":{"image/png":{"height":411,"width":703},"needs_background":"light"},"output_type":"execute_result"}},"pos":32,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"8b573b","input":"my_seed = 0\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\n\ndf = pd.DataFrame(data=[[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]], columns=['x1', 'x2', 'y'])\nX = df[['x1', 'x2']]\ny = df['y']\nmodel = LogisticRegression(C=10)  # Higher C weakens the regularization to avoid underfitting here.\nmodel.fit(X, y);\nprint('Intercept:', model.intercept_)\nprint('Coefficients:', model.coef_)\nprint('Predictions:', model.predict([[0, 0], [0, 1], [1, 0], [1, 1]]))\n\n# We can visualize the model by using VanderPlas' visualization code\n# modified to _assume the model is already fit_:\ndef visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n    ax = ax or plt.gca()\n\n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    ax.axis('off')\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    # Create a color plot with the results\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap=cmap, #clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n\nvisualize_classifier(model, X.values, y.values, ax = plt.gca())\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"Intercept: [-0.49187356]\nCoefficients: [[2.03443073 2.03443073]]\nPredictions: [0 1 1 1]\n"},"1":{"data":{"image/png":"0ebd583b06482c4cf2ef0058872ed63aa9bd5ab9","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":2,"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"},"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"26044c","input":"df = pd.DataFrame(data=[[0, 0, 0], [0, 1, 0], [1, 0, 0], [1, 1, 1]], columns=['x1', 'x2', 'y'])\nX = df[['x1', 'x2']]\ny = df['y']\nmodel.fit(X, y);\nprint('Intercept:', model.intercept_)\nprint('Coefficients:', model.coef_)\nprint('Predictions:', model.predict([[0, 0], [0, 1], [1, 0], [1, 1]]))\n\nvisualize_classifier(model, X.values, y.values, ax = plt.gca())\nplt.show()\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Intercept: [-3.57643092]\nCoefficients: [[2.03395158 2.03395158]]\nPredictions: [0 0 0 1]\n"},"1":{"data":{"image/png":"e42eb6d3ab71cf244ad656c350ff0f99b99a4c82","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"},"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"0c85c0","input":"df = pd.DataFrame(data=[[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0]], columns=['x1', 'x2', 'y'])\nX = df[['x1', 'x2']]\ny = df['y']\nmodel.fit(X, y);\nprint('Intercept:', model.intercept_)\nprint('Coefficients:', model.coef_)\nprint('Predictions:', model.predict([[0, 0], [0, 1], [1, 0], [1, 1]]))\n\nvisualize_classifier(model, X.values, y.values, ax = plt.gca())\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"Intercept: [0.]\nCoefficients: [[0. 0.]]\nPredictions: [0 0 0 0]\n"},"1":{"data":{"image/png":"a8f05ff5f9f020103fc3fa7f2199df150b0aaf00","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":4,"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"},"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"dfb302","input":"model = MLPClassifier(max_iter=1000, random_state=0)\nmodel.fit(X, y);\nprint('Predictions:', model.predict([[0, 0], [0, 1], [1, 0], [1, 1]]))\n\nvisualize_classifier(model, X.values, y.values, ax = plt.gca())\nplt.show()\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Predictions: [0 1 1 0]\n"},"1":{"data":{"image/png":"649072729c9ff84699c474e1320cb43da5034217","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"},"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"dfbbc8","input":"df = pd.read_csv('http://cs.gettysburg.edu/~tneller/ds256/data/hw10/ic10-1.csv', index_col='id')\n\n# Create a normalization of x1, x2\ninput_cols = ['x1','x2']\nx = df[input_cols].values  # returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf_scaled = pd.DataFrame(x_scaled, columns=input_cols)\n\n# Create a mapping of all unique y class/category values to 0, 1, ...\nclasses = df.y.unique()\nclass_map = {c : i for i, c in enumerate(classes)}\ndf_scaled['y'] = df['y'].map(class_map)\n\n# Divide into train, test sets\nX = df_scaled[['x1', 'x2']]\ny = df_scaled['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=0)\n\nmodel = MLPClassifier(random_state=0)\nmodel.fit(X, y);\n\ny_predict = model.predict(X_train)\nacc = accuracy_score(y_train, y_predict)\nprint('Accuracy =', acc)\n\nvisualize_classifier(model, X.values, y.values, ax = plt.gca())\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy = 0.9836\n"},"1":{"data":{"image/png":"443c5ef7d4b9b3635997ffaf8f1e0c4b197fb729","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":6,"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"},"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"4e504d","input":"my_seed = 0\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\n\nnum_classes = 3\nX, y = make_blobs(n_samples=1000, centers=num_classes,\n                  random_state=my_seed, cluster_std=1.0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=my_seed, shuffle=True)\n\nmodel = MLPClassifier(max_iter=1000, random_state=0)\nmodel.fit(X_train, y_train);\n\ny_predict = model.predict(X_test)\nfor c in range(num_classes):\n    print('Class', c, 'testing accuracy: ', accuracy_score(y_test == c, y_predict == c))\n\nvisualize_classifier(model, X, y, ax = plt.gca())\nplt.show()\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Class 0 testing accuracy:  0.928\nClass 1 testing accuracy:  0.974\nClass 2 testing accuracy:  0.946\n"},"1":{"data":{"image/png":"3b9fbeafc89523b640394e04486dabbef82e013c","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":7,"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"},"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"42bb81","input":"my_seed = 0\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\nstdev = 0.075\nnum_clusters = 10\ncentroids = [[.5, .8], [.5, .1], [.3, .7], [.6, .6], [.1, .6], [.4, .5], [.4, .3], [.6, .4], [.2, .4], [.3, .1]]\ndata = [[np.random.normal(loc=centroids[i % num_clusters][0], scale=stdev),\n         np.random.normal(loc=centroids[i % num_clusters][1], scale=stdev),\n         (i % num_clusters) % 2] for i in range(1000)]\nrandom.shuffle(data, random.random)\nx1 = [x[0] for x in data if x[2] == '0']\nx2 = [x[1] for x in data if x[2] == '0']\nplt.scatter(x1, x2, color='orange')\nx1 = [x[0] for x in data if x[2] == '1']\nx2 = [x[1] for x in data if x[2] == '1']\ndf = pd.DataFrame(data, columns=['x1', 'x2', 'y'])\nprint(df.head())\nX = df[['x1', 'x2']]\ny = df['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=my_seed, shuffle=True)\n\nmodel = MLPClassifier(max_iter=1000, random_state=0)\nmodel.fit(X_train, y_train);\n\ny_predict = model.predict(X_test)\nprint('Accuracy: ', accuracy_score(y_test, y_predict))\n\nvisualize_classifier(model, X.values, y.values, ax = plt.gca())\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"         x1        x2  y\n0  0.736644  0.394326  1\n1  0.162748  0.047812  1\n2  0.214820  0.606934  0\n3  0.245322  0.714742  0\n4  0.438145  0.498891  1\n"},"1":{"name":"stdout","output_type":"stream","text":"Accuracy:  0.818\n"},"2":{"data":{"image/png":"b4aaa0d7c29dc9cb8114c955716e869a1c83f90d","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":8,"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"},"output_type":"execute_result"}},"pos":16,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"fa3c45","input":"model = MLPClassifier(alpha=.00005, max_iter=10000, random_state=1)\nmodel.fit(X_train, y_train);\n\ny_predict = model.predict(X_test)\nprint('Accuracy: ', accuracy_score(y_test, y_predict))\n\nvisualize_classifier(model, X.values, y.values, ax = plt.gca())\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy:  0.88\n"},"1":{"data":{"image/png":"48deb3b69830adfb01be25174452af9eae85a763","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":9,"metadata":{"image/png":{"height":394,"width":683},"needs_background":"light"},"output_type":"execute_result"}},"pos":18,"type":"cell"}
{"cell_type":"markdown","id":"0c9591","input":"**Exercise 1:** Complete the in-class exercises if you haven't already.\n\n**Exercise 2:** Model and classify the data of [hw12-1.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw12/hw12-1.csv) with all of the same steps and NN classifier of your in-class classification work.  _However_, you must first preprocess and normalize your data.  **Compare to HW12 solution performance and comment on the MPLClassifier's relative performance.**\n\nTip: If you have problems with an error \"TypeError: '(slice(None, None, None), 0)' is an invalid key\", use the following when you split your data:\n\n```X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, train_size=0.5, random_state=my_seed, shuffle=True)```\n\nMore on this in chapters on ```numpy``` and ```pandas```.\n","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"11c585","input":"(Insert your comparison and comments here.)","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"2002f1","input":"After 10000 iterations, the single 100-unit layer NN hasn't converged.  Observe the difference with two 50-unit layers.","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"45254e","input":"This is much-improved over all decision tree approaches.  Let's see what happens when we make 4 layers of 25 units each.","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"69854e","input":"This is not as good as the .904 accuracy we achieved by optimizing $k$ for $k$-NN, but is a decent result on the first try.\n\n## Regression\n\nNeural Networks may also be applied to regression as well.  Let us observe the result of applying the [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) to our degree 4 polynomial regression dataset.\n","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"7aabc9","input":"# Neural Networks\n\nLearning Objectives:\n* Students will learn about the function and limitations of an artificial neuron.\n* Students will experiment to gain understanding of how an artificial neural network can overcome the limitations of a single artificial neuron.\n* Students will apply sklearn's neural network modules to classification and regression problems.\n\nReadings before class:\n* Michael Nielsen. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html):\n  * [Chapter 1 up to but not including section \"A simple network to classify handwritten digits\"](http://neuralnetworksanddeeplearning.com/chap1.html) _Note: NAND stands for operation \"not and\", a Boolean function of two inputs x1, x2 that is true if and only if it is not the case that x1 and x2 are both true._\n* [Tensorflow Playground](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.09400&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) _This is an excellent interactive demonstration of neural network learning.  See below for ways you can experiment to gain experiential insight to neural network learning._\n\nBefore class:\n* After completing the Nielsen reading, read the summary below and especially note the relationship with logistic regression/classification.\n\nIn class:\n* We will work together on the exercises in section \"In Class\".\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"7bd674","input":"(Insert your comparison and comments here.)","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"80d80f","input":"(Insert your comparison and comments here.)","pos":37,"type":"cell"}
{"cell_type":"markdown","id":"80f1cf","input":"With default parameters, we achieve successful results, but the computation is costly.  Some argue that the main reason that we have seen a rise in the popularity of NNs in recent years is that we finally have machines capable of storing sufficient data and performing costly operations quickly (e.g. using Graphical Processing Units (GPUs) designed for gaming to perform highly parallelized NN computations).  Our techniques have indeed improved since the disappointing boom and bust of NN research in the 1980s, but the main difference between then and now is bigger data, bigger memory to hold it, and faster machines to learn from it.\n\nWhen performing a classification for more than two classes, the common approach is to have a \"softmax\" output that has a separate output unit for each class, with each output indicating the probability that the input will belong to that class.  When a prediction is called upon, the output unit with the highest probability is selected for the classification prediction.  Let's see this approach applied to the \"blobs\" data of the decision tree lesson.","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"8be3fe","input":"At best, logistic regression might classify 3 of 4 points correctly, but it does not have the expressive power to correctly classify such a _linearly inseparable_ function.\n\nSince you can think of an artificial neuron as being a simple logistic regression, we can see that an artificial neuron is restricted to only learning simple linearly separable functions (e.g. AND, OR, NOT).\n\nHowever, we can express XOR as a compound expression of linearly separable functions.  Consider that ```x1 XOR x2``` is equivalent to ```(x1 OR x2) AND NOT (x1 AND X2)```.  The two parenthesized expressions can each be learned by single artificial neurons.  ```NOT (x1 AND X2)``` that reverses the truth values of ```(x1 AND X2)``` can also be learned.  The full AND combination can be learned as well.\n\nThis means that a network of simple neurons can learn more complex functions by passing simple outputs as inputs to other units in what is called a _multilayer feedforward network_.  Such neural networks are a low-bias, high-variance modeling method, and have shown excellent performance for a wide variety of predictive tasks, although it is hard to say what exactly has been learned by such networks, as we cannot easily gain insight from a collection of weights and bias values.\n\nLet us play with a simple neural network and watch something like XOR being learned using this link to the [Tensorflow Playground](https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=3,1&seed=0.73705&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). (Click on the link and press the play button.)\n\nIn this demonstration, we have two classes visualized as orange and blue.  Each artificial neuron \"unit\" in the multilayer feedforward network has a small visualization of what each unit computes as a function of the original units.  Through each layer we can see the combinations form more complex nonlinear shapes, which leads us to an important insight:\n\n**Each layer of a neural network engineers the input features for the next layer.**  One of the appeals to neural networks (NNs) is that they seek to learn internal representations of the input data that are helpful for producing good predictive output.  One might think one should always go to NNs as one's easy hands-free technique to model anything.  However, there are a few points to temper one's enthusiasm:\n\n* It is often said that \"A neural network is the second-best way to solve any problem.\"   What is generally meant is that greater insight to a problem can often suggest a simpler, better solution.  As we saw in our comparison of decision tree regression versus polynomial regression, knowing the form of the solution suggests better and often simpler approaches.  It is often best to work with the simplest model that yields acceptable results, as it is more easily explained, understood, debugged, etc.\n* Neural networks can be notoriously difficult to train successfully with many architectural and learning parameter choices yielding very different results.  We see the many shining successes, but people generally don't share the many convergences failures or note the many hours spent carefully engineering the successes.  With this great versatility and power also comes great complexity and many possible ways the NN approach can fail or yield [fragile successes](https://www.nature.com/articles/d41586-019-03013-5).\n\nStill, NNs are an important tool in one's toolset, and deep neural networks (DNNs) are gaining in popularity for many applications.  (A _deep neural network_ is simply a neural network with many layers in which to learn complex internal representations of data.)","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"8d03ff","input":"# Natural and Artificial Neurons\n\nA [Neuron](https://en.wikipedia.org/wiki/Neuron), depicted in the figure below, allow electrochemical signals that are input through dendrites to excite the neuron so as to send an electrochemical pulse as output along its single axon which, through synapses, signals other dendrites of other neurons, allowing for a network of electrochemical computations.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Blausen_0657_MultipolarNeuron.png/500px-Blausen_0657_MultipolarNeuron.png\" alt=\"Anatomy of a Multipolar Neuron from Wikipedia article\" width=\"800\"/>\n\nSource: [https://en.wikipedia.org/wiki/Neuron](https://en.wikipedia.org/wiki/Neuron)\n\nRelative to the complexity of a natural neuron cell, the [artificial neuron](https://en.wikipedia.org/wiki/Artificial_neuron) is a relatively simple mathematical model.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png\" alt=\"Artificial Neuron\" width=\"800\"/>\n\nSource: [https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png](https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png)\n\nInputs are multiplied by learned weights (positive or negative) and are summed.  This net input then passes through a nonlinear activation function that computes an output.  Outputs are typically from -1 to 1 or from 0 to 1.\n\nOften there is a \"bias\" input, depicted as a constant input of 1 with its own weight, or as a threshold value for the nonlinear activation function (which is effectively the negative weight for such a bias input).  Different learned weights compute different nonlinear functions.  Consider an artificial neuron with a logistic (a.k.a. sigmoid) activation function $\\sigma(z) \\equiv \\frac{1}{1+e^{-z}}$.  With inputs $x_j$, weights $w_j$, and bias $b$, the output of this sigmoid unit is $\\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}$.  \n\nThis form is the same as used in [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)!  Learning the weights and bias of a single artificial sigmoid neuron is equivalent to learning the coefficients and intercept, respectively, of a logistic function in _logistic regression_.  Neural network learning for a single sigmoid neuron is the same as performing logistic regression.\n\nThis gives us a basis for understanding what a neuron can and cannot do.  A one-dimensional logistic regression fits a sigmoid to data.  A value of $\\geq .5$ predicts 1, and a value $< .5$ predicts 0.  In one dimension, logistic regression finds a threshold value for which all values are the same 0/1 on either side of a learned threshold value.\n\nIn two dimensions with two inputs, a logistic regression defines a line separating 0/1 predictions.  In three dimensions with three inputs, a logistic regression defines a plane separating 0/1 predictions.  Although the function that expresses the probability of the output is sigmoidal in shape, the threshold division is always a _linear_ function of the inputs.  That is why it is said that functions expressible by artificial neurons are _linearly separable_.  We can set a threshold value, draw a line, define a plane, etc. to separate our two classifications.  That works sometimes, e.g. the OR function that is true (1) if and only if at least one input is true (1). False is represented by 0.\n\n| x1 | x2 | y = x1 OR x2 |\n| -- | -- | ------------ |\n|  0 |  0 |            0 |\n|  0 |  1 |            1 |\n|  1 |  0 |            1 |\n|  1 |  1 |            1 |\n","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"936c13","input":"## Neural Networks with Scikit Learn\n\nWe will first demonstrate the use of Scikit Learn's Neural Network Classifier \"[MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)\" in learning the XOR example from above.","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"93b00f","input":"However, there is no way to learn a linear boundary to correctly classify the XOR (exclusive or) function which is true if and only if either input x1 or input x2 is true (but not both).\n\n| x1 | x2 | y = x1 XOR x2 |\n| -- | -- | ------------- |\n|  0 |  0 |             0 |\n|  0 |  1 |             1 |\n|  1 |  0 |             1 |\n|  1 |  1 |             0 |","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"9c407c","input":"That was easy enough.  Let's look at one of our earlier classification examples for k-NN classification:\n","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"a4ac61","input":"This yields greater accuracy than XGBoost, which is to be expected, since the optimal boundaries are linear directly between the centers of these normally distributed blobs.  Let's see the result when we have a curvier optimal boundary.","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"ae7ed8","input":"**Exercise 3:** Model and regress the data of [hw12-2.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw12/hw12-2.csv) using a neural network with all of the steps and models of your in-class regression work. Experiment with different NN architectures, parameter settings, etc. and share your best performance within reasonable computational time limits. **Compare to HW12 solution performance and comment on the MPLRegressor's relative performance.**","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"b353c7","input":"## Regression\n\nCreate your best MPLRegressor for the HW12 In Class exercise regression data, compare your results to those of the methods of the HW12 In Class exercises, and comment on your findings. Experiment as above with different numbers of layers and units per layer.  What do you observe about NN regressions with discontinuous functions?","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"b56ee5","input":"In constrast, this is not a good classifier.  How do we get a better fit to the data? By default, we are training with 100 units in a single \"hidden layer\" of the network feeding into the single output unit.  We could add more layers, more units, etc., but sometimes the problem is that we're underfitting because of the default \"regularization\" that seeks to simplify the model, penalizing a more sophisticated model.  The parameter ```alpha``` (default=0.0001) can be used to adjust the regularization:","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"b7c195","input":"This also works for the AND function:\n\n| x1 | x2 | y = x1 AND x2 |\n| -- | -- | ------------- |\n|  0 |  0 |             0 |\n|  0 |  1 |             0 |\n|  1 |  0 |             0 |\n|  1 |  1 |             1 |","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"cfb8b2","input":"(end of homework)","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"daf0d5","input":"## Homework\n","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"dd6ff2","input":"You may note that my ```random_state``` parameter is not 0.  The result was poor for seed 0 and better (but not great) for seed 1.   Outcomes for NN learning can be highly variable, as training is a stochastic (random) search in a very high dimensional space of weights and bias parameters.  There are many local error minima, and it can be easy to have a network that _could_ express a function well, not achieve a good fit.\n\nLet's see what happens when we leave parameters as defaults and increase the _depth_ of the network to include 2 layers of 50 units instead of one layer of 100 units.","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"ef2015","input":"This is better still, and we're getting closer to the 10775 MSE of the knowing-the-model approach of fitting a degree 4 polynomial to the noisy data.  We can see that every design choice, from maximum number of iterations, to the size and depth of the NN architecture can have a significant impact, and this is just scratching the surface of the [many parameters we could tune for regression](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) or [classification](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier).\n\nNeural network learning is often allocated its very own course in order to allow for understanding of the many different dimensions of the NN design space.  For now, it suffices for you to get an experiential taste of this family of models, and gain an understanding of the pros and cons.  Generally speaking, we want to use the simplest, most interpretable model that meets our performance criteria.\n\nOne final note: Neural network libraries have risen and fallen in popularity.  [Tensorflow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/) have been trading places for most popular in recent years, and we can expect fashions to change in the future as well.  There are even algorithms that seek to automate NN experimentation and network architecture design (e.g. [AutoML and Neural Architecture Search](https://towardsdatascience.com/everything-you-need-to-know-about-automl-and-neural-architecture-search-8db1863682bf)) that take common design process templates and seek to automate the tuning of practitioners.  In all tools, there will be tradeoffs of simplicity of use versus freedom of design and fine-grained control of learning.\n\nIt will be well worth your time to gain greater experience with more popular tools.  For example, look at these examples of [PyTorch regression](https://medium.com/@benjamin.phillips22/simple-regression-with-neural-networks-in-pytorch-313f06910379).\n\n","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"f8cea6","input":"(Insert your comparison and comments here.)","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"f92ccf","input":"# In Class\n\n## Classification\n\nCreate your best ```MPLClassifier``` for the HW12 In Class exercise classification data, compare your results to those of the methods of the HW12 In Class exercises, and comment on your findings.  Experiment as above with different numbers of layers and units per layer.","pos":28,"type":"cell"}
{"id":0,"time":1602049811428,"type":"user"}
{"last_load":1602049795544,"type":"file"}