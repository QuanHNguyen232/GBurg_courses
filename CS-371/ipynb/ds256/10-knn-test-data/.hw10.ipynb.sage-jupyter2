{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":63778816},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":1,"id":"2ff950","input":"targets = [[-9.998, 100.06], [-9.993, 100.04], [-9.996, 100.02], [-9.993, 100.02], [-9.996, 100.04], [-9.998, 100.06]]\n\n","pos":25,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"8e0291","input":"targets = [[41.99, 100], [42.003, 106], [42.009, 109]]\n\n","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"aa6cf1","input":"import numpy as np\nimport pandas as pd\n#%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport math\nimport random\n\n# NOTE: It is not necessary for you to understand this code at this point.  It is supplied for the curious.\n\n# We begin by importing our entire pseudorandom number generator (PRNG) module and giving it a \"seed\" for reproducability:\nmy_seed = 0\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\n\n# We generate 1000 shuffled 2D data points sampled from  10 normally-distributed clusters \n#   with half of clusters labeled \"0\" (orange) and half labeled \"1\" (blue).\n\nstdev = 0.075\nnum_clusters = 10\n# centroids = [[random(), random()] for i in range(num_clusters)] # random centroids had too much overlap for this illustration\ncentroids = [[.5, .8], [.5, .1], [.3, .7], [.6, .6], [.1, .6], [.4, .5], [.4, .3], [.6, .4], [.2, .4], [.3, .1]]\n# print(centroids)\ndata = [[np.random.normal(loc=centroids[i % num_clusters][0], scale=stdev),\n         np.random.normal(loc=centroids[i % num_clusters][1], scale=stdev),\n         str((i % num_clusters) % 2)] for i in range(1000)]\nrandom.shuffle(data, random.random)\n\nx1 = [x[0] for x in data if x[2] == '0']\nx2 = [x[1] for x in data if x[2] == '0']\nplt.scatter(x1, x2, color='orange')\nx1 = [x[0] for x in data if x[2] == '1']\nx2 = [x[1] for x in data if x[2] == '1']\nplt.scatter(x1, x2, color='blue')\nplt.show()\n\ndf = pd.DataFrame(data, columns=['x1', 'x2', 'y'])\ndf.head()\n","output":{"0":{"data":{"image/png":"bc7cef4ad913394dadeceeb97def327f150e03f2","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":1,"metadata":{"image/png":{"height":411,"width":706},"needs_background":"light"},"output_type":"execute_result"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.736644</td>\n      <td>0.394326</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.162748</td>\n      <td>0.047812</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.214820</td>\n      <td>0.606934</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.245322</td>\n      <td>0.714742</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.438145</td>\n      <td>0.498891</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         x1        x2  y\n0  0.736644  0.394326  1\n1  0.162748  0.047812  1\n2  0.214820  0.606934  0\n3  0.245322  0.714742  0\n4  0.438145  0.498891  1"},"exec_count":1,"output_type":"execute_result"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"87993f","input":"targets = [[4003, -9.96], [4003, -9.98], [6000, -9.98]]\n\n","pos":20,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"b84385","input":"# drawing from example: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n\n# Create color maps\ncmap_light = ListedColormap(['orange', 'blue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\n# Create input/output dataframe slices\nX = df[['x1', 'x2']]\ny = df['y'].apply(lambda y: 1 if y == '1' else 0)  # Note: these values need to be 0-based indices to the colormap colors.\n                                                   # If y values go beyond the indices of the colormaps, you'll get a \"TypeError: iteration over a 0-d array\"\n\n# Build the k-NN classifier\nclf = KNeighborsClassifier()\nclf.fit(X, y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nh = .005  # step size in the mesh\nx_min, x_max = X['x1'].min() - .1, X['x1'].max() + .1\ny_min, y_max = X['x2'].min() - .1, X['x2'].max() + .1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(10, 10))\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='nearest')\n\n# Plot also the training points\nplt.scatter(X['x1'], X['x2'], c=y, cmap=cmap_bold, edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classification (k = 5)\")\nplt.show()","output":{"0":{"data":{"image/png":"0e5158cf5d40fdb7aeb2bc2ecddbb00d469d7ff6","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":2,"metadata":{"image/png":{"height":590,"width":595},"needs_background":"light"},"output_type":"execute_result"}},"pos":4,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"b8c783","input":"targets = [[10123, -49.9], [10630, -49.4], [10630, -49.9], [10123, -49.4]]\n\n","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"b24591","input":"k_values = [1, 11, 111, 555]\n\nfor k in k_values: \n    # Build the k-NN classifier\n    clf = KNeighborsClassifier(n_neighbors=k)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    h = .005  # step size in the mesh\n    x_min, x_max = X['x1'].min() - .1, X['x1'].max() + .1\n    y_min, y_max = X['x2'].min() - .1, X['x2'].max() + .1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(10, 10))\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='nearest')\n\n    # Plot also the training points\n    plt.scatter(X['x1'], X['x2'], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"k-NN classification (k = {})\".format(k))\n    plt.show()","output":{"0":{"data":{"image/png":"09a3fd98a18ade5a2c988730cb4c00247befde5e","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":590,"width":595},"needs_background":"light"},"output_type":"execute_result"},"1":{"data":{"image/png":"5e89d7ac069bcdabc1033d818f80c425bbbd2dd9","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":590,"width":595},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"aa61d0fe3f14e5eaffe8e9356742db5247d0d463","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":590,"width":595},"needs_background":"light"},"output_type":"execute_result"},"3":{"data":{"image/png":"3d4a5d6270442db901f89694be6f2fad975aa888","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":590,"width":595},"needs_background":"light"},"output_type":"execute_result"}},"pos":6,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"35ad8d","input":"k_values = list(range(1, 111))\n\n# Create input/output dataframe slices\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=0)\n\ndef get_accuracy(k):\n    # Build the k-NN classifier\n    clf = KNeighborsClassifier(n_neighbors=k)\n    clf.fit(X_train, y_train)\n    # Compute predictions\n    y_predict = clf.predict(X_test)\n    return accuracy_score(y_test, y_predict)\n\ndef get_train_accuracy(k):\n    # Build the k-NN classifier\n    clf = KNeighborsClassifier(n_neighbors=k)\n    clf.fit(X_train, y_train)\n    # Compute predictions\n    y_predict = clf.predict(X_train)\n    return accuracy_score(y_train, y_predict)\n\nacc_values = [get_accuracy(k) for k in k_values]\ntrain_acc_values = [get_train_accuracy(k) for k in k_values]\nplt.plot(k_values, train_acc_values, color = 'orange')\nplt.plot(k_values, acc_values, color='blue')\nplt.show()\nbest_index = np.argmax(acc_values)\nprint(\"Parameter k={} gave the maximum accuracy: {}\".format(k_values[best_index], acc_values[best_index]))","output":{"0":{"data":{"image/png":"86612db4196de0f901e4bbcd660deb5127fab609","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":4,"metadata":{"image/png":{"height":411,"width":713},"needs_background":"light"},"output_type":"execute_result"},"1":{"name":"stdout","output_type":"stream","text":"Parameter k=35 gave the maximum accuracy: 0.904\n"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"0028e7","input":"# Build the k-NN classifier\nk = 35\nclf = KNeighborsClassifier(n_neighbors=k)\nclf.fit(X, y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nh = .005  # step size in the mesh\nx_min, x_max = X['x1'].min() - .1, X['x1'].max() + .1\ny_min, y_max = X['x2'].min() - .1, X['x2'].max() + .1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(10, 10))\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='nearest')\n\n# Plot also the training points\nplt.scatter(X['x1'], X['x2'], c=y, cmap=cmap_bold, edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classification (k = {})\".format(k))\nplt.show()","output":{"0":{"data":{"image/png":"fde0b3c7e8896e26cdb7a69e420613853f01d4fc","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":590,"width":595},"needs_background":"light"},"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"fdfbe6","input":"df['x1times100'] = df['x1'] * 100\nX2 = df[['x1times100', 'x2']]\nX_train, X_test, y_train, y_test = train_test_split(X2, y, train_size=0.5, random_state=0)\n\n# Build the k-NN classifier\nk = 35\nclf = KNeighborsClassifier(n_neighbors=k)\nclf.fit(X_train, y_train)\n# Compute predictions\ny_predict = clf.predict(X_train)\nacc = accuracy_score(y_train, y_predict)\n\nprint('Accuracy =', acc)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nh = .005  # step size in the mesh\nx_min, x_max = X2['x1times100'].min() - .1, X2['x1times100'].max() + .1\ny_min, y_max = X2['x2'].min() - .1, X2['x2'].max() + .1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h * 100),\n                     np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(10, 10))\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='nearest')\n\n# Plot also the training points\nplt.scatter(X2['x1times100'], X2['x2'], c=y, cmap=cmap_bold, edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classification without normalization (k = {})\".format(k))\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy = 0.714\n"},"1":{"data":{"image/png":"5ccd3ebf4a30842af8e137e08307bbf5e150b0c3","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":6,"metadata":{"image/png":{"height":590,"width":595},"needs_background":"light"},"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"c37bd1","input":"# Load example data\ndf = pd.read_csv('http://cs.gettysburg.edu/~tneller/ds256/data/hw10/ex10-1.csv', index_col='id')\nprint(df.describe())\nprint(df.head(10))\n\n# Create a normalization of x1, x2\nfrom sklearn import preprocessing\n\ninput_cols = ['x1','x2']\nx = df[input_cols].values  # returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf_scaled = pd.DataFrame(x_scaled, columns=input_cols)\nprint(\"After scaling:\")\nprint(df_scaled.head(10))\n\n# Create a mapping of all unique y class/category values to 0, 1, ...\nclasses = df.y.unique()\nclass_map = {c : i for i, c in enumerate(classes)}\ndf_scaled['y'] = df['y'].map(class_map)\nprint(\"After class-encoding:\")\nprint(df_scaled.head(10))\n\n# Divide into train, test sets\nX = df_scaled[['x1', 'x2']]\ny = df_scaled['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=0)\n\n# Build k-NN models and assess accuracy\nk_values = list(range(1, 400, 5))\nacc_values = [get_accuracy(k) for k in k_values]\nplt.plot(k_values, acc_values, color='blue')\nplt.show()\nbest_index = np.argmax(acc_values)\nprint(\"Parameter k={} gave the maximum accuracy: {}\".format(k_values[best_index], acc_values[best_index]))\n\n# Build our best k-NN model and plot\nk = 101  # Note: Some use k = sqrt(n) as a default for a dataset of size n\nclf = KNeighborsClassifier(n_neighbors=k)\nclf.fit(X, y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nh = .005  # step size in the mesh\nx_min, x_max = X['x1'].min() - .1, X['x1'].max() + .1\ny_min, y_max = X['x2'].min() - .1, X['x2'].max() + .1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(10, 10))\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='nearest')\n\n# Plot also the training points\nplt.scatter(X['x1'], X['x2'], c=y, cmap=cmap_bold, edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classification (k = {})\".format(k))\nplt.show()\n\n# Perform a prediction for x1 = -49, x2 = -0.45\ntarget = [-49, -0.45]\nprint(\"Target input for prediction: \", target)\ntarget_scaled = min_max_scaler.transform([target])\nprint(\"Scaled inputs for prediction from our model:\", target_scaled)\nclass_predict = clf.predict(target_scaled)\nprint(\"Predicted class index:\", class_predict)\nprint(\"Class index mapped to class prediction:\", classes[class_predict[0]])","output":{"0":{"name":"stdout","output_type":"stream","text":"                 x1            x2\ncount  10000.000000  10000.000000\nmean     -46.236768     -0.462488\nstd        2.906567      0.028978\nmin      -52.461559     -0.524602\n25%      -48.719696     -0.487313\n50%      -46.263362     -0.462583\n75%      -43.745542     -0.437293\nmax      -40.009191     -0.401922\n           x1        x2    y\nid                          \n0  -50.335346 -0.440816  bar\n1  -47.063696 -0.479325  bar\n2  -50.012562 -0.462413  bar\n3  -49.374007 -0.442351  bar\n4  -42.125478 -0.428049  bar\n5  -41.479471 -0.442391  foo\n6  -46.589062 -0.480678  bar\n7  -50.023813 -0.506963  foo\n8  -47.188830 -0.440717  foo\n9  -52.024189 -0.447853  bar\nAfter scaling:\n         x1        x2\n0  0.170748  0.682963\n1  0.433481  0.369072\n2  0.196669  0.506923\n3  0.247949  0.670449\n4  0.830049  0.787031\n5  0.881928  0.670127\n6  0.471597  0.358042\n7  0.195766  0.143786\n8  0.423432  0.683768\n9  0.035123  0.625605\nAfter class-encoding:\n         x1        x2  y\n0  0.170748  0.682963  0\n1  0.433481  0.369072  0\n2  0.196669  0.506923  0\n3  0.247949  0.670449  0\n4  0.830049  0.787031  0\n5  0.881928  0.670127  1\n6  0.471597  0.358042  0\n7  0.195766  0.143786  1\n8  0.423432  0.683768  1\n9  0.035123  0.625605  0\n"},"1":{"data":{"image/png":"566c74d0ae9b0a0dda739868703596927a43b23a","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":7,"metadata":{"image/png":{"height":411,"width":719},"needs_background":"light"},"output_type":"execute_result"},"2":{"name":"stdout","output_type":"stream","text":"Parameter k=101 gave the maximum accuracy: 0.9266\n"},"3":{"data":{"image/png":"713f8631d6e442cc9106af910806b5f849083cff","text/plain":"<Figure size 720x720 with 1 Axes>"},"exec_count":7,"metadata":{"image/png":{"height":590,"width":595},"needs_background":"light"},"output_type":"execute_result"},"4":{"name":"stdout","output_type":"stream","text":"Target input for prediction:  [-49, -0.45]\nScaled inputs for prediction from our model: [[0.277984   0.60810338]]\nPredicted class index: [0]\nClass index mapped to class prediction: bar\n"}},"pos":14,"scrolled":true,"type":"cell"}
{"cell_type":"markdown","id":"001904","input":"(end of homework)","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"025e25","input":"# k-Nearest Neighbors\n\nRecall that the supervised learning task of classification is build a model that maps inputs to classes (categories) well given a set of input-class pairs.  We first generate and display data generated in a way similar to that of the example of ISLR 2.2.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"0859be","input":"**Exercise 2:** Model the data of [hw10-2.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw10/hw10-2.csv) with k-nearest neighbor classification as above.  Split your training and test data 50/50, train your k-NN model on the training data, and choose a $k$ yielding good accuracy with the testing data.  Predict the classification of the target values provided below.  Note here that there are more than two class values.","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"49604e","input":"**Exercise 1:** Model the data of [hw10-1.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw10/hw10-1.csv) with k-nearest neighbor classification as above.  Split your training and test data 50/50, train your k-NN model on the training data, and choose a $k$ yielding good accuracy with the testing data.  Predict the classification of the target values provided below.","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"525792","input":"**Exercise 2:** Model the data of [ic10-2.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw10/ic10-2.csv) with k-nearest neighbor classification as above.  Split your training and test data 50/50, train your k-NN model on the training data, and choose a $k$ yielding good accuracy with the testing data.  Predict the classification of the target values provided below.  Note here that there are more than two class values.","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"64f658","input":"# In Class\n\nTogether in class, you will seek to build models for each of the given data sets using the techniques above.","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"6bf5c3","input":"**Exercise 1:** Model the data of [ic10-1.csv](http://cs.gettysburg.edu/~tneller/ds256/data/hw10/ic10-1.csv) with k-nearest neighbor classification as above.  Split your training and test data 50/50, train your k-NN model on the training data, and choose a $k$ yielding good accuracy with the testing data.  Predict the classification of the target values provided below.","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"7fd353","input":"For small $k$, we get a very strange boundary that one would suspect wouldn't generalize well.  For large $k$, we get a very smooth boundary with many misclassifications.  We can rightly expect that there is some best value of $k$ that generalizes well beyond our data.  How do we find that?\n\nThe common practice is to _divide our data into a training set and a test set_.  We build our model with the training data and we test its performance with the testing data.  For our classification task, we will use the ```sklearn.metrics.accuracy_score``` as our performance measure.  It is the fraction of correct classifications.\n\nWe will now divide our data in half into a 500 point training set and a 500 point testing set.  The points are already shuffled, otherwise shuffling them would be an important step to avoid having all of one class in the training set and all of another class in the testing set. \n\nWe can then try a range of $k$ values to see which has the highest accuracy for the test set.  (Note that poor testing with our training data would give perfect accuracy for $k = 1$ and generally decreasing accuracy for increasing $k$.)","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"a03d16","input":"Here we see a relatively smooth boundary with inevitable classification error from the overlap of the source data distributions.\n\nThe main takeaway point here is that performance on the training dataset doesn't tell us how we should expect our model to perform on unseen data.  For this reason, we hold out a testing dataset from the training process and use it only to assess the generalizability of our model.  This is an important practice of a Data Scientist.\n\n## Normalization\n\nSince we're using Euclidean distance as our metric for \"nearest\" in k-NN, it's important that our data is of the same scale.  Watch what happens when we multiply ```x1``` times 100 and perform the same process.","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"a06ab9","input":"This plot shows how our model has decreasing accuracy on the training set as $k$ increases, yet the accuracy on the testing set increases and then decreases, indicating a happy middle range.  If we smoothed this data, we'd see it peak in the 30s.  Any value in the upper 30s gives good accuracy for the test data.  Let's see what the k=35 boundary looks like.","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"b19e1c","input":"The [k-Nearest Neighbors Algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) (k-NN) finds the $k$ closest points to an input point according to a metric (e.g. Euclidean distance) and forms a classification or regression prediction from those points.  Whereas linear regression is a _parametric_ method that learns parameters for its model (e.g. y-intercept, coefficients), k-NN is an instance-based (i.e. working directly from the data) and lazy-learning (i.e. computing results on demand) method that is both flexible and more computationally demanding on the application side of computation.\n\nTo compute a probability of whether a point is in class (category) $c$, k-NN finds the $k$ closest points and computes the fraction of those points that are in class $c$.  To predict a classification, it predicts the class with the highest probability.  Suppose there is a tie, e.g. $k=2$ and each of the two nearest neighboring points belong to different classes.  There are many ways people break ties in practice:\n* arbitrarily\n* randomly\n* weighting the \"votes\" according to nearness of neighbors (inverse distance or inverse distance squared) of neighbors\n* incrementing $k$ for the prediction until the tie is broken, i.e. looking to next neighbor until the tie is broken\n\nWe'll now build a k-NN classifier with the default $k=5$ and visualize the classification.","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"b835b9","input":"This seems like a reasonable classification boundary with the default $k=5$.  Let us see how this looks for other choices of $k$.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"c31ed0","input":"## The Bias-Variance Spectrum\n\nThere are a number of criteria used for judging the characteristics of different machine learning and statistical models:\n* _bias_ - How much is a model constrained to model only a small subset of functions?\n* _variance_ - How much can a change to the output of a single training example affect the learned model?\n* _interpretability_ - How easy is it to interpret what the model has learned?  How well does it aid human explanation of predictions?\n* _performance_ - How well accurately does a model predict for complex functions?\n\nWe will observe that models tend to fall along a spectrum from one extreme to another.  Let us observe what we know of k-NN classification:\n* low-bias - k-NN makes no particular commitment to a functional form for classification.  It predicts what nearby data predicts.\n* high-variance - For low $k$, a single outlying point can affect the predictions for a large region.\n* low-interpretability - For higher dimensions that are difficult to visualize, we cannot learn any useful generalized information about the model.  The model itself is _local_ in nature and only reflects local data, not general patterns.\n* high-performance - k-NN properly applied does a good job of prediction as long as we can handle the computational demands.\n\nContrast this with linear regression:\n* high-bias - Linear regression assumes a linear form to _any_ function one applies it to.  One can perform transformations to allow non-linear regression using linear regression, but linear regression itself assumes linear relationships between variables.\n* low_variance - A single point generally has little affect on the least-squares error, as it is only one point among many contributing to that error measure.\n* high-interpretability - The simple model is easy to explain. One can talk of the significance of different variables, their relative contribution to the prediction, etc.\n* low-performance - Beyond functions that can be transformed to linear relationships, linear regression performs poorly as a predictor.\n\nIn general, classical statistical methods tend to reside at the high-bias, low-variance, high-interpretability, low-performance end of the spectrum.  Modern statistical and machine learning methods lie along the spectrum with k-NN, support vector machines (SVMs), and artificial neural networks (ANNs) residing at the low-bias, high-variance, low-interpretability, high-performance end of the spectrum.\n\nIt will become important to gain an understanding of a spectrum of modeling options depending on your application, especially considering the relative importance of accurate prediction to interpretability.\n","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"dd0f78","input":"## Homework\n","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"e10c84","input":"With ```x1``` times 100 setting the values in the horizontal dimension relatively far apart, we see that the relatively close ```x2``` values are more determining of _nearest_. Accuracy has fallen from 90% to 71%, a significant loss.\n\nThus, is it also good practice to _normalize_ data before creating a k-NN model, or preprocessing data for use with the model.\n\nNormalization can take different forms:\n* scaling values so that the maximum - minimum = 1\n* scaling and translating values to the range $[0, 1]$ or $[-0.5, 0.5]$\n* for normal (a.k.a. Gaussian) distributions, scaling and translating to be mean 0 and standard deviation 1. This is called _standardization_.\n* transformation to percentiles\n\nWhatever the method, the basic idea is to transform the data so that no one dimension of the data has excessive influence on what it means to be nearest.\n\nIn summary, when applying k-NN for classification:\n* Normalize your input data.\n* Map your output classes/categories to 0-based indices.\n* Split your data into training and testing sets.\n* Perform k-NN on training data for a range of $k$ values, assessing accuracy with the testing data to find the best $k$.\n* Predict classification on new data by first normalizing it exactly as you normalized your data.\n* Map your predictions back to your original classes/categories.\n\nLet's now see the entire process with visualization as above for a new two-class dataset.\n","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"fcaf26","input":"# k-Nearest Neighbor Classification and The Importance of Separate Test Data\n\nLearning Objectives:\n* Students will learn about and apply the k-nearest neighbors algorithm for classification.\n* Students will understand the importance of and practice the separation of training and test data to test for model overfitting.\n* Students will learn of the tradeoff spectrum from high-bias, low-variance, interpretable, simple models to low-bias, high-variance, uninterpretable, complex models.\n\nBefore class:\n* Introductory section of the Wikipedia [k-Nearest Neighbors Algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) article\n* Section 2.2 of James et al., [An Introduction to Statistical Learning with Applications in R](http://faculty.marshall.usc.edu/gareth-james/ISL/) _Download the outstanding free PDF textbook. If this course used the R programming language, this would be a core text for us.  Don't concern yourself so much with the mathematical notation, but seek to understand the core tradeoffs we'll be observing here and with other models in this course._\n* Read below up to (but not including) the section marked In Class.\n\nOptional:\n* If you have the time, go back and read section 2.1 of [An Introduction to Statistical Learning with Applications in R (a.k.a. ISLR)](http://faculty.marshall.usc.edu/gareth-james/ISL/).  _The introductory chapters provide excellent perspective on the tasks and challenges of Data Science._\n* Enjoy this [Michael Littman, Charles Isbell Overfitting Music Video](https://www.youtube.com/watch?v=DQWI1kvmwRg) a parody of [Michael Jackson's Thriller](https://www.youtube.com/watch?v=sOnqjkJTMaA).\n\nIn class:\n* We will work together on the exercises in section \"In Class\".\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.\n","pos":0,"type":"cell"}
{"id":0,"time":1601009444861,"type":"user"}
{"last_load":1600839987724,"type":"file"}