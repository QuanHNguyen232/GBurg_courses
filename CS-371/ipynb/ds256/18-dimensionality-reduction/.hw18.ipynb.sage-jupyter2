{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":65970176},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"1524a4","input":"","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b7e80d","input":"","pos":19,"type":"cell"}
{"cell_type":"code","exec_count":110,"id":"7ef766","input":"from sklearn.datasets import load_digits\ndigits = load_digits()\npca = PCA(2)  # project from 64 to 2 dimensions\nprojected = pca.fit_transform(digits.data)\nplt.scatter(projected[:, 0], projected[:, 1],\n            c=digits.target, edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap('Spectral', 10))  # Note: 'Spectral' must now be capitalized, unlike lowercase 'spectral' in text.\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.colorbar()\nplt.show()","output":{"0":{"data":{"image/png":"ff9fb6b678aabd33cbb21e5a298fcc1a99b82551","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":110,"metadata":{"image/png":{"height":428,"width":657},"needs_background":"light"},"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":115,"id":"58772e","input":"perplexities = [5, 30, 50]  # 30 is the default\nfor p in perplexities:\n    print(\"Perplexity =\", p)\n    tsne = TSNE(random_state=0, perplexity=p)\n    projected = tsne.fit_transform(digits.data)  # project from 64 to 2 dimensions\n    plt.scatter(projected[:, 0], projected[:, 1],\n                c=digits.target, edgecolor='none', alpha=0.5,\n                cmap=plt.cm.get_cmap('Spectral', 10))  # Note: 'Spectral' must now be capitalized, unlike lowercase 'spectral' in text.\n    plt.xlabel('component 1')\n    plt.ylabel('component 2')\n    plt.colorbar()\n    plt.show()\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Perplexity = 5\n"},"1":{"data":{"image/png":"b389f71c2bdac7775b5a1c8bdf29c2f60be17fd1","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":115,"metadata":{"image/png":{"height":428,"width":664},"needs_background":"light"},"output_type":"execute_result"},"2":{"name":"stdout","output_type":"stream","text":"Perplexity = 30\n"},"3":{"data":{"image/png":"03ab0f66ef0bd27ef212289a7850ededb9410dcb","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":115,"metadata":{"image/png":{"height":428,"width":657},"needs_background":"light"},"output_type":"execute_result"},"4":{"name":"stdout","output_type":"stream","text":"Perplexity = 50\n"},"5":{"data":{"image/png":"04e9bba2820a184232a6d42bbb2455c70146f4d9","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":115,"metadata":{"image/png":{"height":428,"width":657},"needs_background":"light"},"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":116,"id":"d91d34","input":"# Imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nimport umap\n","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":117,"id":"97424f","input":"projected = umap.UMAP(n_neighbors=5,\n                      min_dist=0.3,\n                      metric='correlation').fit_transform(digits.data)\nplt.scatter(projected[:, 0], projected[:, 1],\n                c=digits.target, edgecolor='none', alpha=0.5,\n                cmap=plt.cm.get_cmap('Spectral', 10))  # Note: 'Spectral' must now be capitalized, unlike lowercase 'spectral' in text.\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.colorbar()\nplt.show()","output":{"0":{"data":{"image/png":"c40d8899a93148c3f6f27f1980db88116a58174f","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":117,"metadata":{"image/png":{"height":428,"width":657},"needs_background":"light"},"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"31f72d","input":"print(df.columns[:4])\npca.components_\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Index(['sepal length', 'sepal width', 'petal length', 'petal width'], dtype='object')\n"},"1":{"data":{"text/plain":"array([[ 0.52237162, -0.26335492,  0.58125401,  0.56561105],\n       [ 0.37231836,  0.92555649,  0.02109478,  0.06541577]])"},"exec_count":19,"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"3d73d1","input":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 Component PCA', fontsize = 20)\ntargets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\ncolors = ['r', 'g', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\n\n# Adapted from https://stackoverflow.com/questions/39216897/plot-pca-loadings-and-loading-in-biplot-in-sklearn-like-rs-autoplot:\nvector_scale = 2.7\ntext_beyond = 1.15\nfor i in range(len(pca.components_[0])):\n    ax.arrow(0, 0, vector_scale * pca.components_[0, i], vector_scale * pca.components_[1, i], color='orange', alpha=0.5, width=.025)\n    plt.text(vector_scale * pca.components_[0, i] * text_beyond, vector_scale * pca.components_[1, i] * text_beyond, df.columns[i], color='black', ha='center', va='center')\n\n    ax.grid()\nplt.show()\n","output":{"0":{"data":{"image/png":"2f487f66ef1a5a0e24183064a671e5cbd022c96b","text/plain":"<Figure size 576x576 with 1 Axes>"},"exec_count":20,"metadata":{"image/png":{"height":505,"width":501},"needs_background":"light"},"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"3d7bfd","input":"# PCA visualization of Iris data from https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/PCA/PCA_Data_Visualization_Iris_Dataset_Blog.ipynb\n\nprint('Load Iris DataSet:')\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n# loading dataset into Pandas DataFrame\ndf = pd.read_csv(url\n                 , names=['sepal length','sepal width','petal length','petal width','target'])\nprint(df.head())\n\nprint('Standardize the Data:')\nfeatures = ['sepal length', 'sepal width', 'petal length', 'petal width']\nx = df.loc[:, features].values\ny = df.loc[:,['target']].values\nx = StandardScaler().fit_transform(x)\nprint(pd.DataFrame(data = x, columns = features).head())\n\nprint('PCA Projection to 2D:')\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\nprint(principalDf.head(5))\nprint(df[['target']].head())\nfinalDf = pd.concat([principalDf, df[['target']]], axis = 1)\nfinalDf.head(5)\n\nprint('Visualize 2D Projection:')\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 Component PCA', fontsize = 20)\ntargets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\ncolors = ['r', 'g', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()\nplt.show()\n\nprint('Explained Variance:')\nprint(pca.explained_variance_ratio_)\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Load Iris DataSet:\n"},"1":{"name":"stdout","output_type":"stream","text":"   sepal length  sepal width  petal length  petal width       target\n0           5.1          3.5           1.4          0.2  Iris-setosa\n1           4.9          3.0           1.4          0.2  Iris-setosa\n2           4.7          3.2           1.3          0.2  Iris-setosa\n3           4.6          3.1           1.5          0.2  Iris-setosa\n4           5.0          3.6           1.4          0.2  Iris-setosa\nStandardize the Data:\n   sepal length  sepal width  petal length  petal width\n0     -0.900681     1.032057     -1.341272    -1.312977\n1     -1.143017    -0.124958     -1.341272    -1.312977\n2     -1.385353     0.337848     -1.398138    -1.312977\n3     -1.506521     0.106445     -1.284407    -1.312977\n4     -1.021849     1.263460     -1.341272    -1.312977\nPCA Projection to 2D:\n   principal component 1  principal component 2\n0              -2.264542               0.505704\n1              -2.086426              -0.655405\n2              -2.367950              -0.318477\n3              -2.304197              -0.575368\n4              -2.388777               0.674767\n        target\n0  Iris-setosa\n1  Iris-setosa\n2  Iris-setosa\n3  Iris-setosa\n4  Iris-setosa\nVisualize 2D Projection:\n"},"2":{"data":{"image/png":"eb2194b912e76dff657bea85b18c31066d3d8c51","text/plain":"<Figure size 576x576 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":505,"width":501},"needs_background":"light"},"output_type":"execute_result"},"3":{"name":"stdout","output_type":"stream","text":"Explained Variance:\n[0.72770452 0.23030523]\n"}},"pos":3,"type":"cell"}
{"cell_type":"markdown","id":"32a512","input":"## Homework\n","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"3f7945","input":"Such visualizations can be helpful to make similarities and difference clear in high dimensional data.  In this case, we have 64-pixel images, so we are projecting 64 dimensions down to only 2!  With this representation, we can note that the dots labeled \"6\" are most similar to \"5\" on one side and \"0\" on the other side.  This makes sense.  If we break below where the loop of the \"6\" meets itself, we can continuously transform it into a \"5\".  Similarly, if we slide the connection of the loop upward and make the loop larger as the stem becomes smaller, the \"6\" continuously transforms into a \"0\".  These similarities, visualized here for us can help make us aware of what is necessary for digits to appear distinctive and where ambiguities can creep in.  (This is important for font design.)\n\nHowever, we also see that PCA has a \"bunching\" problem with many of the other digits overlapping one another in 2 dimensions.\n\n$t$-SNE is an algorithm that seeks to \"unfold\" points in high dimensions so as to project similar points near to one another in 2D, while holding different points apart.  Let us observe how $t$-SNE performs with the same data for different \"perplexity\" parameter settings.","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"4142a3","input":"(end of homework)","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"6f5802","input":"Another way of thinking of this is that a change in any given attribute moves the PCA 2D projection of the point in the direction of it's corresponding arrow with proportional magnitude of change.\n\nAs our authors have noted, PCA is an unsupervised learning algorithm that allows us to transform our original input data to a lower dimension, which can:\n 1. yield helpful visualizations like these to better comprehend patterns in our data,\n 2. yield a lower dimension representation that still captures most of the variation of the data while allowing more efficient machine learning with lower-dimensional data, and\n 3. effectively filter noise by projecting data onto a lower-dimensional representation that retains the most essential components of the data.\n\nIn class, you will perform a similar task as above, but with a different dataset.\n","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"9832ef","input":"What we see in the figure above is 4-dimensional points mapped onto a 2-dimensional plane such that the principal component 1 and 2 axes capture ~73% and ~23% of the variance respectively.  One thing Galarnyk does not do with this example is give a sense of what these two principal components reflect in the original dimensions.  Here we will compute a \"biplot\" which overlays vectors for each of the original dimensions, showing how much they contribute towards each of these principal components.\n\nFirst, let us look at the two principal component vectors themselves.","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"bb99e4","input":"# Dimensionality Reduction\n\nLearning Objectives:\n* Students will learn the motivation for dimensionality reduction techniques.\n* Students will be exposed to two algorithmic approaches to dimensionality reduction: principal component analysis (PCA) and $t$-distributed stochastic neighbor embedding ($t$-SNE).\n* Students will practice the application of PCA with a biplot for gaining visual insight to higher-dimensional data.\n\nVideo/Readings before class:\n* PCA:\n  * Jake VanderPlas. [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/):\n      * [Chapter 5 section \"In Depth: Principal Component Analysis\"](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)\n  * Michael Galarnyk's tutorial [PCA using Python (scikit-learn)](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60) _Note: [MNIST](https://en.wikipedia.org/wiki/MNIST_database) is pronounced \"em nist\", not \"minced\".  If you prefer, you can go through this tutorial with his [companion YouTube video](https://www.youtube.com/watch?v=kApPBm1YsqU).  We will look at his Iris dataset PCA a bit closer below._\n  * Optional: James et al. [An Introduction to Statistical Learning with Applications in R, section 10.2](http://faculty.marshall.usc.edu/gareth-james/ISL/) _This excellent, free Data Science textbook provides greater mathematical insight to algorithms such as these, and features interesting examples.  We will work with one of their examples below in Python._\n* t-SNE:\n  * Wattenberg et al. [How to use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)\n  * Briefly skim the general area of manifold learning: Jake VanderPlas. [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/):\n      * [Chapter 5 section \"In Depth: Manifold Learning\"](https://jakevdp.github.io/PythonDataScienceHandbook/05.10-manifold-learning.html)\n\nFor reference:\n  * [sklearn.decomposition.PCA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n  * [sklearn.manifold.TSNE documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n\nBefore class:\n* View/read the above resources so as to understand how each of the two dimensionality reduction methods works.  Below, we will demonstrate the application of both.\n\nIn class:\n* We will work together on the exercise in section \"In Class\".\n\nHomework after class:\n* Complete the section labeled \"Homework\" below before the next class when it will be collected.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"c05196","input":"Note how sensitive $t$-SNE is to its perplexity parameter.  While it achieves good separation of digits data (without digits target values) using different perplexity values, you will note that the spatial relationships change from one value to the next.  Some strong similarity relationships are reflected in all runs, but take a moment to observe which digits are deemed most similar to one another in the different plots above.\n\nFinally, watch this video where Daniel Smilkov, Fernanda Vi√©gas, and Martin Wattenberg present [\"A.I. Experiments: Visualizing High-Dimensional Space\"](https://www.youtube.com/watch?v=wvsE8jm1GzE).  Not only will you see 3D $t$-SNE visualizations of the MNIST digit data where we can see related images/shapes, but you'll also see how $t$-SNE can discover related _words_ in text.  As you look further into this area, you can find applications to query answering, discovering genetic similarities, etc.\n\nAs unsupervised learning tools, dimensionality reduction can help us gain better insight into high-dimensional data in many application areas.\n\nPostscript: We have been sticking fairly close to what is available in numpy, pandas, matplotlib, and sklearn.  A newer manifold learning method called [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://umap-learn.readthedocs.io/en/latest/) has gained in popularity for high-dimensional applications (e.g. for [single cell genomics](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668)) where $t$-SNE typically requires one to preprocess with PCA to a smaller number of dimensions to make $t$-SNE computationally feasible.  Here's UMAP applied to the same MNIST digit data:","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"c85f92","input":"## $t$-distributed stochastic neighbor embedding ($t$-SNE)\n\nLet's review VanderPlas' MNIST digit example from our PCA reading assignment:","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"cbf6e6","input":"# PCA Visualization of the Iris Dataset\n\nThe [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) is a very common ML example classification dataset that has four inputs (sepal length, sepal width, petal length, and petal width in cm) and one output (class of iris: Iris Setosa, Iris Versicolour, or Iris Virginica).  Below is the code from the Galarnyk tutorial assigned above.","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"e7b791","input":"**Exercise 1:** Complete the in-class exercise if you haven't already.\n\n**Exercise 2:** Perform the same PCA 2D visualization as before on the data of ```eating_in_the_uk.csv```, included in this directory.  In addition to previous steps,\n\n * perform **no data scaling** for this exercise,\n * identify the country that is most different in eating habits from the other countries, and\n * describe the main differences in eating habits of this country, i.e. the largest influences (both positive and negative) of the original food types on the principal components.\n\nHints:\n* For the last bullet above, which attribute (input) points most towards the outlier and most away from the outlier?\n* Note that this is data with 17 food type dimensions for 4 U.K. countries.  You'll do best to identify the index_col parameter when loading the .csv file, and then transpose the dataframe so that rows become columns and vice versa.\n\nOptional: You may also find it interesting to characterize the eating habits of other countries relative to each other by noting the largest influences (both positive and negative) in the same way you did for the outlier country.\n","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"ebb5c9","input":"We could look at these principal component vectors and see that, for instance, sepal width has small negative contribution to principal component 1 (PC1), but very strong positive contribution to principal component 2 (PC2).  Overall, looking at the numbers, we see that all other attributes (i.e. input columns) positively contribute to PC1, but sepal width is considered largely in PC2.  Instead of looking at a sea of numbers, it can be helpful to plot these each of the attributes as a vector on top of our PCA scatterplot to get a 2D sense of the significant of the original 4D inputs:","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"f131d2","input":"# In Class\n\n## Principal Component Analysis (PCA)\n\nFor your first exercise, your challenge is apply PCA and replicate the biplot of Figure 10.1 in James et al. [An Introduction to Statistical Learning with Applications in R, section 10.2](http://faculty.marshall.usc.edu/gareth-james/ISL/).  You will find the necessary data in this directory in file ```USArrests.csv```.  Use ```plt.text``` for each state to manually create a scatter plot of names that are centered horizontally and vertically on their 2D point.\n\nAdditionally:\n* Print the two principal components.\n* Print the explained variance for each component.\n* Print the total explained variance for this 2D representation of the data.\n\nNote:\n* Since each principal component with reversed signs works equally well in explaining variance, do not be concerned if any of your components are pointing in the opposite direction and your graph appears flipped in one or both dimensions.\n* Do not concern yourself with top and right axes showing the different scale of your principal components in your biplot.\n* Both this exercise and the homework exercise will require you to apply more of your Python and pandas-specific skills to reach the goal.  Over time, there will be less copy-paste-edit of template code, and more of the need for you to write the code to take raw data and gain insight to it.  While the solutions to this exercise and the homework exercise are a few dozen lines apiece, work in pairs and give yourselves plenty of time to work through the inevitable bugs (and be able to seek assistance if you are stuck).\n","pos":15,"type":"cell"}
{"id":0,"time":1603266031236,"type":"user"}
{"last_load":1603265983389,"type":"file"}